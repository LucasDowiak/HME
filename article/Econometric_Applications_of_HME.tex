\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{graphicx}

% The hessian:
\newcommand{\bw}[1]{\boldsymbol{\omega}^{#1}}
\newcommand{\Ht}[1]{\mathbf{H}_{t}(#1)}
\newcommand{\HH}[1]{\boldsymbol{H}(#1)}
\newcommand{\HI}[1]{\boldsymbol{H}^{-1}(#1)}

\newcommand{\EIm}[1]{\mathbb{E} \left[ I_{t}(#1) \right]}


\usepackage[
  bibencoding=auto,
  backend=biber,
  url=true,
  doi=true,
  style=authoryear,
  uniquename=false,
  natbib
]{biblatex}

\addbibresource{hme_references.bib}

\newcommand{\mean}[1]{\bar{#1}}
\newcommand{\gateprod}[2]{\pi_{#1 \longrightarrow #2}}
\newcommand{\sumgateprod}[3]{\pi_{#1 \overset{#3}{\longrightarrow} #2}}
\newcommand{\shortsum}[1]{\sum \nolimits_{#1}}
\newcommand{\expmixwt}[0]{\mathbb{\Pi}}
\newcommand{\h}[2]{h^{#1}_{#2}}

\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\iu}[1]{\underline{\textit{#1}}}
\newcommand{\ibu}[1]{\textbf{\underline{\textit{#1}}}}
\newcommand{\T}{\rule{0pt}{2.5ex}}       % Top strut
\newcommand{\Eym}{\mathbb{E}^{m} \left[ y_{t} \right]}
\newcommand{\FnOmegaNaught}[2]{\Omega^{(0)}_{t}( #1, #2 )}
\newcommand{\FnOmegaOne}[4]{\Omega^{(1)}_{t}(#1, #2)(#3, #4)}
\newcommand{\FnOmegaTwo}[3]{\Omega^{(2)}_{t}(#1, #2, #3)}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]

\setlength{\parindent}{0pt}

\bibliography{hme_references}

\graphicspath{{/Users/lucasdowiak/Git/hme/article/images/}}

 
\title{Econometric Applications of Hierarchical Mixture of Experts}
\author{Lucas C. Dowiak}

\begin{document}
 
\maketitle{}


PhD Program in Economics, City University of New York\smallskip, Graduate Center,

New York, NY, 10016, \textit{Email: ldowiak@gradcenter.cuny.edu}

\qquad

\begin{abstract}

In this article, a novel mixture model is studied. Named the hierarchical mixture of experts (HME) in the machine learning literature, the mixture model utilizes a set of covariates and a tree-based architecture to efficiently allocate each observation to the appropriate local regression. The nature of the conditional weighting scheme provides the researcher a natural interpretation of how the local (and latent) sub-populations are formed. Marginal effects, robust standard errors, and model selection are also discussed. The model is demonstrated by estimating a Mincer wage equation using US census data and occupational skills data from the Occupational Information Network. Several Monte Carlo exercises are carried out to better understand the behavior of the model on simulated datasets with varying degrees of heterogeneity.
 

Ë‡\end{abstract}

\vspace{1pt}

\begin{quotation}
\textbf{Keywords}: Hierarchical mixture of experts, expectation maximization,
mixture models, Mincer wage equation, machine learning

\textbf{JEL Classification}: 
\end{quotation}

\vspace{1pt}

\section{Introduction}

The concepts of mixture models and mixture distributions are old hat in the economics field. \citet{Hamilton1989} and \citet{GoldfeldQuant1973} are a few of the pioneering works for time series and cross sectional regression, respectively. Today, the modern computing environment is dominated by machine learning, and its reigning champion, the artificial neural network, has been successfully adapted and studied in the context of applied econometrics. This essay adds to the small body of literature that employs a novel neural network architecture to model the weights of a mixture model. In doing so, the model leverages the highly flexible nature of a neural network but maintain interpretability and the means to quantify marginal effects. The model under investigation is called the Hierarchical Mixture of Experts (HME), a class of mixture models whose defining feature is its conditional weighting scheme. The model's origin story traces back to \citet{JJNH1991}. The authors use a single multinomial classifier to assign, in a probabilistic sense, input patterns to \textit{local experts}. These experts are almost always some flavor of regression or classification model. The multinomial structure that assigns inputs to experts is referred to as the \textit{gating network}. The authors employ this mixture of experts (ME) framework to model vowel discrimination in a speech recognization context. Shortly after, \citet{JordanJacobs1992} generalize this single-layer multinomial gating network to one with an arbitrary number of layers. \citet{JordanJacobs1993} then demonstrate an Expectation-Maximization approach to model estimation that is capable of handling the additional complexity the generalization requires during optimization. The result of this extension is a gating network that takes on a tree-like structure, stemming from an initial multinomial split and filtering down through additional multinomial partitions of the input space. HME models nest ME models as special case. Pushing a little further, one additional case is studied as well. As the depth of an HME grows, so too must the number of experts. In the case of a symmetric HME network, this growth is geometric with respect to the network's depth. With this in mind, a further extension can be considered where each expert is not unique, but a member of a fixed set of experts. This additional model is referred to as a Hierarchical Mixture of Repeated Experts (HMRE). Figure (\ref{fig:network_comparison}) provides an example of each of the variations of this class of model.

\bigskip

This essay investigates the adoption of ME and HME models to an applied econometric framework, with particular attention focused on interpretation of the gating network and robust inference of parameter estimates. The outline for the rest of this essay is as follows: the remainder of this section provides a brief literature review and Section \ref{sec:Model} describes the model in formal detail. Section \ref{sec:Estimation} discusses the expectation-maximization approach to estimation while Section \ref{sec:Inference} concerns itself with robust inference of the estimated parameters. Section \ref{sec:MarginalEffects} provides detail on how to derive the marginal effects of the model's covariates and Section \ref{sec:ModelSelection} discusses an approach for model selection. In Section \ref{sec:SimpleExample}, a very simple demonstration of the HME in action is presented with a more economically relevant example of applying the HME model to a Mincer wage equation in Section \ref{sec:MincerWageEx}. Section \ref{sec:Conclusion} concludes.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{network_types.jpeg}
  \caption{Networks \textbf{A} - \textbf{D} depict various network   architectures that are discussed in this article. For all four networks, gating nodes are represented as blue circles and experts as orange rectangles. Network \textbf{A} illustrates the original Mixture of Experts (ME) architecture with a single multinomial split leading to a set of experts one layer down. Networks \textbf{B} and \textbf{C} both represent different flavors of a Hierarchical Mixture of Experts (HME). Network \textbf{B} is a symmetric network of depth 2 with successive binary splits. Network \textbf{C} is an asymmetric network of depth 3 with successive binary splits. Network \textbf{D} is an example of the Hierarchical Mixture of Repeated Experts (HMRE) architecture. Notice that multiple paths exist from the root node $0$ to each expert. Compare this to networks \textbf{A} - \textbf{C}, where there is only one unique path from the root node to each expert.}
  \label{fig:network_comparison}
\end{figure}


\subsection{Relevant Literature}

ME and HME frameworks have been utilized for both time series and cross-sectional analysis. Within the cross-sectional literature, \citet{WaterhouseRobinson1995} puts forth a method to grow an HME from a single split from the root node. The authors are influenced by the popular technique used for classification and regression trees \citep{CART1984} and apply it to an HME structure. Once the gating structure to an HME tree has been grown, the authors suggest an additional trimming algorithm to prevent overfitting. \citet{FFW1997} extend the approach of \citet{WaterhouseRobinson1995} by altering their growing algorithm with a mind to scaling the model to handle thousands of experts. \citet{JordanXuConverge1995} provide an extended discussion on the convergence of the model used by \citet{JordanJacobs1993}. The authors also suggest algorithmic improvements to help with estimation. Continuing the theoretical discussing, \citet{JiangTanner1999} cover convergence rates of an HME model where experts are from the exponential family with generalized linear mean functions. \citet{JiangTanner2000} provide regularity conditions on the HME structure for a mixture of general linear models estimated by maximum likelihood to produce consistent and asymptotically normal estimates of the mean response. The conditions are validated for poisson, gamma, gaussian, and binomial experts.

\bigskip

Alternatively, \citet{WMS1995} provide a detailed discussion examining ME applied in a time series context and provide valuable insights to avoid overfitting the model to the data, a common problem in neural network applications. The authors' formulation of the model has close similarities to other non-linear time series models developed in the late 1980's and early 1990's. A ME time series model sits between the markov-switching (MS) model of \citet{Hamilton1989} and the smooth transition auto-regressive (STAR) model of \citet{Terasvirta1994}, borrowing a bit from both. From an estimation perspective, the ME time series follows close to the markov-switching model due to the fact that they are both mixture distribution where each (conditional) distribution represents a different "state" of nature. The STAR model, on the other hand, posits only a single distribution and different "states" are represented by unique parameter vectors, and as the name implies, the parameters transition smoothly from one state to another over time. The association between the ME, MS, and STAR models is inverted when it comes to how to frame the time evolution of the states. From this perspective, the ME model is very similar to a STAR model in that it also uses the logistic (or multinomial) function to force the probability of belonging to one state to change over time. For MS models, a discrete state markov process is used to model this dynamic change over the time dimension. \citet{HuertaJiangTanner2003} extend \citep{WMS1995} to an HME framework. Using five and a half decades of monthly US industrial production data, the authors allow the series to choose between two models, one modeled as a random walk and the other as trend stationary. In addition, they present a Bayesian approach to estimation. \citet{CarvalhoTanner2003} lay out the necessary regularity conditions to perform hypothesis tests on stationary ME time series of generalized linear models (ME-GLM) using Wald tests. The dual cases of a well-specified and a miss-specified model are considered. The authors restrict their analysis to ME-GLM models involving lagged dependent and lagged external covariate variables only. Generalization to include lagged conditional mean values is left for another time. \citet{CarvalhoTanner2005} take a similar approach to \citet{CarvalhoTanner2003} but apply their analysis to a purely auto-regressive context restricted to Gaussian models. The authors extend arguments in \citet{CarvalhoTanner2003} to non-stationary series and provide simulated evidence that the BIC is a helpful statistic for selecting the appropriate number of experts to include. \citet{CarvalhoTanner2006} re-focus the discussion on ME of time series regressions restricted to exponential family distributions. Distilling the available literature at the time, the authors cover the important topics of estimation and asymptotic properties in the maximum likelihood framework, selection of the number of experts, model validation and fitting. \citet{CarvalhoSkoulakis2010} applies ME of a single time series. Using stock returns, the authors structure the gating network using lagged dependent variables and an 'external' covariate capturing a measure of the trade volume at that time.

\bigskip

In this essay estimation and inference is from a maximum likelihood perspective and will remain the primary focus. Estimation of ME and HME models from a Bayesian has received considerable amount of attention as well. \citet{Waterhouse1995BayesianMF} provided an initial approach to estimating a ME by combining gaussian priors on the gating and expert parameters with gamma hyper-parameter priors in an approximating ensemble to the true joint density of the model. Optimization of the parameter vector for the approximating density occurs a block of parameters at a time. \citet{UedaGhahramani2002} improve on \citet{Waterhouse1995BayesianMF} by optimizing for the appropriate number of experts in addition to  model parameters. \citet{BishopSvenson2003} find previous bayesian approaches to estimating an HME lacking. Using variational inference, the authors provide a complete bayesian estimation approach to the log marginal likelihood. With an eye to prediction, the authors advocate that their approach makes the HME model easier to estimate without overfitting.

\bigskip


\section{Model} \label{sec:Model}

To start, the HME is presented as a standard mixture model. For a given input and output pair $(\boldsymbol{x}_{t}, y_{t})$, each expert provides a probabilistic model relating input row $\boldsymbol{x}_{t}$ to output $y_{t}$:

\begin{equation} \label{eq:ConditionalDistribution}
  P^{m}_{t} \equiv P^{m}(y_{t}|\boldsymbol{x}_{t}; \boldsymbol{\beta}^{m}), \quad m = 1,2,...,M
\end{equation}

where $m$ is one of the $M$ component experts in the mixture. The experts are combined with associated weights into a mixture distribution

\begin{equation} \label{eq:staticmixture}
  P(y_{t} | \boldsymbol{x}_{t}; \, \boldsymbol{\beta}) = \sum_{m=1}^{M} \expmixwt(m|t) P^{m}(y_{t} | \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})
\end{equation}

Here, $\expmixwt(m|t)$ is the probability that the input unit $t$ belongs to expert $m$ and has the usual restrictions: $0 \leq \expmixwt(m|t) \leq 1$ for each $m$ and $\sum_{m} \expmixwt(m|t) = 1$. The gating network of the model applies a particular functional form to model $\expmixwt(m|t)$, which includes a second set of covariates $\boldsymbol{z}_{t}$ and parameter vector $\boldsymbol{\Omega}$:

\begin{equation} \label{eq:mixture}
  P(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\beta}, \boldsymbol{\Omega}) = \sum_{m=1}^{M} \expmixwt(m | \boldsymbol{z}_{t}; \boldsymbol{\Omega}) P^{m}(y_{t} | \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})
\end{equation}


\subsection{Gating Network and $\expmixwt(m | \boldsymbol{Z}, \boldsymbol{\Omega})$} \label{subsec:GatingNetwork}

The gating network model is structured as a collection of nodes in a tree structure that branches out in successive layers. The location of these nodes will be referred to by their address $a$. The root node resides at the apex of the tree and has the address $0$. The root node then splits into $J$ different nodes, one level down the tree. The addresses for these $J$ new nodes are $1|0, 2|0, ..., J|0$. This type of naming convention continues as the rest of network is traversed. At its most general, each gating node can yield an arbitrary number of splits. While a fully generalized gating network is conceptually attractive, it presents practical challenges for implementation. In this paper we address several architectures for the gating network, each with its own set of structural restrictions on the shape of the network and the number of splits each gating node can take. For arbitrary gating node at address $a$, we use a multinomial logistic regression to model the split in direction $i$ to be:

\begin{equation} \label{eq:softmax}
  g^{a,i}_{t} \equiv g^{a,i}_{t}(\boldsymbol{z_{t}}, \boldsymbol{\omega}^{a}) = \frac{\exp(\boldsymbol{z_{t}} \  \bw{a,i})}{\sum^{J}_{j=1} {\exp(\boldsymbol{z_{t}} \ \bw{a, j})}}
\end{equation}

The parameters in equation (\ref{eq:softmax}) are subject to the usual identification restrictions. For the remainder of this essay, the choice is made to set $\boldsymbol{\omega}^{a,J} = \boldsymbol{0}$ for every gating node. It is important to keep track of the product path an input vector travels from one node to another. If the observation index is suppressed, the product path from one node (say the root node $0$) to another (say $k|\ldots|j|i$) can be defined as

\begin{equation} \label{eq:gpath}
  \gateprod{g^{0}}{g^{k|\ldots|j|i|0}} =
    \begin{cases} 
       g^{0, \, i} \, g^{i|0, \, j} \ldots g^{\dots|j|i|0, \, k} & \textrm{if path is feasible} \\
       0 & \textrm{otherwise}
    \end{cases}
\end{equation}

If one of the nodes is an expert, then we can define the mixture weight of expert $m$ for input pattern $i$ to be the product of the path taken from the root node to expert $m$:

\begin{equation} \label{eq:gpath2}
  \expmixwt(m | \boldsymbol{Z}, \boldsymbol{\Omega}) = \gateprod{g^{0}}{m}
\end{equation}

For network architectures with multiple paths from the root node to the same expert (see bottom right of figure (\ref{fig:network_comparison})), we can index these multiples paths by $l$ so that

\begin{equation} \label{eq:pathsums}
  \expmixwt(m | \boldsymbol{Z}, \boldsymbol{\Omega}) = \shortsum{l} \sumgateprod{g^{0}}{m}{l} 
\end{equation}

By collecting and summing all possible paths from the root node to each expert, the conditional probability given in equation (\ref{eq:mixture}) can be expanded and expressed as:

\begin{equation} \label{eq:contribution}
  \begin{split}
    P(y_{t}| \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \boldsymbol{\Omega}, \boldsymbol{\beta}) =& \sum_{m} \expmixwt(m | \boldsymbol{z}_{t}, \boldsymbol{\Omega}) P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m}) \\ 
      =& \sum_{m} \left( \shortsum{l} \sumgateprod{g^{0}_{t}}{m}{l} \right)  P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})
  \end{split}
\end{equation}

As it was just mentioned, summing over multiple paths $l$ in equation (\ref{eq:contribution}) is only necessary in the HMRE case. For the ME and HME cases, $l$ equals 1, reducing the mixture weight to Equation (\ref{eq:gpath2}). Going forward, this essay will concentrate on the ME and HME cases, leaving the exposition for the HMRE for another time.

\bigskip

If we concatenate the parameters of the gating network with the parameters of the experts as $\boldsymbol{\theta} = [\boldsymbol{\beta} \,\, \boldsymbol{\Omega}]$, then the product of these individual probabilities across the full sample size $T$ yields the likelihood function. 

\begin{equation} \label{eq:likelihood}
  \mathcal{L}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) = \prod_{t}  \sum_{m} \gateprod{g^{0}_{t}}{m}  P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})
\end{equation}

And taking its log yields the log likelihood

\begin{equation} \label{eq:loglikelihood}
  \boldsymbol{l}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) = \sum_{t} \log \left[ \sum_{m} \gateprod{g^{0}_{t}}{m} P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta^{m}}) \right]
\end{equation}

The functional form of the log likelihood (\ref{eq:loglikelihood}) does not lend itself easily to direct optimization, but a well established technique using expectation maximization \citep{EM_DLR1977} to estimate mixture models is available. This was the primary insight of \citet{JordanJacobs1993}'s original paper.


\section{EM Set-Up} \label{sec:Estimation}

The EM approach to estimating an HME model starts by suggesting that if a researcher had perfect information, each input vector $\boldsymbol{x}_{t}$ could be matched to the expert $P^{m}$ that generated it with certainty. If a set of indicator variables is introduced that captures this certainty, an \textit{augmented} version of the likelihood in equation (\ref{eq:likelihood}) can be put forward. Define the indicator set as:

\begin{equation} \label{eq:indicator}
  I_{t}(m) = \begin{cases} 
     1 & \textrm{if observation $t$ is generated by expert $m$} \\
     0 & \textrm{otherwise}
             \end{cases}
\end{equation}

We can then reformulate the likelihood equation

\begin{equation}  \label{eq:auglikelihood}
  \mathcal{L}_{c}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) = \prod_{t} \prod_{m} \left[ \gateprod{g^{0}}{m}  P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m}) \right]^{I_{t}(m)}
\end{equation}

leading to the complete-data log-likelihood

\begin{equation}  \label{eq:augloglikelihood}
  \boldsymbol{l}_{c}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) = \sum_{t} \sum_{m} I_{t}(m) \left[\log \gateprod{g^{0}}{m} + \log P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m}) \right]
\end{equation}


\subsection{E-Step} \label{sec:Estep}
The E-step of the algorithm performs an expectation over the complete log-likelihood equation (\ref{eq:augloglikelihood}), where the expectation includes the additional information contained in the expert regressions. One of the results of this expectation is the creation of second set of weights $h^{a}$ that parallel the weights from the gating network $g^{a}$ discussed in section (\ref{subsec:GatingNetwork}). For an HME model:

\begin{equation} \label{eq:Estep}
  \begin{split}
  Q(\boldsymbol{\theta}) = \mathbb{E} \left [ \boldsymbol{l}_{c}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \right] & = \sum_{t}\sum_{m} \EIm{m} \left[ \log P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m}) + \log \gateprod{g^{0}_{t}}{m} \right] \\ 
   & = \sum_{t} \sum_{m} \EIm{m}  \log P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})   +       \sum_{t} \sum_{a} \mathbb{E} \left[ I_{t}(a) \right] \log \gateprod{g^{0}_{t}}{a} \\
   & = \sum_{t} \sum_{m} \gateprod{h^{0}_{t}}{m}  \log P^{m}(y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})   +       \sum_{t} \sum_{a} \gateprod{h^{0}_{t}}{a} \log \gateprod{g^{0}_{t}}{a} \\
   & = \sum_{t} Q^{(1)}_{t} (\boldsymbol{\beta}) + \sum_{t} Q^{(2)}_{t} (\boldsymbol{\Omega}) \\
   & = \sum_{t} Q_{t}(\boldsymbol{\theta})
 \end{split}
\end{equation}

Here $\gateprod{h^{0}_{t}}{k,\dots|j|i|0}$ is analogous to equation (\ref{eq:gpath})

\begin{equation} \label{eq:hpath}
  \gateprod{h^{0}_{t}}{k|\ldots|j|i|0} =
    \begin{cases} 
       h^{0, \, i}_{t} \ h^{i|0, \, j}_{t} \ldots h^{\dots|j|i|0, \, k}_{t} & \textrm{if path is feasible} \\
       0 & \textrm{otherwise}
    \end{cases}
\end{equation}

and the $h^{a, i}_{t}$ are arrived at using Bayes' theorem.

\begin{equation} \label{eq:posteriornode}
  \h{a,i}{t} = \frac{g^{a, i}_{t} \shortsum{m} P^{m}_{t} \gateprod{g^{i|a}_{t}}{m}}{\shortsum{j} g^{a, j}_{t} \shortsum{k} P^{k}_{t} \gateprod{g^{j|a}_{t}}{k}}
\end{equation}


So, now we have two different forms of weights, $g$'s and $h$'s. The way the $g$'s are formed in equation (\ref{eq:softmax}), they are only functions of the nodes in the gating network, separate from the expert regressions and the information they contain. For this reason, \citet{JordanJacobs1993} refer to $g$'s as \textit{priors}. The $h$'s draw from both the gating network and the expert regressions and are referred to as \textit{posterior} weights.


\subsection{M-Step} \label{sec:Mstep}

One of the more attractive features of using EM to a optimize a HME is how the log-likelihood function compartmentalizes into a set of independent functions which can be individually optimized. After taking the expectation of the log-likelihood function (\ref{eq:Estep}), the parameters governing each expert and each gating network can be grouped together and optimized on their own. For the experts we have:

\begin{equation} \label{eq:BetaUpdate}
  \argmax_{\boldsymbol{\beta^{m}}} \sum_{t} \gateprod{h^{0}_{t}}{m} \log P^{m} (y_{t}| \boldsymbol{x}_{t}; \boldsymbol{\beta}^{m})
\end{equation}

and for the gating nodes:

\begin{equation} \label{eq:OmegaUpdate}
  \argmax_{\boldsymbol{\omega^{a}}} \sum_{t} \gateprod{h^{0}_{t}}{a} \log g( \boldsymbol{z}_{t}, \boldsymbol{\omega}^{a})
\end{equation}

It is worth noting that the weights in these optimizations $\gateprod{h^{0}_{t}}{h^{a}_{t}}$ are provided to the M-step by the E-step and should be considered constant values.

\subsection{The EM-Algorithm}

The EM algorithm iterates back-and-forth between the E-step and the M-step. Given the data ($\boldsymbol{y}_{t}$, $\boldsymbol{X}_{t}$, $\boldsymbol{Z}_{t}$) and the current set of parameters $\boldsymbol{\theta}^{k}$, the expected value of the complete log-likelihood (eq. (\ref{eq:augloglikelihood})) is found, resulting in the deterministic function Q($\boldsymbol{\theta}^{k}$). In essence, the main objective of the E-step is to derive the values of the posterior weights ($\h{a,i}{t}$) using equations (\ref{eq:ConditionalDistribution}), (\ref{eq:softmax}), (\ref{eq:gpath}), (\ref{eq:hpath}) and (\ref{eq:posteriornode}). Once the posterior weights have been calculated in the E-step, the M-step holds them constant and then re-estimates the parameter vector:

\begin{equation}
    \boldsymbol{\theta}^{k + 1}  = \argmax_{\boldsymbol{\theta}} Q(\hat{\boldsymbol{\theta}}^{k}) = \left[ \argmax_{\boldsymbol{\beta}} Q^{(1)} ( \hat{\boldsymbol{\beta}}^{k} ) \,\,\,\, \argmax_{\boldsymbol{\Omega}} Q^{(2)} ( \hat{\boldsymbol{\Omega}}^{k} ) \right]
\end{equation}

Again, due to the separable nature of $Q$ (see the middle equality of eq (\ref{eq:Estep})), the parameters of each expert regression and each gating node can be updated one-at-a-time with equations (\ref{eq:BetaUpdate}) and (\ref{eq:OmegaUpdate}), respectively. The separability of the Q function -- when applied to finite mixture -- was noticed in the original and excellent work of \citet{EM_DLR1977}. See Section 4.3 for the authors' example. From a computational perspective, this set-up has the additional benefit of being embarrassingly parallel, making it easier to scale to larger and larger data sets.

\bigskip

It is worth mentioning a few more of the remarkable properties of the EM algorithm that are established in \citet{EM_DLR1977}:

\begin{enumerate}
  \item Given a sequence of parameter values produced by the General EM algorithm,
  $\boldsymbol{\theta}^{k} \rightarrow \boldsymbol{\theta}^{k+1} \rightarrow \ldots \rightarrow \boldsymbol{\theta}^{k+n}$,
  the sequence of values are non-decreasing in their log-likelihood values
  $\boldsymbol{l}(\boldsymbol{\theta}^{k}| \cdot) \leq \boldsymbol{l}(\boldsymbol{\theta}^{k+1}| \cdot) \leq \ldots \leq \boldsymbol{l}(\boldsymbol{\theta}^{k+n}| \cdot)$
  and are strictly increasing in the Q function $Q(\boldsymbol{\theta}^{k}) < Q(\boldsymbol{\theta}^{k+1}) < \ldots < Q(\boldsymbol{\theta}^{k+n})$.

  \item The sequence of parameter values produced by the General EM algorithm
  converges to a fixed point such that in the limit:
    \begin{equation}
      \boldsymbol{\theta}^{*}  = \argmax_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}^{*})
    \end{equation}
  Crucially, the vector that the general EM algorithm converges
  to is a maximum likelihood estimator of the original log-likelihood equation
  defined in (\ref{eq:loglikelihood}). That is, 
  $\boldsymbol{l}(\boldsymbol{\theta}^{*}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \geq \boldsymbol{l}(\boldsymbol{\theta}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z})$
  for all $\boldsymbol{\theta} \in \boldsymbol{\Theta}$.
\end{enumerate}


\section{Inference} \label{sec:Inference}

When considering inference, it is worth thinking about what would motivate a researcher to turn to an HME model in the first place. At times, a researcher may suspect that a latent structure exists within the data and that a single regression $y_{t} = \boldsymbol{x}_{t} \boldsymbol{\beta}$ may mask a critical change in relationship depending on membership to some unknown sub-group $m$ of the data $y_{t,m} = \boldsymbol{x}_{t} \boldsymbol{\beta}^{m}$. A wide variety of time series, especially those with longer histories, experience changes in behavior over time. They can be subjected to sharp one-off structural breaks or the changes can be more gradual changes over time. Regardless of the context, any latent structural change in the data generating process may also introduce some hidden form of heterogeneity to the error terms. Rather than taking a firm stance on any concealed structure, an HME setup ideally limits the work the researcher needs to do to specifying a set of well-chosen conditioning variables $\boldsymbol{Z}$ to feed through the gating network. This limited workload may come at a cost, though. By allowing the gating network to find its own mixture allocations, the odds of arriving at a misspecified model becomes a concern. To guard against this, we use a sandwich estimator for the variance-covariance matrix:

\begin{equation} \label{eq:robustgatevarcov}
  \boldsymbol{V}(\boldsymbol{\theta}) = \HI{\boldsymbol{\theta}} \boldsymbol{G}(\boldsymbol{\theta}) \HI{\boldsymbol{\theta}}
\end{equation}

where $\boldsymbol{G}(\boldsymbol{\theta})$ is the sum of the outer products of the score vectors

\begin{equation} \label{eq:OPG}
  \boldsymbol{G}(\boldsymbol{\theta}) = \sum_{t} \boldsymbol{S}_{t}(\boldsymbol{\theta}) \boldsymbol{S}_{t}(\boldsymbol{\theta})^\top
\end{equation}

and $\HH{\boldsymbol{\theta}}$ is the empirical Hessian:

\begin{equation} \label{eq:Hessian}
  \HH{\boldsymbol{\theta}} = \sum_{t} \Ht{\boldsymbol{\theta}}
\end{equation}

The following sections discusses how to form the score and hessian matrices for the log-likelihood described in equation (\ref{eq:loglikelihood}).

\subsection{The Score} \label{sec:TheScore}

The notation is tedious but the acyclic nature of the gating network makes interpretation of the score vectors clear and straightforward. The full score vector is the concatenated scores of each gating node and those of each local expert.

\begin{equation}
  \boldsymbol{S}_{t}(\boldsymbol{\theta}) = [ \boldsymbol{S}_{t}(\boldsymbol{\beta})^{\top} \,\, \boldsymbol{S}_{t}(\boldsymbol{\Omega})^{\top} ]^{\top}
\end{equation}

Starting with parameters of the gating network, they can be partitioned in some logical order into the sub-vectors of each node's individual score.

\begin{equation}
  \boldsymbol{S}_{t}(\boldsymbol{\Omega}) = [ \boldsymbol{S}_{t}(\boldsymbol{\omega}^{0})^{\top} \,\, \boldsymbol{S}_{t}(\boldsymbol{\omega}^{1 | 0})^{\top} \,\, \boldsymbol{S}_{t}(\boldsymbol{\omega}^{2 | 0})^{\top} \,\, \ldots \, ]^{\top}
\end{equation}

\begin{equation}
  \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a}) = [ \boldsymbol{S}_{t}(\bw{a, 1})^{\top} \,\, \ldots \,\, \boldsymbol{S}_{t}(\bw{a, J - 1})^{\top} ]^{\top}
\end{equation}

In what follows, the functions $M(a)$ and $M(a, i)$ will be used to return a subset of experts from a general HME model. The function $M(a)$ will return the set of all experts that are ancestors of node $a$, while $M(a, i)$ returns the set of experts that are ancestors from branch $i$ of node $a$. For instance, in network $\boldsymbol{C}$ of Figure \ref{fig:network_comparison}, $M('1|0') = \{'1|1|1|0', \, '2|1|1|0', \, '2|1|0'\}$, $M('1|0', 1) = \{'1|1|1|0', \, '2|1|1|0'\}$, and $M('1|0', 2) = \{'2|1|0'\}$. For a generic gating node $a$ we can define the individual score for sample $t$ as:

\begin{equation} \label{eq:gateScore}
  \boldsymbol{S}_{t}(\bw{a, i}) = \frac{\partial \boldsymbol{l}_{t}(\boldsymbol{\theta}^{*}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) }{\partial \boldsymbol{\omega}^{a,i}} = \left[ \frac{ \FnOmegaNaught{a}{i} }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} } \right] \boldsymbol{z}_{t}^{\top} 
\end{equation}

with

\begin{equation} \label{eq:omegaScore}
  \FnOmegaNaught{a}{i} = \left( (1 - g^{a, i}_{t}) \sum_{m}^{M(a, i)} \gateprod{g^{0}_{t}}{m} P^{m}_{t} - \sum_{j \neq i} g^{a, j}_{t} \sum_{m'}^{M(a, j)} \gateprod{g^{0}_{t}}{m'}  P^{m'}_{t} \right)
\end{equation}

In expression (\ref{eq:gateScore}) above, $ \FnOmegaNaught{a}{i} \left[ \sum_{k} \gateprod{g^{0}_{t}}{P^{k}_{t}} P^{k}_{t}\right]^{-1}$ is the instantaneous rate of change of the $t^{\mathrm{th}}$ contribution to the log-likelihood caused by a small perturbation of $\bw{a, i}$. At the maximum likelihood estimator $ \theta^{*}$, the sum of (\ref{eq:omegaScore}) over the full sample should be approximately zero. This implies that the optimal $\bw{a, i}$ balances any gain of moving more weight to the set of experts that can be reached by taking direction $i$ at node $a$ against the loss suffered by removing weight from the experts at the end of any path $j$ that does not equal $i$.

\bigskip

Turning our attention to the expert regressions, the exact functional form of the score vector depends on the type of regression we wish to run. In most cases, all experts in an HME model are from the same family (\citet{HuertaJiangTanner2003} is a notable exception). When all experts share the same functional form, it is standard to accept the restriction that no experts in the HME model produce the same parameter vector $\boldsymbol{\beta}^{m} \neq \boldsymbol{\beta}^{k}$. Such an HME is defined by \citet{JiangTanner2000} as being \textit{irreducible}. The irreducibility of an HME plays a critical role in guaranteeing the convergence of the model. In this essay, each HME discussed will employ a set of experts running a standard linear regression model with Gaussian errors. To aid with model optimization, the specification of the parameter vector for each regression,  $\boldsymbol{\beta}^{m} = [\beta_{0}^{m} \,\, \ldots \,\, \beta_{k}^m \,\, \phi^{m}]^{\top}$, takes on a unique form where we model the log variance explicitly: $\phi = \log \sigma^{2}$.

\begin{equation}
  P^{m}(y_{t} | \boldsymbol{x}_{t}; \, \boldsymbol{\beta}^{m}, \phi^{m}) = \left( 2 \pi \exp ( \phi^{m} ) \right)^{-\frac{1}{2}} \exp{ \left( -\frac{  ( y_{t} - \boldsymbol{x}_{t} \boldsymbol{\beta}^{m} )^{2}  }{2 \exp (\phi^{m}) } \right) }
\end{equation}

To help save space in the sections below, the following shorthand will be used to denote the residual of each local expert: $\epsilon^{m}_{t} = y_{t} - \boldsymbol{x}_{t} \boldsymbol{\beta}^{m}$. Beginning with the original log-likelihood equation defined in Equation (\ref{eq:loglikelihood}), and noting that there is only one path from root node to each expert in an HME ($l = 1$), the score vector for all expert regressions can be expressed as:

\begin{equation}
  \boldsymbol{S}_{t}(\boldsymbol{\beta}) = [ \boldsymbol{S}_{t}(\boldsymbol{\beta}^{1})^{\top} \,\, \ldots \,\, \boldsymbol{S}_{t}(\boldsymbol{\beta}^{M})^{\top} ]^{\top}
\end{equation}

\begin{equation} \label{eq:expertScore}
  \boldsymbol{S}_{t}(\boldsymbol{\beta^{m}}) = \left[ \frac{\partial \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m}}^{\top} \,\,\,\, \frac{\partial \boldsymbol{l}_{t}}{\partial \phi^{m}} \right]^{\top}
\end{equation}

with

\begin{align} \label{eq:ExpertScoreBeta}
  \frac{\partial \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m}} =& \left[ \frac{\gateprod{g^{0}_{t}}{m}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left( 2 \pi \exp ( \phi^{m} ) \right)^{-\frac{1}{2}} \exp{ \left( -\frac{  (\epsilon^{m})^{2}  }{2 \exp (\phi^{m}) } \right) } \left( - \frac{\epsilon^{m}}{\exp(\phi^{m})} \right) (-\boldsymbol{x}_{t}^{\top}) \nonumber \\
   =& \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{\epsilon^{m}}{\exp(\phi^{m})} \right] \boldsymbol{x}_{t}^{\top}
\end{align}

and

\begin{align} \label{eq:ExpertScoreVariance}
  \frac{\partial \boldsymbol{l}_{t}}{\partial \phi^{m}} =& \left[ \frac{ \gateprod{g^{0}_{t}}{m} }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \bigg[ -\frac{1}{2} (2 \pi)^{-\frac{1}{2}} \left( \exp ( \phi^{m} ) \right)^{-\frac{3}{2}} \exp ( \phi^{m} ) \exp{ \left( -\frac{  ( y_{t} - \boldsymbol{x}_{t} \boldsymbol{\beta}^{m} )^{2}  }{2 \exp (\phi^{m}) } \right) }   + \nonumber \\
   & \left( 2 \pi \exp ( \phi^{m} ) \right)^{-\frac{1}{2}} \exp{ \left( -\frac{  (\epsilon^{m}_{t})^{2}  }{2 \exp (\phi^{m}) } \right) } \left( \frac{ ( \epsilon^{m}_{t} )^{2} }{ 2 \exp( \phi^{m} )^{2} } \right) \exp( \phi^{m} ) \bigg] \nonumber \\
   =& \frac{1}{2} \left[ \frac{ \gateprod{g^{0}_{t}}{m} P^{m}_{t} }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{ (\epsilon^{m}_{t})^{2} }{ \exp( \phi^{m} )} - 1 \right]
\end{align}


Expressions (\ref{eq:ExpertScoreBeta}) and (\ref{eq:ExpertScoreVariance}) are the same as the score vectors for a single (non-logged) OLS regression but with an appended term representing expert $m$'s portion of the total contribution to the likelihood for that observation.


\subsection{The Hessian} \label{sec:TheHessian}

The hessian, admittedly, has a complicated form. At its most general it can be written as $\Ht{\boldsymbol{\theta}}$ in the equation below. The exact nature of the full hessian depends critically on the structure of the gating network and the locations of the gate and expert nodes in relation to each other.

\begin{equation} \label{eq:FullHessian}
  \Ht{\boldsymbol{\theta}} =  \begin{bmatrix}
  \Ht{\boldsymbol{\beta}^{1}, \boldsymbol{\beta}^{1}}                       & \Ht{\boldsymbol{\beta}^{2}, \boldsymbol{\beta}^{1}}                      & \cdots & \Ht{ \boldsymbol{\beta}^{1}, \,  \bw{0, 1} }                & \cdots & \Ht{ \boldsymbol{\beta}^{1}, \,  \bw{\boldsymbol{\cdot}, J - 1} }     \\
  \Ht{\boldsymbol{\beta}^{1}, \boldsymbol{\beta}^{2}}                       & \Ht{\boldsymbol{\beta}^{2}, \boldsymbol{\beta}^{2}}                      & \cdots & \Ht{\boldsymbol{\beta}^{2}, \, \bw{0, 1}}                   & \cdots & \Ht{\boldsymbol{\beta}^{2}, \, \bw{\boldsymbol{\cdot}, J - 1}}        \\
  \vdots                                                                    & \vdots                                                                   & \ddots & \vdots                                                      &        & \vdots                                                                \\
  \Ht{ \boldsymbol{\beta}^{1}, \,  \bw{0, 1} }^{\top}                       & \Ht{ \boldsymbol{\beta}^{2}, \,  \bw{0, 1} }^{\top}                      & \cdots & \Ht{\bw{0, 1}, \bw{0, 1}}                                   & \cdots & \Ht{\bw{0, 1}, \, \bw{\boldsymbol{\cdot}, J - 1} }                    \\
  \vdots                                                                    & \vdots                                                                   &        & \vdots                                                      & \ddots & \vdots                                                                \\
  \Ht{ \boldsymbol{\beta}^{1}, \,  \bw{\boldsymbol{\cdot}, J - 1} }^{\top}  & \Ht{ \boldsymbol{\beta}^{2}, \,  \bw{\boldsymbol{\cdot}, J - 1} }^{\top} & \cdots & \Ht{ \bw{0, 1}, \,  \bw{\boldsymbol{\cdot}, J - 1} }^{\top} & \cdots & \Ht{ \bw{\boldsymbol{\cdot}, J - 1}, \bw{\boldsymbol{\cdot}, J - 1}}  \\
    \end{bmatrix}
\end{equation}

Each block of the hessian will be non-zero. Looking at the score vectors in Equations (\ref{eq:gateScore}), (\ref{eq:ExpertScoreBeta}), and (\ref{eq:ExpertScoreVariance}), each contains the expression $\left[ \sum_{m} \gateprod{g^{0}_{t}}{P^{m}_{t}} P^{m}_{t} \right]^{-1}$. This term holds the parameters for every gate, split, and local expert in the model. The basic details for a generic block of the hessian can be described by the following three expressions:

\begin{align}
  \Ht{\boldsymbol{\beta}^{m}, \boldsymbol{\beta}^{n}} &= \begin{bmatrix} \label{eq:expertNodeHessian}
    \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \boldsymbol{\beta}^{n}}    &  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \phi^{n}}     \\
    \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \phi^{n}}^{\top}           &  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \phi^{m} \partial \phi^{n}}
    \end{bmatrix} \\
    \Ht{\boldsymbol{\beta}^{m}, \, \bw{a, i}} &= \left[ \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \bw{a, i}}  \frac{\partial^{2} \boldsymbol{l}_{t}}{ \partial \phi^{m} \partial \bw{a, i} }  \right] \\
    \Ht{\bw{a, i}, \, \bw{b, n}} &= \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\omega}^{a, i} \partial \boldsymbol{\omega}^{b,n}}
\end{align}

\bigskip

The expressions for the cross-partial derivatives between a gating parameter vector and an expert parameter vector can differ based on the relative position between $\bw{a, i}$ and $\boldsymbol{\beta}^{m}$ in the HME structure. For instance, start at the root node and consider what path is needed to traverse the network to expert $m$. When arriving at node $a$ (which is on the path to expert $m$), if the direction needed to take to reach expert $m$ is along branch $i$, then $\bw{a,i}$ will be called an \textit{explicit} parameter vector with respect to expert $m$. If taking branch $i$ leads to a different expert than $m$, then $\bw{a,i}$ will be referred to as an \textit{implicit} parameter vector. Now, define $\mathbb{1}\{a, i, m\}$ as an indicator function that equals one if $\bw{a, i}$ is an explicit parameter vector to expert $m$ and zero if it is an implicit parameter vector (it can only be one or the other). With this notation, the details to the hessian in equation (\ref{eq:FullHessian}) can now be tackled.

\bigskip

Starting with equation (\ref{eq:gateScore}), the second-order partial derivatives for a pair of gating vectors is:

\begin{equation} \label{eq:nodehessian}
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\omega}^{a, i} \partial \boldsymbol{\omega}^{b,n}} = - \left[ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} \right]^{-2} \FnOmegaNaught{a}{i} \cdot \FnOmegaNaught{b}{n} \, \boldsymbol{z}^{\top}_{t} \boldsymbol{z}_{t} \, + \, \left[ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}\right]^{-1} \boldsymbol{z}^{\top}_{t} \, \frac{ \partial \FnOmegaNaught{a}{i} }{\partial \bw{b, n}}^{\top}
\end{equation}

where the value $\frac{ \partial \FnOmegaNaught{a}{i} }{\partial \bw{b, n}}$ depends on the relative locations of $\bw{a, i}$ and $\bw{b, n}$:

\begin{equation}
  \frac{ \partial \FnOmegaNaught{a}{i} }{\partial \boldsymbol{\omega}^{b, n}} = \begin{cases} 
       0 & \textrm{if $M(a) \cap M(b) = \{ \}$} \\
       \FnOmegaOne{a}{i}{b}{n}  \, \boldsymbol{z}_{t}^{\top} & \textrm{if $M(a) \cap M(b) \neq \{ \}$ and $a \neq b$} \\
       \left[ \FnOmegaOne{a}{i}{a}{n} + \FnOmegaTwo{a}{i}{n} \right] \, \boldsymbol{z}_{t}^{\top} & \textrm{if $M(a) \cap M(b) \neq \{ \}$ and $a = b$}
    \end{cases}
\end{equation}

and the terms $\Omega^{(1)}_{t}$ and $\Omega^{(2)}_{t}$ are defined by:

\begin{equation}
  \FnOmegaOne{a}{i}{b}{n} = \sum_{m}^{ M(a) \, \cap \, M(b) } \left( \mathbb{1}\{a, i, m\} - g^{a,i}_{t} \right) \left( \mathbb{1}\{b, n, m\} - g^{b,n}_{t} \right) \gateprod{g^{0}_{t}}{m} P^{m}_{t}
\end{equation}

\begin{equation}
  \FnOmegaTwo{a}{i}{n} = - \sum_{m}^{M(a)} g^{a, i}_{t} ( \mathbb{1}\{i = n\} - g^{a, n}_{t} ) \gateprod{g^{0}_{t}}{m} P^{m}_{t}
\end{equation}

The cross-partial derivatives for a general gating node and an expert regression depends on their relative location. If expert $m \in M(a)$, then:

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \boldsymbol{\omega}^{a, i}} =  \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \left( \mathbb{1}\{a, i, m\} - g^{a, i}_{t} \right)  - \frac{ \Omega^{(0)}_{t}(a, i) }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} }  \right] \left[ \frac{ (\epsilon^{m}_{t}) }{\exp(\phi^{m})} \right] \boldsymbol{x}_{t}^{\top} \boldsymbol{z}_{t}  
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \phi^{m} \partial \boldsymbol{\omega}^{a, i}} =  \frac{1}{2} \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \left( \mathbb{1}\{a, i, m\} - g^{a, i}_{t} \right)  - \frac{ \Omega^{(0)}_{t}(a, i) }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} }  \right] \left[ \frac{ (\epsilon^{m}_{t})^{2} }{\exp(\phi^{m})} - 1 \right] \boldsymbol{z}_{t}^{\top} 
\end{equation}

And if $m \not\in M(a)$, then:

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \boldsymbol{\omega}^{a, i}} =  - \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \frac{ \Omega^{(0)}_{t}(a, i) }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} }  \right] \left[ \frac{ (\epsilon^{m}_{t}) }{\exp(\phi^{m})} \right] \boldsymbol{x}_{t}^{\top} \boldsymbol{z}_{t}  
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \phi^{m} \partial \boldsymbol{\omega}^{a, i}} =  - \frac{1}{2} \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \frac{ \Omega^{(0)}_{t}(a, i) }{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t} }  \right] \left[ \frac{ (\epsilon^{m}_{t})^{2} }{\exp(\phi^{m})} - 1 \right] \boldsymbol{z}_{t}^{\top} 
\end{equation}

Starting from equations (\ref{eq:ExpertScoreBeta}) and (\ref{eq:ExpertScoreVariance}), the next set of equations express the second-order partial derivatives for parameters of the same individual expert:

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial (\boldsymbol{\beta}^{m})^{2}} =  \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \left( \frac{ \epsilon^{m}_{t} }{ \exp( \phi^{m}) } \right)^{2} \left( 1 - \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right) - \frac{1}{ \exp( \phi^{m} ) }  \right] \boldsymbol{x}_{t}^{\top} \boldsymbol{x}_{t}
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \phi^{m}} =  \frac{1}{2} \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{\epsilon^{m}_{t}}{\exp(\phi^{m})} \right] \left[ \left( \frac{ ( \epsilon^{m}_{t} )^{2} }{ \exp( \phi^{m}) } - 1 \right) \left( 1 - \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right) - 2  \right] \boldsymbol{x}_{t}^{\top}
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial (\phi^{m})^{2}} =  \frac{1}{4} \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \left( \frac{ ( \epsilon^{m}_{t} )^{2} }{ \exp( \phi^{m}) } - 1 \right) \left( 1 - \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right) - \frac{ 2 (\epsilon^{m}_{t})^{2} }{ \exp(\phi^{m}) }  \right] 
\end{equation}

Finally, the set of equations for the second-order partial derivatives for parameters of two separate experts:

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \boldsymbol{\beta}^{n} } = - \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{\gateprod{g^{0}_{t}}{n} P^{n}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \frac{\epsilon^{m}_{t}}{\exp(\phi^{m})} \right] \left[ \frac{\epsilon^{n}_{t}}{\exp(\phi^{n})} \right] \boldsymbol{x}_{t}^{\top} \boldsymbol{x}_{t} 
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\beta}^{m} \partial \boldsymbol{\phi}^{n} } = - \frac{1}{2}  \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{\gateprod{g^{0}_{t}}{n} P^{n}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \frac{\epsilon^{m}_{t}}{\exp(\phi^{m})} \right] \left[  \frac{ (\epsilon^{n}_{t})^{2} }{\exp(\phi^{n})} - 1 \right] \boldsymbol{x}_{t}^{\top} 
\end{equation}

\begin{equation} 
  \frac{\partial^{2} \boldsymbol{l}_{t}}{\partial \boldsymbol{\phi}^{m} \partial \boldsymbol{\phi}^{n} } =  -\frac{1}{4} \left[ \frac{\gateprod{g^{0}_{t}}{m} P^{m}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right] \left[ \frac{\gateprod{g^{0}_{t}}{n} P^{n}_{t}}{ \sum_{k} \gateprod{g^{0}_{t}}{k} P^{k}_{t}} \right]  \left[ \frac{ (\epsilon^{m}_{t})^{2} }{\exp(\phi^{m})} - 1 \right] \left[ \frac{ (\epsilon^{n}_{t})^{2} }{\exp(\phi^{n})} - 1 \right] 
\end{equation}


\section{Marginal Effects} \label{sec:MarginalEffects}

Due to the complexity of the model's structure and the ability to place covariates in either the gating network, the expert regressions, or both, viewing the relationship between the covariates and the dependent variable through their marginal effects may provide a simplifying lens of the model's governing principles. Just as for logistic and multinomial regression, the marginal effects of an HME model have a closed form solution. Starting with equation (\ref{eq:mixture}) we replace the expert distributions $P^{m}_{t}$ with the expected output for each of the $m$ regressions and use the relationship in equation (\ref{eq:gpath2}) to arrive at: 

\begin{equation} \label{eq:mixture2}
  \mathbb{E} \left[ y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}, \boldsymbol{\theta} \right] = \sum_{m=1}^{M} \gateprod{g^{0}_{t}}{m} \mathbb{E} \left[ y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}, \boldsymbol{\theta}, m \right]
\end{equation}

In what follows, $\mathbb{E} \left[y_{t}\right]$ and $\Eym$ will be used as shorthand for $\mathbb{E} \left[ y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}, \boldsymbol{\theta} \right]$ and $\mathbb{E} \left[ y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}, \boldsymbol{\theta}, m \right]$, respectively. The functional form of the marginal effect depends on where the variables appear in the model. Our existing notation labels the covariates in gating network as $\boldsymbol{Z}$ and the covariates in the expert regressions as $\boldsymbol{X}$. As seen later, the variables belonging to $\boldsymbol{Z}$ and $\boldsymbol{X}$ do not need to be mutually exclusive. There is also no requirement that they differ at all. In light of this, a few more notational definitions are needed to cover all the cases:

\begin{itemize}  
  \item $\boldsymbol{T} = \boldsymbol{Z} \cup \boldsymbol{X}$
  \item $\boldsymbol{V} = \boldsymbol{Z} \cap \boldsymbol{X}$
  \item $\boldsymbol{U}_{Z} = \boldsymbol{Z} \setminus \boldsymbol{X}$
  \item $\boldsymbol{U}_{X} = \boldsymbol{X} \setminus \boldsymbol{Z}$
\end{itemize}


The full list of variables considered in the model is labeled $\boldsymbol{T}$. Covariates that appear in both the gating network and the expert regressions are collected in $\boldsymbol{V}$. $\boldsymbol{U}_{Z}$ and $\boldsymbol{U}_{X}$ are used to label variables that appear only in the gating network or only in the expert regressions, respectively. With this notation, we can express the full marginal effects of the HME by where the explanatory variables appear in the model.

\begin{equation} \label{eq:hme_marginal_effect_def}
  \frac{\partial \mathbb{E} \left[y_{t}\right]}{\partial \boldsymbol{T}} \equiv \boldsymbol{\Delta}_{t} = \sum_{m=1}^{M} \boldsymbol{\Delta}^{m}_{t} = \sum_{m=1}^{M} \left[ \frac{\partial \Eym}{\partial \boldsymbol{U}_{Z}}   \quad   \frac{\partial \Eym}{\partial \boldsymbol{V}}   \quad   \frac{\partial \Eym}{\partial \boldsymbol{U}_{X}}   \right]
\end{equation}

with the functional form of the each covariate group in (\ref{eq:hme_marginal_effect_def}) defined as:

\begin{equation} \label{eq:ME_gating}
  \frac{\partial \Eym}{\partial \boldsymbol{U}_{Z}} = \frac{\partial \gateprod{g^{0}_{t}}{m}}{{\partial \boldsymbol{U}_{Z}}} \Eym
\end{equation}


\begin{equation} \label{eq:ME_expert}
  \frac{\partial \Eym}{\partial \boldsymbol{U}_{X}} = \gateprod{g^{0}_{t}}{m} \frac{\partial \Eym}{{\partial \boldsymbol{U}_{X}}}
\end{equation}


\begin{equation} \label{eq:ME_both}
  \frac{\partial \Eym}{\partial \boldsymbol{V}} = \frac{\partial \gateprod{g^{0}_{t}}{m}}{{\partial \boldsymbol{V}}} \Eym + \gateprod{g^{0}_{t}}{m} \frac{\partial \Eym}{{\partial \boldsymbol{V}}}
\end{equation}

Not matter how complex the model becomes, the researcher can always interpret the estimated HME through a single vector of marginal effects of $\boldsymbol{T}$. Of the four components in equations (\ref{eq:ME_gating}) - (\ref{eq:ME_both}), three have already been established: $\Eym$ is the output from local expert $m$, $\gateprod{g^{0}_{t}}{m_{t}}$ is the prior weight for input $t$ for local expert $m$, and $\frac{\partial \Eym}{{\partial \boldsymbol{X}}}$ is the marginal effect of the local expert $m$ with respect to covariates $\boldsymbol{X}$. What is left is the partial derivative of the gating network with respect to a variable in that network $\frac{\partial \gateprod{g^{0}_{t}}{m}}{\partial \boldsymbol{z}_{t}}$. Starting with equation (\ref{eq:gpath}), we take the partial with respect to parameters in the gating matrix:

\begin{equation} \label{eq:hme_gate_marginal_effect_1}
  \boldsymbol{\delta}^{m}_{t} \equiv \frac{\partial \gateprod{g^{0}_{t}}{m}}{\partial \boldsymbol{z}_{t}} = \frac{\partial g^{0, i}_{t} g^{i|0, j}_{t} \cdots g^{k|\cdots|j|i|0, m}_{t}}{\partial \boldsymbol{z}_{t}}
\end{equation}

Applying the product rule yields:

\begin{equation} \label{eq:hme_gate_marginal_effect_2}
  \begin{split}
    \boldsymbol{\delta}^{m}_{t} &= \frac{\partial g^{0, i}_{t}}{\partial \boldsymbol{z}_{t}} g^{i|0, j}_{t} \cdots g^{k|\cdots|j|i|0, m}_{t}                       \\
                                     &+ g^{0, i}_{t} \frac{\partial g^{i|0, j}_{t}}{\partial \boldsymbol{z}_{t}} \cdots g^{k|\cdots|j|i|0, m}_{t} \\
                                     &+ \dots                                                                                     \\
                                     &+ g^{0, i}_{t} g^{i|0, j}_{t} \cdots \frac{\partial g^{k|\cdots|j|i|0, m}_{t}}{\partial \boldsymbol{z}_{t}} \\
  \end{split}
\end{equation}

Since

\begin{equation} \label{eq:node_marginal_effect}
  \frac{\partial g^{a, i}_{t}}{\partial \boldsymbol{z}_{t}} = g^{a, i}_{t} \left( \bw{a, i} - \sum_{j} g^{a, j}_{t} \bw{a, j} \right)^{\top} = g^{a, i}_{t} \left( \bw{a, i} - \mean{\boldsymbol{\omega}}^{a} \right)^{\top}
\end{equation}

we can substitute equation (\ref{eq:node_marginal_effect}) into (\ref{eq:hme_gate_marginal_effect_2}) to arrive at:

\begin{equation} \label{eq:marginal_effects}
  \begin{split}
    \boldsymbol{\delta}^{m}_{t} &= \gateprod{g^{0}_{t}}{m} \left(\boldsymbol{\omega}^{0, i} + \boldsymbol{\omega}^{i|0, j} + \cdots + \boldsymbol{\omega}^{k|\cdots|j|i|0, m} - \left( \mean{\boldsymbol{\omega}}^{0} + \mean{\boldsymbol{\omega}}^{i|0} + \cdots + \mean{\boldsymbol{\omega}}^{k|\cdots|j|i|0} \right) \right)^{\top} \\
                                &= \gateprod{g^{0}_{t}}{m} ( \boldsymbol{W}^{m} )^{\top}
  \end{split}
\end{equation}

Looking closely at equation (\ref{eq:marginal_effects}), the instantaneous rate of change of $\gateprod{g^{0}_{t}}{m}$ to small deviations of $\boldsymbol{z}_{t}$ has an interesting representation. The row vector $( \boldsymbol{W}^{m} )^{\top}$ mean differences the parameter values of each edge in the path from the root node to expert $m$. This path is the \textit{only} path from the root node to expert $m$. The sum of the mean parameter deviations are then appropriately weighted by the prior gate path $\gateprod{g^{0}}{m}$.


\subsection{Delta Method}

Using the delta method, we can approximate standard errors for the marginal effects of the HME model. Starting with equation (\ref{eq:hme_marginal_effect_def}) from the previous section, we break down the gradient of the marginal effects with respect to the parameters by those in the gating network, $\boldsymbol{\Omega}$, and the parameters in the expert regressions, $\boldsymbol{\beta}$. These results are collected in Table \ref{tbl:delta_method_gradients}.

 \bigskip

\begin{table}
  \begin{center}
    \begin{tabular}{| l | c c c |}
    \hline
                                                                                    & \underline{$\boldsymbol{U}_{Z}$}                                                            & \underline{$\boldsymbol{V}$}                                                                                                                                                                                           & \underline{$\boldsymbol{U}_{X}$}   \\ [2ex]
    $\frac{\partial \boldsymbol{\Delta}_{t}^{m}}{\partial \boldsymbol{\omega}^{a}}$ & $\frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \boldsymbol{\omega}^{a}} \Eym$        & $\frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \boldsymbol{\omega}^{a}} \Eym + \frac{\partial \gateprod{g^{0}_{t}}{m}}{\partial \boldsymbol{\omega}^{a}}  \frac{\partial \Eym}{{\partial \boldsymbol{V}}}$  & $\boldsymbol{0}$                   \\ [2ex]
    $\frac{\partial \boldsymbol{\Delta}_{t}^{m}}{\partial \boldsymbol{\beta}^{m}}$  & $\boldsymbol{0}$                                                                            & $\boldsymbol{\delta}^{m}_{t} \frac{\partial \Eym}{\partial \boldsymbol{\beta}^{m}} + \gateprod{g^{0}_{t}}{m}   \frac{\partial^{2} \Eym}{\partial \boldsymbol{V} \partial \boldsymbol{\beta}^{m}}$    & $\gateprod{g^{0}_{t}}{m}  \frac{\partial^{2} \Eym}{\partial \boldsymbol{U}_{X} \partial \boldsymbol{\beta}^{m}}$  \\ [1ex]
    \hline
    \end{tabular}
  \caption{\label{tbl:delta_method_gradients} Delta Method Gradient Cases}
  \end{center}
\end{table}


Again, many of the expressions in Table \ref{tbl:delta_method_gradients} have already been defined in previous sections. The three expressions new to this section are $\frac{\partial^{2} \Eym}{\partial \boldsymbol{X} \partial \boldsymbol{\beta}^{m}}$, $\frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \boldsymbol{\omega}^{a,i}}$, and $\frac{\partial \gateprod{g^{0}_{t}}{m}}{\partial \bw{a,i}}$. For the standard OLS regressions that are considered in this paper, $\frac{\partial^{2} \Eym}{\partial \boldsymbol{X} \partial \boldsymbol{\beta}^{m}} = \boldsymbol{1}$. Conceptually, $\frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \boldsymbol{\omega}^{a,i}}$ describes how the marginal effects of the gating network change in response to small changes in the parameters of $\boldsymbol{\Omega}$. The value of $\frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \boldsymbol{\omega}^{a,i}}$ depends on what role $\boldsymbol{\omega}^{a,i}$ plays in navigating an input pattern from the root node to the expert $m$. In what follows, the indicator notation introduced in Section \ref{sec:TheHessian} will be used where $\mathbb{1}\{a, i, m\}$ is equal to one if $\bw{a, i}$ is an explicit gating vector for expert $m$ and zero if it is an implicit gating vector. With this notation in mind, the partial derivative of the prior weight with respect to gate parameter vector $\bw{a, i}$ is:

\begin{equation}
  \frac{\partial \gateprod{g^{0}_{t}}{m}}{\partial \bw{a,i}} = \gateprod{g^{0}_{t}}{f^{m}} \left( \mathbb{1}\{a, i, m\} - g^{a,i} \right) \boldsymbol{z}_{t}^{\top}
\end{equation}

The partial derivative of the marginal effects of an HME with respect to a gate parameter vector is expressed as:

\begin{equation} \label{eq:delta_gate_partial_exp}
  \frac{\partial \boldsymbol{\delta}^{m}_{t}}{\partial \bw{a,i}} = \gateprod{g^{0}_{t}}{m} (\mathbb{1}\{a, i, m\} - g^{a,i}_{t})  +  \gateprod{g^{0}_{t}}{m} \left[  (\mathbb{1}\{a, i, m\} - g^{a,i}_{t}) (\boldsymbol{W}^{m})^{\top}  - (\boldsymbol{G}^{a,i})^{\top}  \right] \boldsymbol{z}_{t}^{\top}
\end{equation}

where $\boldsymbol{W}^{m}$ was first seen in equation (\ref{eq:marginal_effects}) and

\begin{equation}
  \boldsymbol{G}^{a,i} = \left\{ g^{a,i} (1 - g^{a,i}) \bw{a,i} - \sum_{j \neq i} g^{a,i} g^{a,j} \bw{a,j} \right\}
\end{equation}

Standard errors for the marginal effects for the HME models can then be constructed with the robust variance-covariance matrix from equation (\ref{eq:robustgatevarcov}) and the collection of equations in this Section that fully defines $\frac{\partial \boldsymbol{\Delta}}{\partial \boldsymbol{\theta}}$.


\begin{equation} \label{eq:std_errs_full_marginal_effects}
  Asy.Var \left[ \boldsymbol{ \hat{ \Delta } } \right] = \sum^{M}_{n=1}  \left( \frac{1}{T} \sum^{T}_{t=1} \frac{\partial \boldsymbol{\Delta}_{t}}{\partial \boldsymbol{\theta}_{n}} \right)     \boldsymbol{V}(\boldsymbol{\hat{\theta}})      \left( \frac{1}{T} \sum^{T}_{t=1} \frac{\partial \boldsymbol{\Delta}_{t}}{\partial \boldsymbol{\theta}_{n}} \right)^\top
\end{equation} 


\section{Model Selection} \label{sec:ModelSelection}

For model discrimination we follow the sequential approach described by \citet{Voung1989} for comparing two models with a potential set of overlapping conditional distributions. A few of the author's definitions, equations, and theorems relevant to this article are collected and presented below. Much of the original wording and notation remain unchanged though some small alterations have been made to align with the notation of this article. 

\bigskip

\citet{Voung1989} centers his work on the likelihood-ratio (LR) framework. Suppose that there are two (H)ME models with different functional forms that need to be compared. These models will be labeled model A and model B and have parameters of length p and q, respectively. The log-likelihood for model A at the pseudo-true value $\boldsymbol{\theta}^{o}$ is given by $\boldsymbol{l}^{A}(\boldsymbol{\theta}^{o}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z})$ which is defined in Equation (\ref{eq:loglikelihood}). The value $\mathbb{E} \left[ \boldsymbol{l}^{A}(\boldsymbol{\theta}^{o}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \right]$ is the expectation of the log-likelihood value where the expectation is taken over the joint distribution of $(\boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z})$. Although $\mathbb{E} \left[ \boldsymbol{l}^{A}(\boldsymbol{\theta}^{o}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \right]$ is unknown, it can be consistently estimated by (1/N) times the log-likelihood value evaluated at the quasi-maximum likelihood estimator (MLE). Therefore (1/N) times the log-likelihood ratio statistic evaluated at the MLE is a consistent estimator of $\mathbb{E} \left[ \boldsymbol{l}^{A}(\boldsymbol{\theta}^{o}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \right] - \mathbb{E} \left[ \boldsymbol{l}^{B}(\boldsymbol{\gamma}^{o}| \boldsymbol{y}, \boldsymbol{X}, \boldsymbol{Z}) \right]$. The sample analogue for the LR statistic is defined in equation (\ref{eq:LR_statistic}):

\begin{equation} \label{eq:LR_statistic}
  LR_{N} (\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}}) \equiv  \sum_{t}^{N} \boldsymbol{l}^{A}_{t}(\hat{\boldsymbol{\theta}}) - \boldsymbol{l}^{B}_{t}(\hat{\boldsymbol{\gamma}}) = \sum_{t}^{N} \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\theta}}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\gamma}}) }
\end{equation}

\citet{Voung1989} characterizes the asymptotic distribution of the $LR_{N}$ statistic under very general conditions, covering the cases where the competing models are non-nested, overlapping, or nested and whether both, one, or neither model is misspecified. The overlapping case is particularly relevant for this article, best exemplified when a M-expert ME model is compared to a M-expert HME model and the probability of the two competing models sharing a common set of conditional distributions is high. What follows is a sequence procedure that identifies the limiting distribution of the likelihood-ratio statistic expressed in Equation (\ref{eq:sample_LR_convergence_to_population_LR}).

\begin{equation} \label{eq:sample_LR_convergence_to_population_LR}
  \frac{1}{c} LR_{N}(\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}}) \overset{a.s.}{\longrightarrow} \mathbb{E} \left[ \log \frac{P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o})}{P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o})} \right]
\end{equation}

First, the correct asymptotic distribution of $LR_{N}(\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}})$ needs to be identified and its convergence rate ($1/c$). Second, given the correct asymptotic distribution, provide a directional test for to determine which model is preferred over the other or if they are observationally equivalent given the data. For the overlapping model case there are two possible limiting distributions for $LR_{N}(\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}})$, as indicated below. In one circumstance, the asymptotic distribution is normally distributed. In a second circumstance, a weighted sum of chi-squares is the limiting distribution. A weighted sum of chi-squares distribution has the following definition:

\begin{definition}[Weighted Sums of Chi-Square Distributions] \label{def:WeightSum_of_ChiSq}
  \emph{Let $\boldsymbol{Z} = (Z_{1}, ..., Z_{m})^{\top}$ be a vector of m independent standard normal variables,
  and $\boldsymbol{\lambda} = (\lambda_{1}, ..., \lambda_{m})^{\top}$ be a vector of m real numbers. Then, the random variable
  $\sum_{i}^{m} \lambda_{i} Z_{i}^{2}$ is distributed as a weighted sum of chi-squares with parameters
  $(m, \boldsymbol{\lambda})$. Its cumulative distribution function (c.d.f.) is denoted by
  $M_{m}(\cdot; \boldsymbol{\lambda})$.}
\end{definition}

Theorem 3.1 from \citet{Voung1989} states the conditions that lead to the two different asymptotic distributions for $LR_{N}$. This theorem is reproduced here:

\begin{theorem}[Asymptotic Distribution of the LR Statistic] \label{th:AsymDistLR}
  \emph{Given assumptions A1-A5 in \citet{Voung1989}:
  \begin{enumerate}
    \item If $P^{A}(\cdot | \cdot ; \, \boldsymbol{\theta}^{o}) = P^{B}(\cdot | \cdot; \, \boldsymbol{\gamma}^{o})$, then:
      \begin{itemize}
        \item[] $2 LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}}) \overset{D}{\longrightarrow} M_{p+q}(\cdot; \boldsymbol{\lambda}_{o})$
      \end{itemize}
      where $\boldsymbol{\lambda}_{o}$ is the vector of p + q (possibly negative) eigenvalues of
      \begin{equation}
        \boldsymbol{W} = \begin{bmatrix}
          -\boldsymbol{G}(\boldsymbol{\theta}) \HI{\boldsymbol{\theta}}                        & -\boldsymbol{G}(\boldsymbol{\theta}, \boldsymbol{\gamma}) \HI{\boldsymbol{\gamma}} \\
           \boldsymbol{G}(\boldsymbol{\gamma}, \boldsymbol{\theta}) \HI{\boldsymbol{\theta}}   &  \boldsymbol{G}(\boldsymbol{\gamma}) \HI{\boldsymbol{\gamma}} 
        \end{bmatrix}
      \end{equation}
    \item If $P^{A}(\cdot | \cdot ; \, \boldsymbol{\theta}^{o}) \neq P^{B}(\cdot | \cdot; \, \boldsymbol{\gamma}^{o})$, then
      \begin{equation}
        N^{-\frac{1}{2}} \, LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}}) - N^{-\frac{1}{2}} \mathbb{E} \left[ \log \frac{P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o})}{P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o})} \right] \overset{D}{\longrightarrow} N(0, w^{2}_{o})
      \end{equation}
  \end{enumerate}
  }
\end{theorem}

In the theorem above, $\boldsymbol{G}(\boldsymbol{\theta})$ and $\HI{\boldsymbol{\theta}}$ have been defined in Equations (\ref{eq:OPG}) and (\ref{eq:Hessian}) while $\boldsymbol{G}(\boldsymbol{\theta}, \, \boldsymbol{\gamma}) = \sum_{t} \boldsymbol{S}_{t}(\boldsymbol{\theta}) \boldsymbol{S}_{t}(\boldsymbol{\gamma})^\top$. The term $w^{2}_{o}$ also appears in Theorem (\ref{th:AsymDistLR}). This represents the pseudo-true variance of the LR statistic and is defined as:

\begin{align*}
  w_{o}^{2} \equiv & \mathbb{Var} \left[ \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o}) } \right] \\
   = & \mathbb{E} \left[ \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o}) } \right]^{2} - \left[ \mathbb{E} \left[ \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o}) } \right] \right]^{2}
\end{align*}

The sample analogue for the population statistic has a straightforward definition:

\begin{equation}
  \hat{w}^{2}_{N} \equiv \frac{1}{N} \sum_{t}^{N} \left[ \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\theta}}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\gamma}}) } \right]^{2} - \left[ \frac{1}{N} \sum_{t}^{N} \left[ \log \frac{ P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\theta}}) }{ P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \hat{\boldsymbol{\gamma}}) } \right] \right]^{2}
\end{equation}

\citet{Voung1989} demonstrates that the sample variance converges almost surely to the population pseudo-true variance and gives the limiting distribution in Theorem 4.3 of \citet{Voung1989}. That theorem is reproduced here:

\begin{theorem}[Asymptotic Distribution of the Variance Statistics given $w^{2} = 0$] \label{th:AsymDistVar_at_null}
  \emph{Given assumptions A1-A7 in \citet{Voung1989} and Definition (\ref{def:WeightSum_of_ChiSq})
  \begin{equation}
    N \hat{w}^{2}_{N} \overset{D}{\longrightarrow} M_{p+q}(\cdot; \, \boldsymbol{\lambda}^{2}_{o})
  \end{equation}
  where $\boldsymbol{\lambda}^{2}_{o}$ is the vector of squares of the p + q eigenvalues $\boldsymbol{\lambda}_{o}$ of matrix $\boldsymbol{W}$.}
\end{theorem}


\subsection{The Variance Test}

The first step in comparing model A and model B is to determine whether the following relationship holds: $P^{A}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\theta}^{o}) = P^{B}(y_{t} | \boldsymbol{x}_{t}, \boldsymbol{z}_{t}; \, \boldsymbol{\gamma}^{o})$. As seen in Theorem (\ref{th:AsymDistLR}), whether $P^{A} = P^{B}$ holds has direct implications on the limiting distribution of $LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}})$. \citet{Voung1989} points out that testing this relationship is equivalent to testing whether the variance of the LR statistic is significantly different from zero.

\bigskip

For the variance test the null hypothesis is that $w^{2}_{o} = 0$, which is equivalent to $H_{0}: P^{A} = P^{B}$. For some significance level $\alpha_{w}$, if we fail to reject the null we can draw the conclusion that the two models are observationally equivalent given the data. If we do have enough evidence in the data to reject the null, the next step is to test for a preference between the two competing models. 


\subsection{Directional Test}

For models with an overlapping relationship, after a rejection of the null hypothesis of the variance test ($H_{0}: P^{A} = P^{B}$), Theorem (\ref{th:AsymDistLR}) ensures that the limiting distribution of the LR statistic is the normal distribution. With this knowledge and Theorem (5.1) from \citet{Voung1989} it is straightforward to test if the LR statistic is statistically different from zero. Theorem (5.1) from \citet{Voung1989} is reproduced here:

\begin{theorem}[Model Selection Tests for Strictly Non-Nested Models] \label{th:NonNestAsyLRDist}
  \emph{
    Given assumptions A1-A6 in \citet{Voung1989}, if models A and B are strictly
    non-nested, then:
    \begin{enumerate}
      \item Under $H_{0}$: $N^{-\frac{1}{2}} \, LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}}) / \hat{w}_{N} \overset{D}{\longrightarrow} N(0, 1) $  \\
      \item Under $H_{A}$: $N^{-\frac{1}{2}} \, LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}}) / \hat{w}_{N} \overset{D}{\longrightarrow} +\infty $  \\
      \item Under $H_{B}$: $N^{-\frac{1}{2}} \, LR_{N}(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\gamma}}) / \hat{w}_{N} \overset{D}{\longrightarrow} -\infty $
    \end{enumerate}
  }
\end{theorem}

The null of the model selection test is that of no observational difference between the competing models. At some significance level $\alpha_{LR}$, a rejection of the null indicates that one model is preferred over the other and the sign of the LR statistic indicates which one. Referencing the LR statistics definition in Equation (\ref{eq:LR_statistic}), a positive value indicates that model A is preferred over model B and vice-versa for a negative value.

It is worth pointing out that the size of the HME models can become quite large. The class of models investigated in this article have parameter vectors that range in length from 21 to 122. To penalize parameter size when weighing competing models, a correction factor can be deducted from the LR statistic. This essay uses a correction factor based on the \citet{Schwarz1978} information criterion and provided in \citet{Voung1989}. The correction factor is defined in Equation (\ref{eq:Schwarz_correction_factor}) and the adjusted LR statistic is defined in Equation (\ref{eq:AdjustedLR}):

\begin{equation} \label{eq:Schwarz_correction_factor}
  K_{N}(P^{A}_{\boldsymbol{\theta}}, P^{B}_{\boldsymbol{\gamma}}) = (p / 2) log N - (q / 2) log N
\end{equation}

\begin{equation} \label{eq:AdjustedLR}
  LR_{N}^{adj}(\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}}) = LR_{N}(\hat{\boldsymbol{\theta}}, \, \hat{\boldsymbol{\gamma}}) - K_{N}(P^{A}_{\boldsymbol{\theta}}, P^{B}_{\boldsymbol{\gamma}})
\end{equation}


\section{A Simple Example} \label{sec:SimpleExample}

In order to provide a concrete example of the concepts discussed previously, the ME and HME models are demonstrated on a small and well known dataset collected by \citet{Anderson1936} and popularized in the statistics literature by \citet{Fisher1936}. Anderson collected 50 measurements each from three different species of iris flowers; the width and length of both the petal and the sepal. Figure \ref{fig:Iris_dataset} provides a basic view of the species specific clustering inherent in the data. The work below uses the ME and HME architectures to estimate a flower's sepal width using only its petal width as a predictor. The petal width will be used as the sole covariate in the local linear expert regressions ($\boldsymbol{X}$) as well as in the gating network ($\boldsymbol{Z}$). 

\begin{figure}[!ht]
  \includegraphics[width=\textwidth]{basic_iris_plot.jpeg}
  \caption{Three different iris species: Setosa (blue circles), Versicolor (orange triangles), Virginia (green crosses). Sepal width is on the vertical axis and petal width on the horizontal axis.}
  \label{fig:Iris_dataset}
\end{figure}

\begin{equation} \label{eq:HME_iris}
    sepal.width_{i} = \beta_{0} + \beta_{1} * petal.width_{i} + \varepsilon_{i} \enspace | \enspace \omega_{0} + \omega_{1} * petal.width_{i}
\end{equation}

The goal is to have the gating network of the models identify the inherent species-specific clustering without explicit knowledge of each observation's species classification and then fit an appropriate local regression to the self-identified clusters. As a benchmark, an OLS model is run where a flower's petal width is interacted with its species, resulting in a species-specific estimation of sepal width.

\begin{equation} \label{eq:OLS_iris}
    sepal.width_{is} = \beta_{0,s} + \beta_{1, s} * petal.width_{is} + \varepsilon_{is}
\end{equation}

Two sets of regressions are run. Since the Versicolor and Virginica species can be viewed as one large cluster, a two-expert ME model is run and compared to a benchmark OLS where Versicolor and Virginica are labelled as the same species. A second set of regressions are run with three mixture experts. When moving to the three expert model, there is now a choice on what kind of gating architecture to employ. We can go deep by adding a gating network with depth two (HME), or we can go wide by keeping the depth of the gating network at one (ME). Again, for comparative purposes, a benchmark OLS regression is estimated for each species separately. Results are collected in Table \ref{tbl:Iris}. Coefficients for local experts in the two expert ME regression match closely with the OLS benchmark with the exception being the coefficient for Setosa Peta.Width. The strong separation between the Setosa and Versicolor/Virginica clusters makes it easy for the ME gating network to discriminate between the two using just the Petal Width dimension. This task becomes more complicated when considering all three species at the same time since there exists some overlap between the Versicolor and Virginica clusters. When comparing the coefficients of the local regressions (see Table \ref{tbl:Iris}), the HME architecture clearly outperforms the ME architecture. While the ME model does obtain a larger likelihood value than the OLS estimate, it fails to identify the three separate species that are known to exist. The HME model, on the other hand, naturally picks up on the three underlying clusters while also providing a superior likelihood value. This speaks to one of the major caveats of using this class of model. The likelihood value of an ME or HME can always been improved by adding more and more experts, but this improvement should not be confused with the model gaining a finer understanding of the underlying data generating process. It simply may start to over-fit the data at hand.

\begin{landscape}
\begin{table} 
	\caption{Iris Dataset - OLS vs ME vs HME}
	\begin{threeparttable}
		\begin{tabular}[l]{l l l l l l l l l l l l}
  \cmidrule{1-12}

  & \multicolumn{4}{c}{2 Expert Mixture} & & \multicolumn{6}{c}{3 Expert Mixture} \\
  \cmidrule(r){2-5}  \cmidrule(r){7-12}
  & \multicolumn{2}{c}{OLS}  & \multicolumn{2}{c}{ME} & & \multicolumn{2}{c}{OLS}  & \multicolumn{2}{c}{HME}  & \multicolumn{2}{c}{ME} \\
  \cmidrule(r){2-3}  \cmidrule(r){4-5}  \cmidrule(r){7-8}  \cmidrule(r){9-10}  \cmidrule(r){11-12}
  & Coef. & SE & Coef. & SE && Coef. & SE & Coef. & SE & Coef. & SE     \\
  \cmidrule{1-12}
  
  Setosa \\
  \cmidrule(r){1-1}
  Const.             & 3.22 & 0.11 *  & 3.22 & 0.13 *   && 3.22  & 0.11 * & 3.22 & 0.13 * & 2.73  & 0.08 *       \\
  Petal.Width        & 0.84 & 0.42 *  & 0.95 & 0.49 -   && 0.84  & 0.41 + & 0.84 & 0.49 - & 2.62  & 0.47 *       \\[0.3cm]
  
  Virginica \\
  \cmidrule(r){1-1}
  Const.             & --   & --     & --   & --        && 1.70  & 0.32 * & 1.66 & 0.27 * & 2.13  & 0.09 *       \\
  Petal.Width        & --   & --     & --   & --        && 0.63  & 0.16 * & 0.63 & 0.13 * & 0.44  & 0.06 *       \\[0.3cm]
  
  Versicolor \\
  \cmidrule(r){1-1}
  Const.             & --   & --     & --   & --        && 1.37  & 0.29 * & 1.16 & 0.19 * & 3.65  & 0.23 *       \\
  Petal.Width        & --   & --     & --   & --        && 1.05  & 0.22 * & 1.26 & 0.14 * & -0.15 & 0.73         \\[0.3cm]

  Virg + Versi \\
  \cmidrule(r){1-1}
  Const.             & 2.13 & 0.13 * & 2.13 & 0.10 *    && --    & --     & --   & --     & --    & --           \\
  Petal.Width        & 0.44 & 0.07 * & 0.44 & 0.06 *    && --    & --     & --   & --     & --    & --           \\[0.3cm]

  AME: Petal.Width \\
  \cmidrule(r){1-1}
  Gate               & --   & --     & -0.13 & --        && --    & --     & -0.23 & --     & 0.06  & --           \\
  Expert             & --   & --     &  0.61 & --        && --    & --     &  0.88 & --     & 0.70  & --           \\
  Total              & 0.57 & --     &  0.49 & --        && 0.84  & --     &  0.66 & --     & 0.76  & --           \\[0.3cm]


  Log-Like           & -0.24 & --    & 0.97  & --      && -0.20  & --    & 1.08 & --     & 1.01  & --            \\
  N                  & 150   & --    & 150   & --      && 150    & --    & 150  & --     & 150   & --            \\

	\hline
		\end{tabular}
		\begin{tablenotes}
			\item{\footnotesize Signif. Codes: 0 '*' 0.01 '+' 0.05 '-' 0.1 ' ' 1}
			\item{\footnotesize OLS regressions are modeled using equation (\ref{eq:OLS_iris})}
			\item{\footnotesize ME regressions are modeled using equation (\ref{eq:HME_iris}) and architecture $\boldsymbol{A}$ from Figure \ref{fig:network_comparison}}
			\item{\footnotesize HME regressions are modeled using equation (\ref{eq:HME_iris}) and architecture $\boldsymbol{C}$ from Figure \ref{fig:network_comparison}}
		\end{tablenotes} \label{tbl:Iris}
	\end{threeparttable}
\end{table}
\end{landscape}


\begin{figure}[!ht]
  \includegraphics[width=\textwidth]{Iris_fitted_regressions.jpeg}
  \caption{Comparison of the fitted experts between the ME and HME architectures   applied all three species of the Iris dataset. OLS regression estimates are drawn in solid lines. Although the HME and ME both achieve superior log-likelihood values compared to OLS, only the HME is able to identify the three iris species clusters.}
  \label{fig:Iris_fitted_regressions}
\end{figure}

\clearpage


\section{A Mincer Wage Equation} \label{sec:MincerWageEx}

For a more economically relevant example, we turn our attention to a common topic in labor economics: the income return on an additional year of education. At times called the "Mincer wage equation", this essay's version of it will be:

\begin{equation} \label{eq:mincer_equ}
  \log (wage) = \beta_{0} + \beta_{1} * \textnormal{Age} + \beta_{2} * \textnormal{Age}^{2} + \beta_{3} * \textnormal{YrsEdu} + \boldsymbol{\beta_{4}}\boldsymbol{X} + \varepsilon
\end{equation}

with $\boldsymbol{X}$ containing a set of individual-specific variables as well as a set of occupation-specific attributes. The data will come from two sources. First, from the 2000 Census, a measure of the hourly (log) wage is devised. In addition to income, information on age, years of education (YrsEdu), job occupations codes, and a set of demographic identifiers indicating the race of the individuals are also obtained from the Census sample. For the occupational codes, the Standard Occupation Classification (SOC) codes from the Occupation Information Network (ONet) are used. Each occupation is associated with a set of knowledge and skill-based attributes describing which qualities are necessary to perform each job suitably. A federally sponsored source, ONet details, "the knowledge, skills, and abilities required as well as how the work is performed in terms of tasks, work activities, and other descriptors" \citep{ONET}. The cross walk provided by \citet{Crosswalk} is used to link the occupational codes in the Census data to the SOC codes used by ONet. This mapping is not one-to-one. When more than one SOC code points to a single census code, the average of the SOC codes is taken. After a quick but careful scan of the job attributes available on ONet, the following four were selected to provide a small but diverse set of attributes that contrast well, with each attribute embodying a skill valued across industry, culture, and society: Social Perceptiveness \footnote{https://www.onetonline.org/find/descriptor/result/2.B.1.a}, Data Analytics \footnote{https://www.onetonline.org/find/descriptor/result/4.A.2.a.4}, Design \footnote{https://www.onetonline.org/find/descriptor/result/2.C.3.c}, and Creative Thinking \footnote{https://www.onetonline.org/find/descriptor/result/4.A.2.b.2}. The footnotes provide a link to  full classification hierarchy listed on the website. 

\bigskip

For these selected attributes, ONet grades their relevance on a 100 point scale. Each attribute contains two scales, an "importance" (I) scale and a "level" (L) scale. The importance scale denotes how critical the attribute is to the occupation while the level indicates how much the skill is required or needed to perform the occupation. To unify the two measures into a single value, a Cobb-Douglass style average with a 2/3 weight for importance and a 1/3 weight for the level scale is used: $A = L^{\frac{1}{3}} I^{\frac{2}{3}}$. With a unified attribute measure for every occupation in ONet's index, each attribute is mean centered and scaled to have unit variance across all ONet occupations.


\begin{figure}[t!]
  \includegraphics[width=\textwidth]{Job_characteristic_density.jpeg}
  \caption{Density estimates of ONet job attributes for the Census sample broken down by sex. \textbf{Note:} The job attributes have been mean centered and scaled to have unit variance at the \textit{occupational} level and not at the observation level with respect to the sample.}
  \label{fig:JobChar_vs_sex}
\end{figure}


\begin{table}[t!] \centering
  \caption{Summary Statistics}
  \begin{threeparttable}
    \begin{tabular}[l]{l r r r r}
  \cmidrule{1-5}

                   & 25\%   & Mean & 50\% & 75\%    \\
  \cmidrule{1-5}
  $\log$ Wage (hr) &  2.22 &  2.61 &  2.59  &  2.96 \\
  Yrs Edu          & 12.00 & 13.78 & 14.00  & 16.00 \\
  Age              & 30.00 & 39.15 & 39.00  & 48.00 \\
  Age-16           & 14.00 & 23.15 & 23.00  & 32.00 \\
  Female           & --    & 40.47 & --     & --    \\
  Af Amer          & --    &  8.62 & --     & --    \\
  Indian           & --    &  1.05 & --     & --    \\
  White            & --    & 77.00 & --     & --    \\
  Hispanic         & --    & 10.00 & --     & --    \\
  Asian            & --    &  3.36 & --     & --    \\
  Creative         & -0.81 & -0.23 & -0.14  &  0.33 \\
  Design           & -0.94 & -0.36 & -0.54  &  0.11 \\
  Analytic         & -0.80 & -0.24 & -0.26  &  0.28 \\
  Perceptive       & -0.82 &  0.16 &  0.13  &  1.08 \\

  \hline
    \end{tabular}
    \begin{tablenotes}
      \item{\footnotesize Summary statistics for the covariates used in the Mincer wage equation. N = 68,642}
    \end{tablenotes} \label{tbl:census_cov_summary}
  \end{threeparttable}
\end{table}

The total number of individuals in the Census data numbers 105,796. After applying the crosswalk, 75,957 cases remain with complete information across both datasets. Of those 75,957, roughly ten percent (7,315) are randomly held out and used as a test set to gauge out-of-sample forecast performance across model specifications. This leaves 68,642 individuals left as a training set. A statistical summary of the covariates is provided in Table \ref{tbl:census_cov_summary}.

\bigskip

A natural question to consider as a researcher is where to put the variable(s) of interest while performing an HME estimation. \citet{JiangTanner2000} provide their proof of model consistency for HME of GLMs for the case where all covariates appear in the gating network as well as the experts. This will be referred to as the \textit{full} specification:

\begin{equation} \label{eq:full_formula}
  log(wage) = Age + YrsEdu + Sex +  Race + Occ \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

The \textit{full} specification will be compare to two others. First, a \textit{mid} specification where the local experts contain age and years of education while removing demographic indicators:

\begin{equation} \label{eq:mid_formula}
  log(wage) = Age + YrsEdu \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

And second, a \textit{minimal} specification where our core variable of interest, years of education, appears solely in the gating network.

\begin{equation} \label{eq:min_formula}
  log(wage) = Age \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

For comparative purposes, several different regressions across three different dimensions will be estimated: model architectures (ME vs HME), the number of experts, and the regression specification (Equations (\ref{eq:full_formula}) - (\ref{eq:min_formula})). Model comparison results across these dimensions are collected in Tables \ref{tbl:model_comparison} and \ref{tbl:voung_comparison}. Table \ref{tbl:model_comparison} reports the standard set of model selection criterion including the log-likelihood value at the fitted parameter estimates, the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), and the predictive mean squared error (MSE) of the model when applied to the hold-out test set. Several items are worth mentioning. First, there is a general advantage to using the HME structure if the aim is to maximize the likelihood value. The HME structure shows consistent improvement across specifications over the ME structure, holding the number of experts constant. The only exception is the full specification HME with four experts which has a lower likelihood value than its ME analogue. This increase in efficiency is most likely due to the HME's more refined gating architecture, whose recursive partitioning is more effective at finding the next improvement in the parameter vector than the single multinomial split in the ME. Second, it is best to give the expert regressions as much information as possible. The Full specification clearly outperforms the Mid specification, which outperforms the Min specification. If one holds the architecture and the number of experts constant, the performance metrics show clear improvement as the model specification adds more explanatory variables to the expert regressions. Third, for Full specification models, there is disagreement among the selection criterion about which model has the best performance. If using the log-likelihood or AIC values as the discriminating factor the 5-expert HME model is selected. If a heavier penalty term is used for models with a greater number of parameters, as is the case with the BIC, the 3-expert HME model would be selected. And, finally, if judging by out-of-sample forecast performance using the MSE the 3-expert ME model would prevail. This nuance does not hold for the Mid and Min specifications. For these cases, the 5-expert HME is the unanimous winner. In Table \ref{tbl:voung_comparison}, the results for Voung's LR based model comparison tests discussed in Section \ref{sec:ModelSelection} are summarized. Overall, the LR based selection tests align with the models selected by the BIC and MSE selection criterion with one important caveat: the possibility of two models being observationally equivalent. Having a potential outcome of model equivalency is what separates the LR based test from the standard selection criterion. It provides a more nuanced comparison of the (H)ME models and can help the researcher detect when the number of experts in a model starts to exceed the number of latent populations in the data. This topic will be expanded upon during the Monte Carlo exercises in Section \ref{sec:MonteCarlo}. According to the LR tests, the 3-expert HME and ME models perform \emph{equally} well, either being the preferred model or observationally equivalent to all other model specifications and architectures. Looking at the middle block of tests where the Mid specifications are compared to each other, the 4 and 5-expert HME models are jointly favored over the other Mid specifications. The last diagonal block comparing the Min specifications to themselves indicates that all three HME models are joint preferred over the ME models. 

\begin{table}[b!] \centering
  \caption{Comparing Complexity, Architecture, and Regression Specification}
  \begin{threeparttable}
    {\footnotesize
    \begin{tabular}[r]{l l l r r r r}
  \cmidrule{1-7}
         &       &         &  \multicolumn{4}{c}{Performance Metrics} \\ 
   \cmidrule(l){4-7}
Specification & Architecture  & Experts & Log-Lik & AIC    & BIC    & MSE   \\ 
  \cmidrule{1-7}

Full     &  ME   &    2    & 0.703      & -1.404      & -1.399      & 0.1829      \\
         &  ME   &    3    & 0.718      & -1.434      & -1.425      & \iu{0.1820} \\
         &  ME   &    4    & 0.716      & -1.429      & -1.416      & 0.1823      \\
         &  ME   &    5    & 0.715      & -1.426      & -1.410      & 0.1825      \\
         &  HME  &    3    & 0.719      & -1.436      & \iu{-1.427} & 0.1822      \\
         &  HME  &    4    & 0.703      & -1.404      & -1.391      & 0.1825      \\
         &  HME  &    5    & \iu{0.721} & \iu{-1.439} & -1.423      & 0.1824      \\
         \cmidrule(l){2-7}
         &  Avg. &         & 0.714      & -1.425      & -1.413      & 0.1824      \\
         \cmidrule(l){2-7}
         &       &         &            &             &             &             \\
Mid      &  ME   &    2    & 0.681      & -1.361      & -1.358      & 0.1852      \\
         &  ME   &    3    & 0.689      & -1.376      & -1.371      & 0.1858      \\
         &  ME   &    4    & 0.686      & -1.370      & -1.362      & 0.1948      \\
         &  ME   &    5    & 0.645      & -1.288      & -1.278      & 0.2187      \\
         &  HME  &    3    & 0.695      & -1.388      & -1.383      & 0.1838      \\
         &  HME  &    4    & 0.706      & -1.410      & -1.402      & 0.1838      \\
         &  HME  &    5    & \it{0.716} & \it{-1.429} & \it{-1.419} & \it{0.1822} \\
         \cmidrule(l){2-7}
         &  Avg. &         & 0.688      & -1.375      & -1.367      & 0.1906      \\
         \cmidrule(l){2-7}
         &       &         &            &             &             &             \\
         &       &         &            &             &             &             \\
Min      &  ME   &    2    & 0.650      & -1.299      & -1.296      & 0.1923      \\
         &  ME   &    3    & 0.660      & -1.318      & -1.313      & 0.1930      \\
         &  ME   &    4    & 0.644      & -1.286      & -1.279      & 0.2082      \\
         &  ME   &    5    & 0.609      & -1.216      & -1.206      & 0.2453      \\
         &  HME  &    3    & 0.689      & -1.377      & -1.372      & 0.1833      \\
         &  HME  &    4    & 0.692      & -1.383      & -1.375      & 0.1830      \\
         &  HME  &    5    & \it{0.702} & \it{-1.403} & \it{-1.393} & \it{0.1823} \\
         \cmidrule(l){2-7}
         &  Avg. &         & 0.664      & -1.326      & -1.319      & 0.1982      \\
        \cmidrule(l){1-7}
    \end{tabular}
    }
    \begin{tablenotes}
      \item{\footnotesize \textbf{Note:} Log-Likelihood, AIC, and BIC are divided by the sample size of 68,642. Italicized entries are the winning values within specification while underlined entries are the best values across all three specifications. The MSE is calculated from a hold-out test set with sample size of 7,315}
      \item{\footnotesize \textbf{Note:} After looking at the results two themes emerge. \textbf{One}, there is a general advantage to using the HME structure if the aim is to maximize the likelihood value. The HME structure shows consistent improvement across specifications over the ME structure, holding the number of experts constant. The only exception is the the full specification with four experts.
      \textbf{Two}, give the expert regressions as much information as possible. The Full specification clearly outperforms the Mid and Min specifications across the board.
      }
      \item{\footnotesize }
    \end{tablenotes} \label{tbl:model_comparison}
  \end{threeparttable}
\end{table}


\begin{landscape}
\begin{table}[t!] \centering
  \caption{Voung Comparison Results}
  \begin{threeparttable}
    {\footnotesize
    \begin{tabular}[r]{l l l r r r r r r r r r r r r r r r r r r r r r r r}
    \cmidrule{1-26}
         &     &         &  \multicolumn{7}{c}{Full}                         &&  \multicolumn{7}{c}{Mid}                          &&  \multicolumn{7}{c}{Min}                         \\
         &     &         &  \multicolumn{4}{c}{ME} & \multicolumn{3}{c}{HME} &&  \multicolumn{4}{c}{ME} & \multicolumn{3}{c}{HME} && \multicolumn{4}{c}{ME} & \multicolumn{3}{c}{HME} \\
        \cmidrule(l){4-7} \cmidrule(lr){8-10}              \cmidrule(l){12-15} \cmidrule(l){16-18}             \cmidrule(l){20-23} \cmidrule(l){24-26}
         &     & Experts & 2   &  3  &  4  &  5  &  3   &  4   &  5          && 2   &  3  &  4  &  5  &  3   &  4   &  5          &&  2  &  3  &  4  &  5  &  3   &  4   &  5         \\
    \cmidrule{3-26}
Full     & ME  & 2       &  .  & -1  &  0  & -1  & -1   &  0   &  -1          &&  1  &   1 &  1  &  1  &  1   & -1   &  -1          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         & ME  & 3       &  1  &  .  &  1  &  0  &  0   &  1   &   0          &&  1  &   1 &  1  &  1  &  1   &  1   &   0          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         & ME  & 4       &  0  & -1  &  .  &  0  & -1   &  0   &   0          &&  0  &   0 &  1  &  1  &  0   & -1   &  -1          &&  1  &  0  &  1  &  1  &  0   &  0   &  0       \\
         & ME  & 5       &  1  &  0  &  0  &  .  &  0   &  1   &   0          &&  1  &   1 &  1  &  1  &  1   &  1   &  -1          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         & HME & 3       &  1  &  0  &  1  &  0  &  .   &  1   &   0          &&  1  &   1 &  1  &  1  &  1   &  1   &   1          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         & HME & 4       &  0  & -1  &  0  & -1  & -1   &  .   &  -1          &&  1  &   1 &  1  &  1  &  1   & -1   &  -1          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         & HME & 5       &  1  &  0  &  0  &  0  &  0   &  1   &   .          &&  1  &   1 &  1  &  1  &  1   &  0   &   0          &&  1  &  1  &  1  &  1  &  1   &  1   &  0       \\
         &     &         &     &     &     &     &      &      &              &&     &     &     &     &      &      &              &&     &     &     &     &      &      &          \\
Mid      & ME  & 2       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          &&  .  &   0 &  1  &  1  & -1   & -1   &  -1          &&  1  &  1  &  1  &  1  & -1   & -1   & -1       \\
         & ME  & 3       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          &&  0  &   . &  1  &  1  &  0   & -1   &  -1          &&  1  &  0  &  1  &  1  &  0   & -1   &  0       \\
         & ME  & 4       & -1  & -1  & -1  & -1  & -1   & -1   &  -1          && -1  &  -1 &  .  &  1  & -1   & -1   &  -1          &&  1  &  1  &  1  &  1  & -1   & -1   & -1       \\
         & ME  & 5       & -1  & -1  & -1  & -1  & -1   & -1   &  -1          && -1  &  -1 & -1  &  .  & -1   & -1   &  -1          && -1  & -1  & -1  &  1  & -1   & -1   & -1       \\
         & HME & 3       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          &&  1  &   0 &  1  &  1  &  .   & -1   &  -1          &&  1  &  0  &  1  &  1  &  0   &  1   &  0       \\
         & HME & 4       &  1  & -1  &  1  & -1  & -1   &  1   &   0          &&  1  &   1 &  1  &  1  &  1   &  .   &   0          &&  1  &  1  &  1  &  1  &  1   &  1   &  1       \\
         & HME & 5       &  1  &  0  &  1  &  1  & -1   &  1   &   0          &&  1  &   1 &  1  &  1  &  1   &  0   &   .          &&  1  &  1  &  1  &  1  &  1   &  1   &  1       \\
         &     &         &     &     &     &     &      &      &              &&     &     &     &     &      &      &              &&     &     &     &     &      &      &          \\
Min      & ME  & 2       & -1  & -1  & -1  & -1  & -1   & -1   &  -1          && -1  &  -1 & -1  &  1  & -1   & -1   &  -1          &&  .  &  0  &  1  &  1  & -1   & -1   & -1       \\
         & ME  & 3       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          && -1  &   0 & -1  &  1  &  0   & -1   &  -1          &&  0  &  .  &  0  &  1  &  0   &  0   &  0       \\
         & ME  & 4       & -1  & -1  & -1  & -1  & -1   & -1   &  -1          && -1  &  -1 & -1  &  1  & -1   & -1   &  -1          && -1  &  0  &  .  &  1  & -1   & -1   & -1       \\
         & ME  & 5       & -1  & -1  & -1  & -1  & -1   & -1   & --1          && -1  &  -1 & -1  & -1  & -1   & -1   &  -1          && -1  & -1  & -1  &  .  & -1   & -1   & -1       \\
         & HME & 3       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          &&  1  &   0 &  1  &  1  &  0   & -1   &  -1          &&  1  &  0  &  1  &  1  &  .   &  0   &  0       \\
         & HME & 4       & -1  & -1  &  0  & -1  & -1   & -1   &  -1          &&  1  &   1 &  1  &  1  & -1   & -1   &  -1          &&  1  &  0  &  1  &  1  &  0   &  .   &  0       \\
         & HME & 5       &  0  &  0  &  0  &  0  &  0   &  0   &   0          &&  1  &   0 &  1  &  1  &  0   & -1   &  -1          &&  1  &  0  &  1  &  1  &  0   &  0   &  .       \\
        \cmidrule(l){1-26}
    \end{tabular}
    }
    \begin{tablenotes}
      \item{\footnotesize \textbf{Legend:}
      \begin{itemize}
        \item 0 :  Models are observationally equivalent
        \item 1 :  The model in the row is favored
        \item -1: The model in the column is favored
      \end{itemize}
      }
      \item{\footnotesize \textbf{Note:}
      A significance level of 5\% was used for both the variance test and the LR test. When comparing models, an adjustment to the LR statistic is made to penalize models with a larger number of parameters. See Equation (\ref{eq:Schwarz_correction_factor}). By looking at the top set of rows, we can see that the Full specification clearly outperforms the Mid and Min specifications as a group.}
    \end{tablenotes} \label{tbl:voung_comparison}
  \end{threeparttable}
\end{table}
\end{landscape}


Turning attention to the main variable of focus, Table \ref{tbl:YrsEdu_coef} provides a comparison of the average marginal effect for \textit{YrsEdu} across the same dimensions explored for the performance metrics. There is a noticeable change across model specifications. Compared to the OLS coefficient of 0.076, the Min specification, which includes \textit{YrsEdu} only in the gating network, underestimates the returns to education. The Mid specification, which includes \textit{Age} and \textit{YrsEdu} in the expert regressions as well as the gating network, overestimates the returns to education for all model specifications. The Full specification, which has the entire suite of variables in both places, matches the closest to the OLS estimate across the estimated models.



\begin{table}[t!] \centering
  \caption{Returns to Years of Education}
  \begin{threeparttable}
    \begin{tabular}[l]{r r r r r}
  \cmidrule{1-5}
        &         & \multicolumn{3}{c}{Avg. Marginal Effect} \\ 
  \cmidrule(r){3-5}
  Depth & Experts & Min   & Mid   & Full      \\
  \cmidrule{1-5}

  ME      & 2     & 0.051 & 0.082 & 0.076     \\
  ME      & 3     & 0.051 & 0.080 & 0.074     \\ 
  ME      & 4     & 0.050 & 0.086 & 0.073     \\
  ME      & 5     & 0.016 & 0.098 & 0.074     \\
  HME     & 3     & 0.063 & 0.080 & 0.073     \\
  HME     & 4     & 0.063 & 0.082 & 0.071     \\
  HME     & 5     & 0.065 & 0.077 & 0.075     \\

  \hline
    \end{tabular}
    \begin{tablenotes}
      \item{\footnotesize \textbf{Note:} OLS coef: 0.076}
      \item{\footnotesize \textbf{Note:} There is a noticeable change across in the marginal return to an extra year of education. Compared to the OLS coefficient of 0.076, the Min specification, which includes \textit{YrsEdu} only in the gating network, underestimates the returns to education. The Mid specification, which includes \textit{Age} and \textit{YrsEdu} in the expert regressions as well as the gating network, overestimates the returns to education in all the models except the HME with four and five experts. The Full specification, which has the entire suite of variables in both places, matches most closely to the OLS estimate across the estimated models.}
    \end{tablenotes} \label{tbl:YrsEdu_coef}
  \end{threeparttable}
\end{table}

\bigskip

Given the model selection results in Tables \ref{tbl:model_comparison} and \ref{tbl:voung_comparison}, the remainder of this section will focus on exploring the details of the full specification regressions for the 2-expert and 3-expert models only. Full regression results for the Mincer wage equation (\ref{eq:mincer_equ}) with two experts is presented first in Table \ref{tbl:2E_full_regressions_results}. At this specification there is no distinction between the HME and ME. Results for the three expert models are then presented for the two different architectures (HME vs ME) in Tables \ref{tbl:3W_full_regressions_results} and \ref{tbl:3D_full_regressions_results}. To compliment the regression summaries, Tables \ref{tbl:ME2_sample_comparison}, \ref{tbl:ME3_sample_comparison}, and \ref{tbl:HME3_sample_comparison} provide mean and median values for the subset of individuals in the census sample that are attributed to each expert based on the value of their prior weights\footnote{For example, observation $i$ is assigned to expert $j$ if the prior weight vector's largest value is the $j$-th index: $\argmax \boldsymbol{h}_{i} = h_{ij}$.}.

\bigskip

Broadly speaking, all three models explored share the same macro view of the data. On the right side of Tables \ref{tbl:2E_full_regressions_results}, \ref{tbl:3W_full_regressions_results}, and \ref{tbl:3D_full_regressions_results} are a group of columns titled '(H)ME Marginal Effects'. Here the marginal effects of the model can be broken down and attributed to the gating network or the expert regressions. "Both", "Experts", and "Gates" refers to marginal effects referenced by equations (\ref{eq:ME_both}), (\ref{eq:ME_expert}), and (\ref{eq:ME_gating}), respectively. The values are fairly consistent across variables and model architectures with the coefficients for \textit{Age} and its square a modest exception, ranging from 0.028 (HME) to 0.042 (2-Expert ME) for \textit{Age}. Notice also that the marginal effects from the expert regressions are the lion's share of total marginal effect, ranging from one to two orders of magnitude larger than marginal effects for the gating network. When looking at the occupational attributes there is similar agreement between the estimated models. The marginal effects for all three are in close proximity between the ME and HME models. Those individuals who specialize in performing analytics enjoy the greatest hourly rate (0.126 - 0.128). Design (0.074 to 0.081) and Perceptive (0.053 to 0.057) attributes get a smaller bump to the their hourly wage while Creative types (-0.044 to -0.043) clearly have alternative motivation than monetary gain.

\bigskip

When left to segment the data set on its own, the fitted HME models that are returned lead to some interesting conclusions. The first segmentation of the sample is seen by the two expert ME model that estimates two different wage equations. One for a majority of the population that tends to be older (median Age-16 = 25), whiter (78\%), more educated (median YrsEduc = 14), and a second smaller population that is more diverse (70\% white), significantly younger (median Age-16 = 7) and with less education on average (median YrsEduc = 12) (see Tables \ref{tbl:2E_full_regressions_results} and \ref{tbl:ME2_sample_comparison}). The difference between the average age of the two populations is noticeable and may play a role behind the marginal effects for \textit{Age} moving around as much as it does. Notice also that the members of the younger cohort hold lower-skilled jobs: the mean and median values for their occupational attributes are uniformly lower than their older and more educated counterparts. Finally, notice that the "penalty" for occupying a female or non-white body is less severe (and even turns positive for Indian and Asian) in the younger cohort. 

\bigskip

Additional narratives present themselves as the segmentation continues and the number of experts expands. To reduce the chance of confusion the results from the deep three-expert HME model are used in what follows due to its superior likelihood value over the three-expert ME model (see Table \ref{tbl:model_comparison}). The main segmentation discovered by the two-expert ME model is carried over to the three expert HME model while a third latent sub-population emerges. The dominate cluster from the two-expert model is still quite large (78.3\% of the posterior weight) compared to the younger cohort (13.3\% of the posterior weight) and the new third cohort (8.4\%). Three features distinguish this new population:

\begin{enumerate}
  \item It skews slightly older than dominate cluster (27 vs 25 for median age-16)
  \item It is the most educated of the three sub-populations with median years of education equal to 16 (compared to 14 for the dominate cluster and 12 for the younger group).
  \item Members of this group are employed in positions where it is critically important to be aware of and understand others individual's behavior (Perceptive).
\end{enumerate}

Just as with the two-expert ME model, the returns to education vary across these sub-groups. The young cohort, whose typical member has a high school diploma, has the lowest returns to education (0.034). The dominate cohort, whose median educational attainment is an Associate's degree, sees the highest returns to their years of schooling (0.082). There is a drop in returns (0.074) for the third and oldest cohort, even though the educational attainment for that group is the highest of the three groups with the median years of education equaling a Bachelor's degree. Taken together, the HME models suggests there is significant heterogeneity to returns to education over an individual's lifetime, across job types, and even by within similar cohorts.


\begin{landscape}
  \begin{table} \centering
    \caption{Regression Results: Two-Expert, Full Parameter Specification}
      \begin{threeparttable}
        \tabcolsep=0.11cm
        \begin{tabular}[l]{l r l l r l l c r l c r l l r l l r l l}
  
  \hline
  & \multicolumn{6}{c}{ME Regressions$^{1}$} &&  \multicolumn{2}{c}{OLS$^{2}$} && \multicolumn{9}{c}{ME Marginal Effects$^{3}$} \\
  \cmidrule(l){2-7}    \cmidrule(l){9-10}     \cmidrule(l){12-20}
  & Coef.  & $^{[S]}$ & $^{[O]}$ &  Coef. & $^{[S]}$ & $^{[O]}$  && \multicolumn{2}{c}{Coef.} && Both   & $^{[S]}$ & $^{[O]}$ & Experts & $^{[S]}$ & $^{[O]}$ & Gates  & $^{[S]}$ & $^{[O]}$ \\
                  \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){9-10} \cmidrule(l){12-14} \cmidrule(l){15-17} \cmidrule(l){18-20}
  Intercept              &  1.231 & *   & *   &  1.423 & *   & *    &&  1.241 & *     &&  1.225 & *   & *    &  1.260  & *   & *   & -0.040 &     &   \\
  Age-16                 &  0.032 & *   & *   &  0.066 & *   & *    &&  0.035 & *     &&  0.042 &     &      &  0.038  & *   & *   &  0.004 &     &   \\
  $\textrm{Age-16}^{2}$  & -0.000 & *   & *   & -0.002 & *   & *    && -0.001 & *     && -0.001 &     &      & -0.001  & *   & *   & -0.000 &     &   \\
  YrsEduc                &  0.082 & *   & *   &  0.042 & *   & *    &&  0.076 & *     &&  0.076 &     &      &  0.075  & *   & *   &  0.000 &     &   \\
  Female                 & -0.244 & *   & *   & -0.031 & -   & *    && -0.215 & *     && -0.209 & *   & *    & -0.207  & *   & *   & -0.002 &     &   \\
  Af Amer                & -0.076 & *   & *   & -0.044 & *   & *    && -0.076 & *     && -0.076 & +   & *    & -0.071  & *   & *   & -0.005 &     &   \\
  Indian                 & -0.081 & *   & *   & -0.069 & -   & *    && -0.091 & *     && -0.085 & +   & -    & -0.079  & *   & *   & -0.005 &     &   \\
  Asian                  & -0.045 & *   & +   &  0.053 &     & +    && -0.032 & *     && -0.024 &     &      & -0.028  & +   & *   &  0.003 &     &   \\
  Hisp                   & -0.121 & *   & *   & -0.069 & *   & *    && -0.106 & *     && -0.112 & *   & *    & -0.112  & *   & *   & -0.000 &     &   \\
  Creativity             & -0.054 & *   & *   & -0.004 &     &      && -0.046 & *     && -0.044 & *   & *    & -0.045  & *   & *   &  0.002 &     &   \\
  Design                 &  0.080 & *   & *   &  0.080 & *   & *    &&  0.082 & *     &&  0.081 & *   & *    &  0.080  & *   & *   &  0.001 &     &   \\
  Analytics              &  0.133 & *   & *   &  0.107 & *   & *    &&  0.131 & *     &&  0.126 & *   & *    &  0.129  & *   & *   & -0.003 &     &   \\
  Perceptive             &  0.063 & *   & *   & -0.017 & +   & *    &&  0.058 & *     &&  0.053 & *   & *    &  0.049  & *   & *   &  0.004 &     &   \\
  Log-Variance           & -1.651 & *   & *   & -2.675 & *   & *    &&  --    &       &&  --    &     &      &  --     &     &     &  --    &     &   \\
                  \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){9-10} \cmidrule(l){12-14} \cmidrule(l){15-17} \cmidrule(l){18-20}
  Share$^{4}$:           &  0.826 &     &     &  0.174 &     &      &&  1.000 &       &&  --    &     &      &  --     &     &     &  --    &     &   \\
  \hline
        \end{tabular}
        \begin{tablenotes}
          \item Signif. Codes: 0 '*' 0.01 '+', 0.05 '-' 0.1 ' ' 1
          \item Log-Likelihood: ME 0.703, OLS -0.558
          \item $^{[S]}$: Standard errors based on the sandwich variance estimator
          \item $^{[O]}$: Standard errors based on the OPG variance estimator
          \item $^{1}$ Fitted coefficients from the two-expert model with the full parameter specification from equation (\ref{eq:full_formula})
          \item $^{2}$ Fitted coefficients from an OLS regression. These coefficient values can be compared to the HME coefficients to their left as well as to the marginal values to their right
          \item $^{3}$ Marginal effects for the HME model. Standard errors are estimated by equation (\ref{eq:std_errs_full_marginal_effects}).
          \item $^{4}$ The share is calculated by summing the prior weights across observations for each expert.
        \end{tablenotes} \label{tbl:2E_full_regressions_results}
      \end{threeparttable}
  \end{table}
  \end{landscape}
  
  
  \begin{table} \centering
    \caption{Sample Mean Comparison: Two-Expert ME}
    \begin{threeparttable}
      \begin{tabular}[l]{l r r r r}
    \cmidrule{1-5}
    Share:$^{1}$& \multicolumn{2}{c}{(0.826)} & \multicolumn{2}{c}{(0.174)} \\
                & Mean & Median & Mean & Median \\
    \cmidrule{1-5}
    $\log$ Wage (hr)      &   2.679 &   2.681 &  2.175 &  2.197 \\
    Age-16                &  25.814 &  25.000 &  6.965 &  7.000 \\
    $\textrm{Age-16}^{2}$ & 759.812 & 625.000 & 62.478 & 49.000 \\
    Female                &   0.408 &   0.000 &  0.386 &  0.000 \\
    Af Amer               &   0.084 &   0.000 &  0.101 &  0.000 \\
    Indian                &   0.009 &   0.000 &  0.018 &  0.000 \\
    White                 &   0.778 &   1.000 &  0.698 &  1.000 \\
    Hispanic              &   0.037 &   0.000 &  0.028 &  0.000 \\
    Asian                 &   0.091 &   0.000 &  0.155 &  0.000 \\
    YrsEduc               &  13.916 &  14.000 & 12.974 & 12.000 \\
    Creative              &  -0.191 &  -0.137 & -0.464 & -0.542 \\
    Design                &  -0.344 &  -0.535 & -0.442 & -0.635 \\
    Analytic              &  -0.196 &  -0.247 & -0.499 & -0.550 \\
    Perceptive            &   0.230 &   0.127 & -0.233 & -0.532 \\
    \cmidrule{1-5}
    N                     &      -- &  58,939 &     -- &  9,703 \\
    \hline
      \end{tabular}
      \begin{tablenotes}
        \item{\footnotesize $^{1}$ The share is calculated by summing the 
        posterior weights across observations for each expert.}
        \item{\footnotesize \textbf{Note:} Mean and median values are applied to individuals in the census sample that are classified based on the value of their prior weights. For example, observation $i$ is assigned to expert $j$ if the prior vector's largest value is the $j$-th index: $\argmax \boldsymbol{h}_{i} = h_{ij}$}
      \end{tablenotes} \label{tbl:ME2_sample_comparison}
    \end{threeparttable}
  \end{table}
  
  
  \begin{landscape}
  \begin{table} \centering
    \caption{Regression Results: Wide Three-Expert, Full Parameter Specification}
      \begin{threeparttable}
        \tabcolsep=0.11cm
        \begin{tabular}[l]{l r l l r l l r l l c r l c r l l r l l r l l}
  
  \hline
  & \multicolumn{9}{c}{ME Regressions$^{1}$} &&  \multicolumn{2}{c}{OLS$^{2}$} && \multicolumn{9}{c}{ME Marginal Effects$^{3}$} \\
  \cmidrule(l){2-10}    \cmidrule(l){12-13}     \cmidrule(l){15-23}
  & Coef.  & $^{[S]}$ & $^{[O]}$ &  Coef. & $^{[S]}$ & $^{[O]}$ & Coef.  & $^{[S]}$ & $^{[O]}$  && \multicolumn{2}{c}{Coef.} && Both   & $^{[S]}$ & $^{[O]}$ & Experts & $^{[S]}$ & $^{[O]}$ & Gates  & $^{[S]}$ & $^{[O]}$ \\
                        \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){12-13} \cmidrule(l){15-17} \cmidrule(l){18-20} \cmidrule(l){21-23}
  Intercept             &  1.379 &   & * &  1.574 &    & * &  0.562 &    & * &&  1.241 & *     &&  1.367 &   & *    &  1.335 &    & *   &  0.032 &   &     \\
  Age-16                &  0.021 &   & * &  0.045 &    & * &  0.060 &    & * &&  0.035 & *     &&  0.029 &   &      &  0.027 &    & *   &  0.002 &   &     \\
  $\textrm{Age-16}^{2}$ & -0.000 &   & * & -0.001 &    & + & -0.001 &    & * && -0.001 & *     && -0.000 &   &      & -0.000 &    & *   &  0.000 &   &     \\
  YrsEduc               &  0.082 &   & * &  0.032 &    & * &  0.080 &    & * &&  0.076 & *     &&  0.074 &   &      &  0.077 &    & *   & -0.002 &   &     \\
  Female                & -0.251 &   & * & -0.022 &    & + & -0.149 &    & * && -0.215 & *     && -0.206 &   & *    & -0.218 &    & *   &  0.012 &   &     \\
  Af Amer               & -0.084 &   & * & -0.056 &    & * & -0.054 &    &   && -0.076 & *     && -0.076 &   & +    & -0.078 &    & *   &  0.002 &   &     \\
  Indian                & -0.105 & * & * & -0.046 &    &   &  0.010 &    &   && -0.091 & *     && -0.091 &   &      & -0.090 &    & *   & -0.002 &   &     \\
  Asian                 & -0.030 &   & * &  0.057 &    & - & -0.091 &    &   && -0.032 & *     && -0.024 &   &      & -0.025 &    & +   &  0.001 &   &     \\
  Hisp                  & -0.136 &   & * & -0.061 &    & * &  0.071 &    &   && -0.106 & *     && -0.107 &   & *    & -0.111 &    & *   &  0.004 &   &     \\
  Creativity            & -0.038 &   & * & -0.022 &    & * & -0.177 &    & * && -0.046 & *     && -0.044 &   & -    & -0.047 &    & *   &  0.003 &   &     \\
  Design                &  0.080 & * & * &  0.080 & *  & * & -0.037 &    & - &&  0.082 & *     &&  0.075 &   & +    &  0.071 &    & *   &  0.004 &   &     \\
  Analytics             &  0.123 & + & * &  0.110 &    & * &  0.196 &    & * &&  0.131 & *     &&  0.128 & * & *    &  0.128 &    & *   &  0.000 &   &     \\
  Perceptive            &  0.060 &   & * & -0.008 &    &   &  0.168 &    & * &&  0.058 & *     &&  0.057 &   & -    &  0.061 &    & *   & -0.004 &   &     \\
  Log-Variance          & -1.893 & * & * & -2.891 &    & * & -0.627 &    & * &&  --    &       &&        &   &      &        &    &     &        &   &     \\
                        \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){12-13} \cmidrule(l){15-17} \cmidrule(l){18-20} \cmidrule(l){21-23}
  Share$^{4}$:          & 0.811  &   &   & 0.110  &    &   &  0.080 &    &   &&  1.000 &       &&  --    &   &      &  --    &    &     &  --    &   &     \\
  \hline
        \end{tabular}
        \begin{tablenotes}
          \item Signif. Codes: 0 '*' 0.01 '+', 0.05 '-' 0.1 ' ' 1
          \item Log-Likelihood: ME 0.718, OLS -0.558
          \item $^{[S]}$: Standard errors based on the sandwich variance estimator
          \item $^{[O]}$: Standard errors based on the OPG variance estimator
          \item $^{1}$ Fitted coefficients from the three-expert model with the full parameter specification from equation (\ref{eq:full_formula})
          \item $^{2}$ Fitted coefficients from an OLS regression. These coefficient values can be compared to the HME coefficients to their left as well as to the marginal values to their right
          \item $^{3}$ Marginal effects for the HME model. Standard errors are estimated by equation (\ref{eq:std_errs_full_marginal_effects}).
          \item $^{4}$ The share is calculated by summing the prior weights across observations for each expert.
        \end{tablenotes} \label{tbl:3W_full_regressions_results}
      \end{threeparttable}
  \end{table}
  \end{landscape}
  
  
  \begin{table} \centering
    \caption{Sample Mean Comparison: Wide Three-Expert HME}
    \begin{threeparttable}
      \begin{tabular}[l]{l r r r r r r}
    \cmidrule{1-7}
    Share:$^{1}$& \multicolumn{2}{c}{(0.809)} & \multicolumn{2}{c}{(0.111)} & \multicolumn{2}{c}{(0.080)} \\
                & Mean & Median & Mean & Median & Mean & Median \\
    \cmidrule{1-7}
    $\log$ Wage (hr)      &   2.664 &   2.667 &  2.106 &  2.096 &   2.549 &   2.221 \\
    Age-16                &  24.916 &  24.000 &  5.830 &  6.000 &  27.827 &  28.000 \\
    $\textrm{Age-16}^{2}$ & 722.355 & 576.000 & 41.789 & 36.000 & 904.103 & 784.000 \\
    Female                &   0.420 &   0.000 &  0.301 &  0.000 &   0.250 &   0.000 \\
    Af Amer               &   0.090 &   0.000 &  0.060 &  0.000 &   0.064 &   0.000 \\
    Indian                &   0.010 &   0.000 &  0.012 &  0.000 &   0.010 &   0.000 \\
    Hispanic              &   0.036 &   0.000 &  0.020 &  0.000 &   0.102 &   0.000 \\
    Asian                 &   0.100 &   0.000 &  0.114 &  0.000 &   0.045 &   0.000 \\
    YrsEduc               &  13.802 &  14.000 & 13.101 & 12.000 &  15.837 &  16.000 \\
    Creative              &  -0.209 &  -0.141 & -0.422 & -0.456 &  -0.195 &  -0.282 \\
    Design                &  -0.344 &  -0.535 & -0.387 & -0.535 &  -0.765 &  -0.860 \\
    Analytic              &  -0.218 &  -0.264 & -0.472 & -0.412 &  -0.072 &   0.049 \\
    Perceptive            &   0.177 &   0.127 & -0.122 & -0.455 &   0.851 &   0.877 \\
    \cmidrule{1-7}
    N                     &      -- &  60,396 &     -- &  6,603 &      -- &   1,643 \\
    \hline
      \end{tabular}
      \begin{tablenotes}
        \item{\footnotesize $^{1}$ The share is calculated by summing the 
        posterior weights across observations for each expert.}
        \item{\footnotesize \textbf{Note:} Mean and median values are applied to individuals in the census sample that are classified based on the value of their prior weights. For example, observation $i$ is assigned to expert $j$ if the prior vector's largest value is the $j$-th index: $\argmax \boldsymbol{h}_{i} = h_{ij}$}
      \end{tablenotes} \label{tbl:ME3_sample_comparison}
    \end{threeparttable}
  \end{table}
  
  
  \begin{landscape}
    \begin{table} \centering
      \caption{Regression Results: Deep Three-Expert, Full Parameter Specification}
        \begin{threeparttable}
          \tabcolsep=0.11cm
          \begin{tabular}[l]{l r l l r l l r l l c r l c r l l r l l r l l}
  
            \hline
            & \multicolumn{9}{c}{ME Regressions$^{1}$} &&  \multicolumn{2}{c}{OLS$^{2}$} && \multicolumn{9}{c}{ME Marginal Effects$^{3}$} \\
            \cmidrule(l){2-10}    \cmidrule(l){12-13}     \cmidrule(l){15-23}
            & Coef.  & $^{[S]}$ & $^{[O]}$ &  Coef. & $^{[S]}$ & $^{[O]}$ & Coef.  & $^{[S]}$ & $^{[O]}$  && \multicolumn{2}{c}{Coef.} && Both   & $^{[S]}$ & $^{[O]}$ & Experts & $^{[S]}$ & $^{[O]}$ & Gates  & $^{[S]}$ & $^{[O]}$ \\
            \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){12-13} \cmidrule(l){15-17} \cmidrule(l){18-20} \cmidrule(l){21-23}
    Intercept             &  1.404 & * & *   &  1.559 & * & *   &  0.898 & * & *   &&  1.241 & *     &&  1.393 & + & *     &  1.382 & * & *     &  0.011 &  \\
    Age-16                &  0.020 & * & *   &  0.050 & * & *   &  0.044 & * & *   &&  0.035 & *     &&  0.028 &   &       &  0.026 & * & *     &  0.003 &  \\
    $\textrm{Age-16}^{2}$ & -0.000 & * & *   & -0.001 & * & *   & -0.001 & * & *   && -0.001 & *     && -0.000 &   &       & -0.000 & * & *     &  0.000 &  \\
    YrsEduc               &  0.082 & * & *   &  0.034 & * & *   &  0.074 & * & *   &&  0.076 & *     &&  0.073 &   &       &  0.075 & * & *     & -0.001 &  \\
    Female                & -0.257 & * & *   & -0.034 & * & *   & -0.131 & * & *   && -0.215 & *     && -0.209 & * & *     & -0.217 & * & *     &  0.008 &  \\
    Af Amer               & -0.086 & * & *   & -0.048 & * & *   & -0.041 &   &     && -0.076 & *     && -0.076 &   & +     & -0.077 & * & *     &  0.001 &  \\
    Indian                & -0.113 & * & *   & -0.057 &   & -   &  0.043 &   &     && -0.091 & *     && -0.100 &   &       & -0.093 & * & *     & -0.007 &  \\
    Asian                 & -0.033 & * & *   &  0.058 & + & +   & -0.062 &   &     && -0.032 & *     && -0.025 &   &       & -0.023 & + & +     & -0.001 &  \\
    Hisp                  & -0.143 & * & *   & -0.066 & * & *   &  0.077 &   &     && -0.106 & *     && -0.111 &   & *      & -0.114 & * & *     &  0.003 &  \\
    Creativity            & -0.042 & * & *   & -0.021 & + & *   & -0.136 & * & *   && -0.046 & *     && -0.043 &   & +     & -0.047 & * & *     &  0.004 &  \\
    Design                &  0.080 & * & *   &  0.068 & * & *   & -0.048 & + & +   &&  0.082 & *     &&  0.074 & + & *     &  0.068 & * & *     &  0.006 &  \\
    Analytics             &  0.124 & * & *   &  0.112 & * & *   &  0.183 & * & *   &&  0.131 & *     &&  0.128 & * & *     &  0.127 & * & *     &  0.000 &  \\
    Perceptive            &  0.063 & * & *   & -0.003 &   &     &  0.135 & * & *   &&  0.058 & *     &&  0.056 & + & *     &  0.061 & * & *     & -0.004 &  \\
    Log-Variance          & -1.895 & * & *   & -2.791 & * & *   & -0.622 & * & *   &&  --    &       &&        &   &       &        &   &       &        &  \\
    \cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){12-13} \cmidrule(l){15-17} \cmidrule(l){18-20} \cmidrule(l){21-23}
    Share$^{4}$:          & 0.783  &   &     & 0.133  &   &     & 0.084  &   &     &&  1.000 &       &&  --    &   &       &  --    &   &       &  --    &  \\
    \cmidrule{1-17}
          \end{tabular}
    
          \begin{tablenotes}
            \item Signif. Codes: 0 '*' 0.01 '+', 0.05 '-' 0.1 ' ' 1
            \item Log-Likelihood: HME 0.719, OLS -0.558
            \item $^{[S]}$: Standard errors based on the sandwich variance estimator
            \item $^{[O]}$: Standard errors based on the OPG variance estimator
            \item $^{1}$ Fitted coefficients from the three-expert model with the full parameter specification from equation (\ref{eq:full_formula})
            \item $^{2}$ Fitted coefficients from an OLS regression. These coefficient values can be compared to the HME coefficients to their left as well as to the marginal values to their right
            \item $^{3}$ Marginal effects for the HME model. Standard errors are estimated by equation (\ref{eq:std_errs_full_marginal_effects}).
            \item $^{4}$ The share is calculated by summing the prior weights across observations for each expert.
    
          \end{tablenotes} \label{tbl:3D_full_regressions_results}
    
    
        \end{threeparttable}
    
    \end{table}
    \end{landscape}
  
  
    \begin{table} \centering
      \caption{Sample Mean Comparison: Deep Three-Expert HME}
      \begin{threeparttable}
        \begin{tabular}[l]{l r r r r r r}
      \cmidrule{1-7}
      Share:$^{1}$& \multicolumn{2}{c}{(0.783)} & \multicolumn{2}{c}{(0.133)} & \multicolumn{2}{c}{(0.084)} \\
                  & Mean & Median & Mean & Median & Mean & Median \\
      \cmidrule{1-7}
      $\log$ Wage (hr)      &   2.683 &   2.676 &  2.137 &  2.140 &   2.432 &   2.075 \\
      Age-16                &  25.523 &  25.000 &  6.494 &  7.000 &  26.913 &  27.000 \\
      $\textrm{Age-16}^{2}$ & 746.250 & 625.000 & 50.858 & 49.000 & 873.924 & 729.000 \\
      Female                &   0.414 &   0.000 &  0.358 &  0.000 &   0.313 &   0.000 \\
      Af Amer               &   0.088 &   0.000 &  0.073 &  0.000 &   0.075 &   0.000 \\
      Indian                &   0.010 &   0.000 &  0.016 &  0.000 &   0.018 &   0.000 \\
      White                 &   0.770 &   1.000 &  0.753 &  1.000 &   0.749 &   1.000 \\
      Hispanic              &   0.036 &   0.000 &  0.027 &  0.000 &   0.101 &   0.000 \\
      Asian                 &   0.096 &   0.000 &  0.131 &  0.000 &   0.057 &   0.000 \\
      YrsEduc               &  13.846 &  14.000 & 13.077 & 12.000 &  15.378 &  16.000 \\
      Creative              &  -0.198 &  -0.137 & -0.444 & -0.508 &  -0.201 &  -0.282 \\
      Design                &  -0.330 &  -0.530 & -0.477 & -0.635 &  -0.757 &  -0.859 \\
      Analytic              &  -0.206 &  -0.253 & -0.471 & -0.412 &  -0.161 &  -0.007 \\
      Perceptive            &   0.185 &   0.127 & -0.082 & -0.308 &   0.756 &   0.877 \\
      \cmidrule{1-7}
      N                     &      -- &  58,429 &     -- &  8,674 &      -- &   1,539 \\
      \hline
        \end{tabular}
        \begin{tablenotes}
          \item{\footnotesize $^{1}$ The share is calculated by summing the 
          posterior weights across observations for each expert.}
          \item{\footnotesize \textbf{Note:} Mean and median values are applied to individuals in the census sample that are classified based on the value of their prior weights. For example, observation $i$ is assigned to expert $j$ if the prior vector's largest value is the $j$-th index: $\argmax \boldsymbol{h}_{i} = h_{ij}$}
        \end{tablenotes} \label{tbl:HME3_sample_comparison}
      \end{threeparttable}
    \end{table}
  

\clearpage


\section{Monte Carlo Exercise} \label{sec:MonteCarlo}

As a final exercise, a series of simulation exercises are designed to better understand the practical behavior of the (H)ME models as the relative size and heterogeneity of the sub-populations change, changes in the strength of association between the variables in the model, and misspecification of the number of experts. Both experiments will use the same Monte Carlo framework discussed below to generate the data but will differ slightly in how the observations are allocated to one expert or another. All experts will have the following function form. Each local expert regression will have two dependent variables $(X, Z)$ and normally distributed error terms. 

\begin{equation} \label{eq:MC_regression_eq}
    y_{i,t} = \beta_{i}^{c} + \beta_{i}^{x}  x_{t} + \beta_{i}^{z}  z_{t} + \varepsilon_{i,t}
\end{equation}

\begin{equation} \label{eq:MC_regression_errors}
    \varepsilon_{i,t} \thicksim N(0, \sigma^{2}_{i})
\end{equation}

Recall from Section \ref{sec:Inference} that the variance parameter is not estimated directly. Rather, a transform is used to remove the non-zero constraint. This transform is defined as $\phi_{i} = \log(\sigma^{2}_{i})$. The Monte Carlo exercise will follow this convention when setting parameters that govern the underlying data generating process. The random variable $X$ will be distributed as a standard normal:

\begin{equation} \label{eq:MC_X_distribution}
    X \thicksim N(0, 1)
\end{equation}

The random variable $Z$ will be correlated with $X$ in the following manner:

\begin{equation} \label{eq:MC_XZ_correlation}
    Z = \Phi \left( \frac{(X + \rho X_{1})}{\sqrt{1 + \rho^{2}}} \right)
\end{equation}

where $X_{1}$ is also sampled from a standard normal distribution independent of $X$. The function $\Phi$ is the CDF of the standard normal. The strength of the correlation will be governed by parameter $\rho$. Smaller values of $\rho$ will lead to a more tightly coupled relationship. Given the definitions in Equations (\ref{eq:MC_X_distribution}) and (\ref{eq:MC_XZ_correlation}) the joint distribution of $(X, Z)$ will have a mean vector: $\boldsymbol{\mu} = (0, 0.5)$. The HME will use the gating node to split on the $Z$ covariate and then run the regression in Equation (\ref{eq:MC_regression_eq}). Using the formula notation in this essay the HME regression equation is summarized in Equation (\ref{eq:MC_hme_formula}).

\begin{equation} \label{eq:MC_hme_formula}
    Y = X + Z \,\,|\,\, Z
\end{equation}


\subsection{Experiment One}

The first experiment will examine the variability of the estimated model parameters across Monte Carlo samples and compare the standard deviation of the converged parameter values to the standard errors produced by the three available variance-covariance matrices (the outer-product-of-the-gradient, the hessian, and the sandwich estimator). The baseline sampling procedure discussed above will be used to create 1000 Monte Carlo samples. Each sample will have 1000 observations. After the sample has been created, the probability of membership to each sub-population is then calculated as a function of Z. The logistic function will be used estimate these probabilities:

\begin{equation} \label{eq:general_logistic}
    P(z) = \frac{1}{1 + \exp\{\alpha (c - z)\}}
\end{equation}

In this version of the logistic function, the parameter $c$ governs the location of the mid-point while $\alpha$ controls the severity of the slope at the mid-point. A 2-expert ME model will be fit on each sample and the converged parameter estimates and their standard errors will be recorded. This process will then be repeated at different values for the parameters governing the association between X and Z and for the parameters that govern the probabilities of group membership. For the baseline model, the sampling and modeling parameters are set to the values below:

\begin{itemize}
    \item Simulation Parameters: $\rho = 1$,\; $c = 0.5$,\; $\alpha = 8$
    \item Expert One Parameters: $\beta_{1}^{c} = 0.8$, \; $\beta_{1}^{x} = 2.0$, \; $\beta_{1}^{z} = 1.1$, \; $\phi_{1} = -3.429597$
    \item Expert Two Parameters: $\beta_{2}^{c} = 1.2$, \; $\beta_{2}^{x} = 1.5$, \; $\beta_{2}^{z} = 0.5$, \; $\phi_{2} = -3.218876$
\end{itemize}

Three experiments will be run:

\begin{enumerate}
    \item The homogeneity of each sub-population will be altered by selecting different values of the slope parameter ($\alpha$) in the logistic function described in Equation \ref{eq:general_logistic}. Larger values of $\alpha$ lead to a sharper delineation in probabilities (along the Z-axis) of membership between the two sub-populations while values close to zero lead to an equal probability of belonging each sub-group, regardless of the value of Z. Analysis will be run at the following values: $\alpha = [125, 25, 8, 4, 2, 1, 0.01]$. The left side of Figure \ref{fig:MC_logistic_pars} shows the logistic curve with different values of $\alpha$.
    
    \item The relative size of the two sub-populations can be controlled by manipulating the mid-point parameter ($c$) of the logistic function. At the baseline value of 0.5, every Monte Carlo sample will have roughly equal proportions of the two groups. Smaller values of $c$ will lead to a larger number of observations in the sample whose conditional mean will be governed by expert regression two. Analysis will be run at the following values: $c = [0, 0.1, 0.2, 0.3, 0.4, 0.5]$. The right side of Figure \ref{fig:MC_logistic_pars} shows the logistic curve with different values of $c$.
  
    \item The strength of association between $X_{1}$ and $Z$ is controlled by $\rho$ in Equation \ref{eq:MC_XZ_correlation}. Values closer to zero lead to a more tightly coupled relationship while values farther from zero introduce more noise between $X_{1}$ and $Z$. Analysis will be run at the following values: $\rho = [0.125, 0.25, 0.5, 1, 2, 4]$. Figure \ref{fig:MC_rho} shows the different samples of $(X, Z)$ drawn with different values of $\rho$.
\end{enumerate}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{MonteCarlo_logistic_parameters.jpeg}
  \caption{The effect of different parameter values for the logistic function. On the left the is the slope ($\alpha$) parameter, which regulates how homogenous the two sub-groups are. To the right the mid-point parameter ($c$). which indicates the value of $Z$ where there is an equal chance of belonging to either sub-population. A bolded line represents the baseline values of $\alpha = 8$ and $c = 0.5$.}
  \label{fig:MC_logistic_pars}
\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{MonteCarlo_rho.jpeg}
    \caption{The effect of different values of $\rho$ on the strength of association between the joint distribution of $(X, Z)$. The baseline model sets $\rho = 1$.} 
    \label{fig:MC_rho}
\end{figure}

The results of the first experiment are presented in two ways. First, we plot the comparisons between the standard deviation of the fitted parameter estimates against the average standard error of the parameter estimates resulting from each fitted model. Three different average standard error values are calculated, one for each of the three variance-covariance matrices described in Section \ref{sec:Inference}. These results are summarized in Figures \ref{fig:MonteCarlo_results_c} - \ref{fig:MonteCarlo_results_rho}. The main take-away from these figures is agreement between the standard deviation of the parameter values with the average standard error, regardless of the variance-covariance estimator used, for every parameter except the log-variance. Surprisingly, for $\phi_{i}$, the OPG estimator is the only estimator that produces standard errors in-line with the standard deviation of the estimated log-variance while the sandwich estimator's estimates are the most biased towards zero. This pattern is consistent across the three simulation parameters that are being altered: $\rho$, c, and $\alpha$. It is not immediately clear the reason behind this discrepancy and the accuracy of the the OPG estimator over the Hessian and Sandwich estimators. 

\bigskip

A second summary of the Monte Carlo exercise is provided in Tables \ref{tbl:coverage_ratio_c} - \ref{tbl:coverage_ratio_rho}. These tables provide the coverage probability that the true parameter values underpinning the DGP are contained in a confidence interval produced by each model's fitted parameter vector and variance-covariance estimator. The nominal coverage probability is set at 95\% and the percentages in the table are the proportion of times the confidence interval failed to capture the true parameter value. The general pattern captured in Figures \ref{fig:MonteCarlo_results_c} - \ref{fig:MonteCarlo_results_rho} holds here as well: coverage ratios for the regression parameters ($\boldsymbol{\beta_{i}}$) hew closely to the nominal fail rate of 5\% across variance estimators but are drastically inflated for the log-variance parameters ($\phi_{i}$). The value of this view is its demonstration of the practical impact of using an inappropriate variance estimator for this simulation study. For the Hessian estimator, the 95\% confidence intervals fail to capture the true parameter values of $\phi_{i}$ from 18 to 23 percent of the time. This ratio jumps to a range of 39 to 47 percent of the time if the Sandwich estimator is used. 

\begin{figure}[t!]
  \includegraphics[width=\textwidth]{MonteCarlo_results_c.jpeg}
  \caption{Comparison of average standard errors versus the standard deviation of fitted parameter vectors at different values of logistic parameter $c$. Smaller values of $c$ result in a greater imbalance in size between the two sub-populations (see Figure \ref{fig:MC_logistic_pars}). In this experiment, smaller values of $c$ lead to larger number of observations in the sample whose conditional mean will be governed by expert regression two and fewer observations whose conditional mean is governed by expert regression one. The relationship is clear: the greater the number of observations in a sub-population the more precise (smaller values of the standard errors) estimated parameter vector will be.}
  \label{fig:MonteCarlo_results_c}
\end{figure}


\begin{figure}[t!]
  \includegraphics[width=\textwidth]{MonteCarlo_results_alpha.jpeg}
  \caption{Comparison of average standard errors versus the standard deviation of fitted parameter vectors at different values of logistic parameter $\alpha$. Larger values of $\alpha$ result in more homogenous sub-populations (see Figure \ref{fig:MC_logistic_pars}). More homogenous groups in the data lead to smaller standard errors for the log-variance parameter and increasing standard errors for in the $Z$ variable. Both these parameters have noticeably larger standard error values than the regression coefficient on the $X$ variable and the intercept term.}
  \label{fig:MonteCarlo_results_alpha}
\end{figure}


\begin{figure}[t!]
  \includegraphics[width=\textwidth]{MonteCarlo_results_rho.jpeg}
  \caption{Comparison of average standard errors versus the standard deviation of fitted parameter vectors at different values of association parameter $\rho$. Standard errors for the regression coefficients share a similar decreasing relationship with the value of $\rho$ while standard errors for the log-variance parameter remain unaffected and flat. As the data become less tightly coupled in $(X, Z)$-space (see Figure \ref{fig:MC_rho}) standard errors decline.}
  \label{fig:MonteCarlo_results_rho}
\end{figure}


\begin{table}[b!] \centering
  \caption{Simulated Coverage Probability: Mid-point c}
  \begin{threeparttable}
    {\footnotesize
    \begin{tabular}[r]{l l l r r r r r r r r}
  \hline
Std Err Type & Value & $\beta_{1}^{c}$ & $\beta_{1}^{x}$ & $\beta_{1}^{z}$ & $\phi_{1}$ & $\beta_{2}^{c}$ & $\beta_{2}^{x}$ & $\beta_{2}^{z}$ & $\phi_{2}$ \\ 
  \hline

Sandwich  & 0.0 & 0.058 & 0.067 & 0.075 & 0.083 & 0.046 & 0.057 & 0.051 & 0.064 \\
          & 0.1 & 0.066 & 0.055 & 0.072 & 0.080 & 0.066 & 0.063 & 0.060 & 0.053 \\
          & 0.2 & 0.055 & 0.057 & 0.051 & 0.079 & 0.057 & 0.055 & 0.052 & 0.061 \\
          & 0.3 & 0.059 & 0.058 & 0.059 & 0.051 & 0.045 & 0.047 & 0.043 & 0.051 \\
          & 0.4 & 0.049 & 0.047 & 0.052 & 0.061 & 0.069 & 0.057 & 0.066 & 0.061 \\
          & 0.5 & 0.043 & 0.051 & 0.053 & 0.049 & 0.052 & 0.065 & 0.045 & 0.069 \\
          &     &       &       &       &       &       &       &       &       \\
Hessian   & 0.0 & 0.058 & 0.067 & 0.075 & 0.083 & 0.046 & 0.057 & 0.051 & 0.064 \\
          & 0.1 & 0.066 & 0.055 & 0.072 & 0.080 & 0.066 & 0.063 & 0.060 & 0.053 \\
          & 0.2 & 0.055 & 0.057 & 0.051 & 0.079 & 0.057 & 0.055 & 0.052 & 0.061 \\
          & 0.3 & 0.059 & 0.058 & 0.059 & 0.051 & 0.045 & 0.047 & 0.043 & 0.051 \\
          & 0.4 & 0.049 & 0.047 & 0.052 & 0.061 & 0.069 & 0.057 & 0.066 & 0.061 \\
          & 0.5 & 0.043 & 0.051 & 0.053 & 0.049 & 0.052 & 0.065 & 0.045 & 0.069 \\
          &     &       &       &       &       &       &       &       &       \\
OPG       & 0.0 & 0.048 & 0.052 & 0.038 & 0.055 & 0.049 & 0.058 & 0.050 & 0.054 \\
          & 0.1 & 0.057 & 0.047 & 0.051 & 0.076 & 0.066 & 0.064 & 0.056 & 0.051 \\
          & 0.2 & 0.048 & 0.043 & 0.052 & 0.064 & 0.059 & 0.054 & 0.053 & 0.058 \\
          & 0.3 & 0.054 & 0.044 & 0.042 & 0.046 & 0.047 & 0.044 & 0.041 & 0.046 \\
          & 0.4 & 0.051 & 0.035 & 0.044 & 0.050 & 0.064 & 0.046 & 0.065 & 0.057 \\
          & 0.5 & 0.041 & 0.045 & 0.047 & 0.047 & 0.049 & 0.051 & 0.045 & 0.057 \\
  \hline
    \end{tabular}
    }
    \begin{tablenotes}
      \item{\footnotesize One minus the coverage probability for logistic parameter c. For 1000 simulated samples, the proportions in the table indicate how often the constructed 95\% confidence intervals fail to captures the true parameter value. This table corresponds to Figure \ref{fig:MonteCarlo_results_c}. $CI_{0.95} = \hat\beta \, \pm \, 1.96 SE_{\hat{\beta}}$.}
    \end{tablenotes} \label{tbl:coverage_ratio_c}
  \end{threeparttable}
\end{table}


\begin{table}[b!] \centering
  \caption{Simulated Coverage Probability: Slope $\alpha$}
  \begin{threeparttable}
    {\footnotesize
    \begin{tabular}[r]{l l r r r r r r r r}
  \hline
Std Err Type & Value & $\beta_{1}^{c}$ & $\beta_{1}^{x}$ & $\beta_{1}^{z}$ & $\phi_{1}$ & $\beta_{2}^{c}$ & $\beta_{2}^{x}$ & $\beta_{2}^{z}$ & $\phi_{2}$ \\ 
  \hline

Sandwich  & 0.01 & 0.039 & 0.042 & 0.034 & 0.049 & 0.062 & 0.063 & 0.065 & 0.062 \\
          &    1 & 0.057 & 0.046 & 0.047 & 0.046 & 0.057 & 0.046 & 0.055 & 0.040 \\
          &    2 & 0.055 & 0.069 & 0.056 & 0.051 & 0.051 & 0.057 & 0.057 & 0.048 \\
          &    4 & 0.059 & 0.055 & 0.061 & 0.056 & 0.051 & 0.052 & 0.051 & 0.055 \\
          &    8 & 0.043 & 0.051 & 0.053 & 0.049 & 0.052 & 0.065 & 0.045 & 0.069 \\
          &   25 & 0.060 & 0.049 & 0.058 & 0.062 & 0.068 & 0.052 & 0.065 & 0.056 \\
          &  125 & 0.058 & 0.060 & 0.062 & 0.070 & 0.055 & 0.045 & 0.054 & 0.059 \\
          &      &       &       &       &       &       &       &       &       \\
Hessian   & 0.01 & 0.035 & 0.049 & 0.039 & 0.047 & 0.065 & 0.060 & 0.063 & 0.063 \\
          &    1 & 0.052 & 0.043 & 0.046 & 0.046 & 0.056 & 0.047 & 0.058 & 0.040 \\
          &    2 & 0.058 & 0.069 & 0.048 & 0.048 & 0.052 & 0.054 & 0.057 & 0.046 \\
          &    4 & 0.060 & 0.053 & 0.058 & 0.054 & 0.049 & 0.051 & 0.043 & 0.053 \\
          &    8 & 0.043 & 0.046 & 0.050 & 0.050 & 0.049 & 0.057 & 0.044 & 0.064 \\
          &   25 & 0.057 & 0.047 & 0.058 & 0.058 & 0.065 & 0.049 & 0.060 & 0.050 \\
          &  125 & 0.055 & 0.059 & 0.061 & 0.067 & 0.051 & 0.047 & 0.051 & 0.055 \\
          &      &       &       &       &       &       &       &       &       \\
OPG       & 0.01 & 0.036 & 0.050 & 0.042 & 0.045 & 0.064 & 0.051 & 0.062 & 0.057 \\
          &    1 & 0.046 & 0.038 & 0.048 & 0.043 & 0.058 & 0.051 & 0.059 & 0.034 \\
          &    2 & 0.056 & 0.062 & 0.048 & 0.043 & 0.053 & 0.047 & 0.061 & 0.041 \\
          &    4 & 0.065 & 0.048 & 0.056 & 0.051 & 0.042 & 0.044 & 0.042 & 0.048 \\
          &    8 & 0.041 & 0.045 & 0.047 & 0.047 & 0.049 & 0.051 & 0.045 & 0.057 \\
          &   25 & 0.058 & 0.045 & 0.058 & 0.053 & 0.065 & 0.047 & 0.061 & 0.049 \\
          &  125 & 0.056 & 0.061 & 0.059 & 0.066 & 0.046 & 0.043 & 0.046 & 0.052 \\
  \hline
    \end{tabular}
    }
    \begin{tablenotes}
      \item{\footnotesize One minus the coverage probability for logistic parameter $\alpha$. For 1000 simulated samples, the proportions in the table indicate how often the constructed 95\% confidence intervals fail to captures the true parameter value. This table corresponds to Figure \ref{fig:MonteCarlo_results_alpha}. $CI_{0.95} = \hat\beta \, \pm \, 1.96 SE_{\hat{\beta}}$}
    \end{tablenotes} \label{tbl:coverage_ratio_alpha}
  \end{threeparttable}
\end{table}


\begin{table}[b!] \centering
  \caption{Simulated Coverage Probability: Association $\rho$}
  \begin{threeparttable}
    {\footnotesize
    \begin{tabular}[r]{l l r r r r r r r r}
  \hline
Std Err Type & Value & $\beta_{1}^{c}$ & $\beta_{1}^{x}$ & $\beta_{1}^{z}$ & $\phi_{1}$ & $\beta_{2}^{c}$ & $\beta_{2}^{x}$ & $\beta_{2}^{z}$ & $\phi_{2}$ \\ 
  \hline

Sandwich  & 0.125 & 0.072 & 0.064 & 0.073 & 0.065 & 0.052 & 0.045 & 0.055 & 0.071 \\
          &  0.25 & 0.041 & 0.044 & 0.036 & 0.063 & 0.051 & 0.049 & 0.059 & 0.061 \\
          &   0.5 & 0.043 & 0.038 & 0.047 & 0.064 & 0.065 & 0.052 & 0.062 & 0.065 \\
          &     1 & 0.043 & 0.051 & 0.053 & 0.049 & 0.052 & 0.065 & 0.045 & 0.069 \\
          &     2 & 0.044 & 0.049 & 0.048 & 0.066 & 0.066 & 0.063 & 0.062 & 0.050 \\
          &     4 & 0.043 & 0.049 & 0.052 & 0.063 & 0.065 & 0.061 & 0.062 & 0.055 \\
          &       &       &       &       &       &       &       &       &       \\
Hessian   & 0.125 & 0.063 & 0.058 & 0.065 & 0.062 & 0.053 & 0.043 & 0.054 & 0.061 \\
          &  0.25 & 0.039 & 0.040 & 0.043 & 0.061 & 0.048 & 0.049 & 0.054 & 0.057 \\
          &   0.5 & 0.044 & 0.035 & 0.046 & 0.059 & 0.062 & 0.047 & 0.060 & 0.063 \\
          &     1 & 0.043 & 0.046 & 0.050 & 0.050 & 0.049 & 0.057 & 0.044 & 0.064 \\
          &     2 & 0.045 & 0.050 & 0.047 & 0.064 & 0.061 & 0.056 & 0.061 & 0.044 \\
          &     4 & 0.047 & 0.049 & 0.048 & 0.057 & 0.066 & 0.059 & 0.060 & 0.051 \\
          &       &       &       &       &       &       &       &       &       \\
OPG       & 0.125 & 0.052 & 0.052 & 0.061 & 0.052 & 0.048 & 0.045 & 0.049 & 0.055 \\
          &  0.25 & 0.039 & 0.034 & 0.038 & 0.049 & 0.040 & 0.050 & 0.050 & 0.059 \\
          &   0.5 & 0.040 & 0.031 & 0.048 & 0.053 & 0.054 & 0.048 & 0.055 & 0.055 \\
          &     1 & 0.041 & 0.045 & 0.047 & 0.047 & 0.049 & 0.051 & 0.045 & 0.057 \\
          &     2 & 0.049 & 0.047 & 0.047 & 0.052 & 0.061 & 0.054 & 0.063 & 0.041 \\
          &     4 & 0.041 & 0.049 & 0.046 & 0.055 & 0.061 & 0.056 & 0.060 & 0.049 \\
  \hline
    \end{tabular}
    }
    \begin{tablenotes}
      \item{\footnotesize One minus the coverage probability for association  parameter $\rho$. For 1000 simulated samples, the proportions in the table indicate how often the constructed 95\% confidence intervals fail to captures the true parameter value. This table corresponds to Figure \ref{fig:MonteCarlo_results_rho}. $CI_{0.95} = \hat\beta \, \pm \, 1.96 SE_{\hat{\beta}}$.}
    \end{tablenotes} \label{tbl:coverage_ratio_rho}
  \end{threeparttable}
\end{table}


\clearpage

\subsection{Experiment Two} \label{subsec:ExperimentTwo}

A second experiment is designed to gauge the behavior and performance of the Voung LR test and it's ability to correctly discriminate between different (H)ME models when the number experts in the model does not match the true number of latent sub-populations in the data. The simulation has the following structure. The baseline sampling procedure described in Section \ref{sec:MonteCarlo} will be used to create three different samples. A new probability model will then be used to allocate observations into two, three, and four groups -- one for each of the three samples. For each of the newly created samples, a 2, 3, and 4-expert ME and HME model will be fit to the data and each fitted model will be compared to the others with Voung's LR test, producing a table similar to Table \ref{tbl:voung_comparison}. A multinomial logistic regression (as described in Equation \ref{eq:softmax}) will be used to create the probabilities of group membership and, like the logistic function in Equation \ref{eq:general_logistic}, the probabilities of group membership is based on the variable $Z$ and an intercept term. The coefficients for the sampling and regression experts will be set to the following: 

\begin{itemize}
  \item Simulation Parameters: $\rho = 1$
  \item Expert One Parameters: \, $\beta_{1}^{c} = 1.2$, \; $\beta_{1}^{x} = 1.5$, \; $\beta_{1}^{z} = 0.5$, \; $\phi_{1} = -3.218876$
  \item Expert Two Parameters: \, $\beta_{2}^{c} = 0.8$, \; $\beta_{2}^{x} = 2.0$, \; $\beta_{2}^{z} = 1.1$, \; $\phi_{2} = -3.429597$
  \item Expert Three Parameters: $\beta_{3}^{c} = 0.4$, \; $\beta_{3}^{x} = 0.8$, \; $\beta_{3}^{z} = 2.0$, \; $\phi_{3} = -3.543914$
  \item Expert Four Parameters: \, $\beta_{4}^{c} = 1.5$, \; $\beta_{4}^{x} = 0.5$, \; $\beta_{4}^{z} = 0.8$, \; $\phi_{4} = -3.321462$
\end{itemize}

The parameters for the multinomial logistic regression are set to:

\begin{itemize}
  \item Multinomial One Parameters: \, $\omega_{1}^{c} = -4$, \; $\omega_{1}^{z} = 6$
  \item Multinomial Two Parameters: \, $\omega_{2}^{c} = -2$, \; $\omega_{2}^{z} = -6$
  \item Multinomial Three Parameters: $\omega_{3}^{c} = 6$, \; $\omega_{3}^{z} = 18$
  \item Multinomial Four Parameters: \, $\omega_{4}^{c} = -15$, \; $\omega_{4}^{z} = 20$
\end{itemize}

The identification restricting requiring $\boldsymbol{\omega}_{4} = 0$ is ignored. For the sample with two sub-populations, the first two parameter vectors for the regressions and multinomial splits are used. For the three sub-population sample, parameters vectors one through three are used. All vectors are used for the sample with four sub-populations. Figure \ref{fig:MonteCarlo_cross_experts} provides a visual reference for the sub-populations that are formed as a result of the sampling approach and the distribution of the dependent variable conditioned on the appropriate group membership.

\bigskip 

Table \ref{tbl:monte_carlo_voung_results} collects the results of the experiment. When the data generating process has two sub-populations, all estimated models are equivalent according to Voung's LR tests. This is due to the fact that models with more than two experts continue to split the two individual heterogeneous populations into smaller but homogenous groups with similar regression parameters estimated estimated for each one. The results become more nuanced as attention is turned to the sample with three sub-populations. There are several items worth pointing out. First, the 2-expert ME model is the least preferred amongst all the candidate models. The value of it's log-likelihood is significantly lower than the competing models with three and four experts. Second, the HME models are preferred over the ME models when keeping for the number of experts constant. This provides more support to the idea that the recursive gating structure of the HME model is more efficient at discovering the latent structure of a given dataset than the ME models which must partition the data all at once using a single multinomial split. Third, the 3-expert and 4-expert HME models are considered equivalent according to the Voung LR test. The same determination holds for the 3 and 4-expert ME models as well. Again, the idea here is similar to what is observed for the sample with two sub-populations: the extra expert has a fitted parameter vector that aligns closely with another expert in the model and applied to observations in the same sub-population. Results for the sample with four sub-populations is complicated and speaks to the difficulty of the (H)ME architecture to pull apart sub-groups that share considerable overlap both in the domain of the joint-distribution of $(X, Z)$ and their conditional distributions. Although there are four sub-groups in the simulated data, the 3-expert HME model is clearly preferred according to the LR test and the log-likelihood values. From Figure \ref{fig:MonteCarlo_cross_experts}, the fourth expert (in light blue) has it's entire mass contained in an interval of $y$ ([1.8, 4]) that also is highly associated with values of $y$ for the first sub-population (in black). The noise in the data has become too great for the model to distinguish without additional covariates.

\begin{figure}[t!]
  \includegraphics[width=\textwidth]{MonteCarlo_cross_experts_data.jpeg}
  \caption{Samples drawn from the process described in Subsection \ref{subsec:ExperimentTwo}. Each column is a sample with a different number of sub-populations. The top figures plot the observations and their group membership. The bottom figures are density estimations of the dependent variable $y$, broken out by sub-population.}
  \label{fig:MonteCarlo_cross_experts}
\end{figure}

\begin{table}[t!] \centering
  \caption{Monte Carlo Simulation and the Voung LR Test}
  \begin{threeparttable}
    \begin{tabular}[l]{r r r r r r r r}
              &         &       &       &       &       &     &          \\
      \multicolumn{8}{c}{Two Population Groups}               \\
      \hline
      Arch    &         & ME    & HME   & ME    & HME   & ME  & Log-Lik  \\
              & Experts & 2     & 3     & 3     & 4     & 4   &          \\
              \hline
      ME      & 2       & .     & 0     & 0     & 0     & 0   & 1.383    \\
      HME     & 3       & 0     & .     & 0     & 0     & 0   & 1.382    \\
      ME      & 3       & 0     & 0     & .     & 0     & 0   & 1.387    \\
      HME     & 4       & 0     & 0     & 0     & .     & 0   & 1.389    \\
      ME      & 4       & 0     & 0     & 0     & 0     & .   & 1.385    \\
      \hline
              &         &       &       &       &       &     &         \\
      \multicolumn{8}{c}{Three Population Groups}            \\
      \hline
      Arch    &         & ME    & HME   & ME    & HME   & ME  & Log-Lik \\
              & Experts & 2     & 3     & 3     & 4     & 4   &         \\
              \hline
      ME      & 2       & .     & -1    & -1    & -1    & -1  & 1.074   \\
      HME     & 3       & 1     & .     & 1     & 0     & 1   & 1.269   \\
      ME      & 3       & 1     & -1    & .     & -1    & 0   & 1.202   \\
      HME     & 4       & 1     & 0     & 1     & .     & 1   & 1.267   \\
      ME      & 4       & 1     & -1    & 0     & -1    & .   & 1.207   \\
      \hline
              &         &       &       &       &       &     &         \\
      \multicolumn{8}{c}{Four Population Groups}             \\
      \hline
      Arch    &         & ME    & HME   & ME    & HME   & ME  & Log-Lik \\
              & Experts & 2     & 3     & 3     & 4     & 4   &         \\
      \hline
      ME      & 2       & .     & -1    & -1    & -1    & -1  & 0.633   \\
      HME     & 3       & 1     & .     & 1     &  1    & 1   & 1.133   \\
      ME      & 3       & 1     & -1    & .     & -1    & -1  & 0.798   \\
      HME     & 4       & 1     & -1    & 1     & .     & 1   & 1.008   \\
      ME      & 4       & 1     & -1    & 1     & -1    & .   & 0.890   \\
      \hline
    \end{tabular}
    \begin{tablenotes}
      \item{\footnotesize This table summarizes the comparison of varying (H)ME models estimated on a three different datasets with a known structure. A value of 1 indicates the model in the row is favored, -1 indicates the model in the column is favored, and 0 indicates equivalency of the models.}
    \end{tablenotes} \label{tbl:monte_carlo_voung_results}
  \end{threeparttable}
\end{table}

\clearpage

\section{Conclusion} \label{sec:Conclusion}

In this article, a novel mixture model is explored that borrows equally from the economic and deep learning fields. A flexible gating network is used to learn the latent structure of a sample and then apply local regressions to that latent structure. Robust inference and closed form expressions for marginal effects were developed and demonstrated on two different datasets. Methods for model selection are also explored and demonstrated on these two dataset as well. Several simulation studies were  carried out to better understand the behavior of the competing gating architectures and how they perform on samples with varying degrees of heterogeneity.



\clearpage

\printbibliography

\end{document}
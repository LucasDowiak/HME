\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathbbol}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{pdflscape}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{graphicx}

\usepackage[
  backend=biber,
  style=alphabetic,
  url=true,
  doi=true,
  style=authoryear
]{biblatex}
\addbibresource{hme_references.bib}

\newcommand{\mean}[1]{\bar{#1}}
\newcommand{\gateprod}[2]{\pi_{#1 \longleftrightarrow #2}}
\newcommand{\sumgateprod}[3]{\pi_{#1 \overset{#3}{\longleftrightarrow} #2}}
\newcommand{\shortsum}[1]{\sum \nolimits_{#1}}
\newcommand{\expmixwt}[0]{\mathbb{\Pi}}
\newcommand{\h}[2]{h^{#1}_{#2}}

\setlength{\parindent}{10pt}

\bibliography{hme_references}

\graphicspath{{/Users/lucasdowiak/hme/article/images/}}

 
\title{Econometric Applications of Hierarchical Mixture of Experts}
\author{Lucas C. Dowiak}

\begin{document}
 
\maketitle{}


Department of Economics, City University of New York\smallskip, Graduate
Center,

New York, NY, 10016, \textit{Email: ldowiak@gc.cuny.edu}

\qquad

\begin{abstract}
In this article, a novel mixture model is studied.
Named the hierarchical mixture of experts (HME) in the machine learning literature,
the mixture model utilizes a set of covariates and a tree-based architecture to
efficiently allocate each observation to the appropriate local regression.
The nature of the conditional weighting scheme provides the researcher a natural
interpretation of how the local (and latent) sub-populations are formed. The model is
demonstrated by estimating a Mincer earning function using census data. Marginal effects,
robust standard errors, a tree-growing algorithm, and a modest extension are also
discussed.

\end{abstract}

\vspace{1pt}

\begin{quotation}
\textbf{Keywords}: Hierarchical mixture of experts, expectation maximization

\textbf{JEL Classification}: 
\end{quotation}

\vspace{1pt}

\section{Introduction}

The concepts of mixture models and mixture distributions are old hat
in the economics business. \cite{Hamilton1989} and \cite{GoldfeldQuant1973}
are a few of the pioneering works for time series and cross sectional
regression, respectively. We are also knee deep in the age of machine
learning, and it's reigning champion, the artificial neural network, has
been successfuly adapted and studied in the context of applied econometrics.
This article adds to the small body of literature that employs a specific
neural network architecture to model the weights of a mixture model. In doing
so, we leverage the highly flexible nature of a neural network but maintain
interpretability and the means to quantify marginal effects.
The model under investigation is called the Hierarchical Mixture
of Experts (HME), a class of mixture models whose defining feature is 
its conditional weighting scheme. The model's origin story traces back to \cite{JJNH1991}.
The authors use a single multinomial classifier to assign, in a probablistic
sense, input patterns to local \textit{experts}. These \textit{experts} are
almost always some flavor of regression or classification model.
The multinomial structure that assigns inputs to experts is
refered to as the \textit{gating network}. \cite{JJNH1991} employ this
mixture of experts (ME) framework to model vowel discrimination in a speech
recognization context. Extending this approach, \cite{JordanJacobs1993}
propose a gating network that allows for additional layers of multinomial
partitioning of the input space, occurring in a recursive manner. The result
of this extension is a gating network that takes on a tree-like structure,
stemming from an initial multinomial split and filtering down through additional
multinomial partitions of the input space. The hierarchical nature of this
gating network is what gives this class of model it's name: the Hierarchical
Mixture of Experts (HME). HME models nest ME models as special case. Pushing a 
little further, one additional case is studied as well. As the depth of an HME
grows, so too must the number of experts. If we have a symmetric HME network,
this growth is geometric with respect to the network's depth. With this in mind,
we propose a model where each expert is not unique, but a member of a fixed
set of experts that are allowed to repeat at different terminal nodes of the
network. We refer to this additional model as a Hierarchical Mixture of Repeated
Experts (HMRE). Figure (\ref{fig:network_comparison}) provides an example
of each of these models studied in this article.

\bigskip

This article investigates the adoption of ME, HME, and HMRE models to an applied
econometric framework, with particular attention focused on interpretation of
the gating network and robust inference of parameter estimates. The outline for the
rest of this manuscript is as follows: the remainder of this section
fills out the literature review and section \ref{sec:Model} describes
the model in formal detail. Section \ref{sec:Estimation} discuss approaches
for estimation while section \ref{sec:Inference} concerns itself with robust
inference of the estimated parameters. Section \ref{sec:MarginalEffects}
provides detail on marginal effects of the gating network. Section
\ref{sec:NetworkGrowth} develops a proceedure to grow the gating network
in an objective manner. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{network_types.jpeg}
  \caption{Networks \textbf{A} - \textbf{D} depict various network
  architectures that are discussed in this article. For all four networks,
  gating nodes are represented as blue circles and experts as orange
  rectangles. Network \textbf{A} illustrates the original Mixture of
  Experts (ME) architexture with a single multinomial split leading
  to a set of experts one layer down. Networks \textbf{B} and \textbf{C} both
  represent different flavors of a Hierarchical Mixture of Experts (HME). Network
  \textbf{B} is a symmetric network of depth 2 with successive binary splits.
  Network \textbf{C} is an asymmetric network of depth 3 with successive
  binary splits. Network \textbf{D} is an example of the Hierarchical
  Mixture of Repeated Experts (HMRE) architecture. Notice that multiple paths
  exist from the root node $0$ to each expert. Compare this to networks
  \textbf{A} - \textbf{C}, where there is only one unique path from the root
  node to each expert.}
  \label{fig:network_comparison}
\end{figure}


\subsection{Relevant Literature}

\cite{WaterhouseRobinson1995} puts forth a method to grow an HME from a 
single split from the root node. The authors are influenced by the growing
technique used for classification and regression trees (\cite{CART1984}) and
apply it to an HME structure. Once the gating structure to an HME tree has
been grown, the authors put forth an additional trimming algorithm as well.
\cite{FFW1997} consider (\cite{WaterhouseRobinson1995}) and alter their
growing algorithm with a mind to scaling the model to handle thousands of
experts.

\cite{JordanXuConverge1995} An extended discussion on the convergence of the
model used by \cite{JordanJacobs1993} ? (VERIFY). The authors also suggest
algorithmic improvements to help with estimation.

\cite{JiangTanner1999} discuss convergence rates of an HME model where experts
are from the exponential family with generalized linear mean functions.

\cite{JiangTanner2000} provide regularity conditions on the HME structure for
for a mixture of general linear models estimated by maximum likelihood to
produce consistent and asymptotically normal estimates of the mean response.
The conditions are validated for poisson, gamma, gaussian, and binomial experts.


\subsection{Don't leave behind time series analysis}

\cite{WMS1995} provides a detailed discussion about examining ME applied in
a time series context and provide valuable insights to avoid overfitting the
model to the data, a common problem in neural network applications.

\cite{HuertaJiangTanner2003} Extends \cite{WMS1995} to an HME.
Five and a half decades of monthly US Industrial Production
Index data. They allowed the series to choose between two models, one modeled
as a random walk and the other as trend stationary. In addition, they present a
Bayesian approach to estimation.

\cite{CarvalhoTanner2003} lay out the necessary regularity conditions to perform
hypthesis tests on stationarity MoE time series of generalized linear models (HoE-GLM)
using Wald tests. The dual cases of a well-specificed and a miss-specified model
are considered. The authors restrict their analysis to MoE-GLM models involving
lagged dependent and lagged external covariate variables only. Generazlization to
include lagged conditional mean values is left for another time.

\cite{CarvalhoTanner2005} is similar to \cite{CarvalhoTanner2003} but applied
in a purely auto-regressive context restricted to gaussian models. The authors
extend arguments in \cite{CarvalhoTanner2003} to non-stationary series and
provide simulated evidence that using the BIC is helpful in selecting the 
approprite number of experts to include.

\cite{CarvalhoTanner2006} refocus the discussion on MoE of time series
regressions restricted to exponential family distributions. Distilling
the available literature at the time, the authors cover the important
topics of estimation and asymptotic properties in the maximum likelihood
framework, selection of the number of experts, model validation and
fitting.

\cite{CarvalhoSkoulakis2010} Applies mixture-of-experts of a single time series.
Using stock returns, the authors structure the gating network using lagged
dependent variabels and an 'external' covariate capturing a measure of the
trade volume at that time. THIS NEEDS A LOT MORE...Mention Simulations...READ 
THIS ONE AGAIN




\subsection{Additional Articles to Include}


\cite{JeffriesPfeiffer2001} cross section

\cite{BleiKucukelbirMcAuliffe2006} A review of variational inference applied
to generalized linear models and basic examples.

\cite{UedaGhahramani2002}

\cite{BishopSvenson2003} find previous bayesian approaches to estimating an HME
lacking [\cite{HuertaJiangTanner2003}, \cite{UedaGhahramani2002}]. Using
variational inference, the authors provide a bayesian estimation approach
to the log marginal likelihood. With an eye to prediction, the author's advocate
that their approach makes the HME model easier to estimate without overfitting.
[Discuss how the authors approach model selection]


\cite{CarvalhoSkoulakis2005}

\bigskip


\section{Model} \label{sec:Model}

We start by presenting the HME as a standard mixture model.
For a given input and output pair $(X_{t}, Y_{t})$, each expert
provides a probablistic model relating input $X_{t}$ to output
$Y_{t}$:

\begin{equation} \label{eq:ConditionalDistribution}
  P^{m}_{t} \equiv P^{m}(Y_{t}|X_{t}, \boldsymbol{\beta}^{m}), \quad m = 1,2,...,M
\end{equation}

where $m$ is one of the $M$ component experts in the mixture. The experts
are combined with associated weights into a mixture distribution

\begin{equation} \label{eq:staticmixture}
  P(Y_{t} | X_{t}; \, \boldsymbol{\beta}) = \sum_{m=1}^{M} \expmixwt(m|t) P^{m}(Y_{t} | X_{t}; \boldsymbol{\beta}^{m})
\end{equation}

Here, $\expmixwt_{t}(m)$ is the probability that the input unit $t$ belongs
to expert $m$ and has the usual restrictions: $0 \leq \expmixwt(m|t) \leq 1$
for each $m$ and $\sum_{m} \expmixwt(m|t) = 1$. The gating network of the model
applies a particular functional form to model $\expmixwt(m|t)$, which includes a
second set of covariates $Z_{t}$ and parameter vector $\boldsymbol{\omega}$:

\begin{equation} \label{eq:mixture}
  P(Y_{t} | X_{t}, Z_{t}; \, \boldsymbol{\beta}, \boldsymbol{\omega}) = \sum_{m=1}^{M} \expmixwt(m | Z_{t}; \boldsymbol{\omega}) P^{m}(Y_{t} | X_{t}; \boldsymbol{\beta}^{m})
\end{equation}


\subsection{Gating Network and $\expmixwt(m | Z, \boldsymbol{\omega})$} \label{subsec:GatingNetwork}

The gating network model is structured as a collection of nodes in a tree
structure that branches out in successive layers. The location of these nodes will
be referred to by their address $a$. The root node resides at the apex of the tree
and has the address $0$. The root node then splits into $J$ different nodes,
one level down the tree. The addresses for these $J$ new nodes are 
$1|0, 2|0, ..., J|0$. This type of naming convention continues as the
rest of network is traversed. At its most general, each gating node can yield an
arbitrary number of splits. While a fully generalized gating network is
conceptually attractive, it presents practical challenges for implementation.
In this paper we address several architectures for the gating network, each
with its own set of structural restrictions on the shape of the network and
the number of splits each gating node can take. For arbitary node at address $a$,
we use a multinomial logistic regression to model the split in direction $i$ to be:

\begin{equation} \label{eq:softmax}
  g^{a,i}_{t} \equiv g^{a,i}_{t}(Z_{t}, \boldsymbol{\omega}^{a}) = \frac{\exp(Z_{t} \ \boldsymbol{\omega}^{a,i})}{\sum^{J}_{j=1} {\exp(Z_{t} \ \boldsymbol{\omega}^{a,j})}}
\end{equation}

The parameters in equation (\ref{eq:softmax}) are subject to the usual
identification restrictions. For the remainder of the article, we choose
to set $\boldsymbol{\omega}^{a,J} = \boldsymbol{0}$ for every gating node.
It is important to keep track of the product path an input vector travels from
one node to another. If the observation index is supressed, the product path
from one node (say the root node $0$) to another (say $m|\ldots|j|i$)
can be defined as

\begin{equation} \label{eq:gpath}
  \gateprod{g^{0}}{g^{k|\ldots|j|i|0}} =
    \begin{cases} 
       g^{0, \, i} \, g^{i|0, \, j} \ldots g^{\dots|j|i|0, \, k} & \textrm{if path is feasible} \\
       1 & \textrm{otherwise}
    \end{cases}
\end{equation}

If one of the nodes is an expert, then we can define the mixture weight
of expert $m$ for input pattern $i$ to be the product of the path taken
from the root node to expert $m$:

\begin{equation} \label{eq:gpath2}
  \expmixwt(m | Z, \boldsymbol{\omega}) = \gateprod{g^{0}}{P^{m}}
\end{equation}

For network architectures with multiple paths from the root node to
the same expert (see bottom right of figure (\ref{fig:network_comparison})),
we can index these multiples paths by $l$ so that:

\begin{equation} \label{eq:pathsums}
  \expmixwt(m | Z_{t}, \boldsymbol{\omega}) = \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l} 
\end{equation}


By collecting--and summing--all possible paths from the root node to each
expert, the conditional probability given in equation (\ref{eq:mixture}) can be
expanded and expressed as:

\begin{equation} \label{eq:contribution}
  \begin{split}
    P(Y_{t}|X_{t}, Z_{t}; \boldsymbol{\omega}, \boldsymbol{\beta}) =& \sum_{m} \expmixwt(m | Z_{t}, \boldsymbol{\omega}) P^{m}(Y_{t}|X_{t},\beta^{m}) \\ 
      =& \sum_{m} P^{m}(Y_{t}|X_{t}, \beta^{m}) \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l}
  \end{split}
\end{equation}

The product of these individual probabilities across the full sample size $T$ yields
the likelihood function.

\begin{equation} \label{eq:likelihood}
  \mathcal{L}(\boldsymbol{\theta}| Y, X, Z) = \prod_{t}\sum_{m}P^{m}(Y_{t}|X_{t}, \beta^{m}) \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l}
\end{equation}

And taking its log yields to log likelihood

\begin{equation} \label{eq:loglikelihood}
  \boldsymbol{l}(\boldsymbol{\theta}|Y, X, Z) = \sum_{t}\log\sum_{m}P^{m}(Y_{t}|X_{t},\beta^{m}) \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l}
\end{equation}

The functional form of the log likelihood (\ref{eq:loglikelihood}) does not
lend itself easily to direct optimization, but a well established
technique using expectation maximization (\cite{EM_DLR1977}) to estimate mixture
models is available. This was the primary insight of \cite{JordanJacobs1993}'s
original paper.


\section{The EM Set-Up} \label{sec:Estimation}

The EM approach to estimating an HME model starts by suggesting that if a
resarcher had perfect information, each input vector $X_{t}$ could be matched
to the expert $P^{m}$ that generated it with certainty. If a set of indicator
variables is introduced that captures this certainty, an \textit{augmented}
version of the likelihood in equation (\ref{eq:likelihood}) can be put forward.
Define the indicator set as:

\begin{equation} \label{eq:indicator}
  I_{t}(m) = \begin{cases} 
     1 & \textrm{if observation $t$ is generated by expert $m$} \\
     0 & \textrm{otherwise}
             \end{cases}
\end{equation}

We can then reformulate the likelihood equation

\begin{equation}  \label{eq:auglikelihood}
  \mathcal{L}_{c}(\boldsymbol{\theta}|Y, X, Z) = \prod_{t} \prod_{m} \left[ P^{m}(Y_{t}|X_{t}, \boldsymbol{\beta}^{m}) \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l} \right]^{I_{t}(m)}
\end{equation}

leading to the complete-data log-likelihood

\begin{equation}  \label{eq:augloglikelihood}
  \boldsymbol{l}_{c}(\boldsymbol{\theta}|Y, X, Z) = \sum_{t} \sum_{m} I_{t}(m) \left[\log P^{m}(Y_{t}|X_{t}, \boldsymbol{\beta}^{m}) + \log \shortsum{l} \sumgateprod{g^{0}}{P^{m}}{l} \right]
\end{equation}

As mentioned previously, summing over multiple paths $l$ in equation
(\ref{eq:augloglikelihood}) is only necessary in the HMRE case. For the ME
and HME cases, $l = 1$, simplifying the second log in (\ref{eq:augloglikelihood})
to $\log(\gateprod{g^{0}}{P^{m}})$. Going forward, we will focus our analysis on
the ME and HME specifications with work on the HMRE case arriving in subsequent
iterations of the article.


%In previous works, each terminal node of thevtree was usually its own unique
%expert. This assumption would simplify equation (\ref{eq:likelihood})
%by removing the summation of the gating paths to expert $m$ into a 
%single path,
%$\prod_{t}\prod_{m}[P^{m}(Y_{t}|X_{t}, Z_{t}, \beta^{m}) \pi_{n_{0} \longleftrightarrow P^{m}}]^{I_{t}(m)}$,
%simplifying its value and the value of it's gradient and hessian. This paper
%allows the analyst to specify the number of experts before optimization,
%adding a bit more control over the mixture model but at the cost of a more
%complicated complete likelihood function. In fact this paper will test whether
%this can lead to any improvements, specifically the concers regarding overfitting
%(see \cite{BishopSvenson2003} for more references on this) when an expert latches
%on to a single observation, producing a singularity in the likelihood function.


\subsection{E-Step}
The E-step of the algorithm performs an expectation over the complete
log-likelhood equation (\ref{eq:augloglikelihood}), where the expectation
includes the additional information contained in the expert regressions.
One of the results of this expectation is the creation of second set of
weights $h^{a}$ that parallel the weights from the gating network $g^{a}$
discussed in section (\ref{subsec:GatingNetwork}). For an HME model:

\begin{equation} \label{eq:Estep}
  \begin{split}
  Q(\boldsymbol{\theta}) = \mathbb{E} \left [ \boldsymbol{l}_{c}(\boldsymbol{\theta}|Y,X,Z) \right] & = \sum_{t}\sum_{m} \mathbb{E} \left[ I_{t}(m) \right] \left[ \log P^{m}(Y_{t}|X_{t}, \beta^{m}) + \log \gateprod{g^{0}_{t}}{P^{m}_{t}} \right] \\ 
   & = \sum_{t} \sum_{m} \gateprod{h^{0}_{t}}{P^{m}_{t}} \left[ \log P^{m}(Y_{t}|X_{t},\beta^{m}) + \log \gateprod{g^{0}_{t}}{P^{m}_{t}} \right]
 \end{split}
\end{equation}

Here $\gateprod{h^{0}}{h^{k,\dots|j|i|0}}$ is analagous to equation (\ref{eq:gpath})

\begin{equation} \label{eq:hpath}
  \gateprod{h^{0}}{h^{k|\ldots|j|i|0}} =
    \begin{cases} 
       h^{0, \, i} \ h^{i|0, \, j} \ldots h^{\dots|j|i|0, \, k} & \textrm{if path is feasible} \\
       1 & \textrm{otherwise}
    \end{cases}
\end{equation}

and the $h^{a, i}$ are arrived at using bayes' theoreom.

\begin{equation} \label{eq:posteriornode}
  \h{a,i}{t} = \frac{g^{a, i} \shortsum{k} P^{k}_{t} \gateprod{g^{i|a}_{t}}{P^{k}}}{\shortsum{j} g^{a, j} \shortsum{m} P^{m}_{t} \gateprod{g^{j|a}_{t}}{P^{m}}}
\end{equation}


So, now we have two different forms of weights, $g$'s and $h$'s. The
way the $g$'s are formed in equation (\ref{eq:softmax}), they are only
functions of the nodes in the gating network, separate from the
expert regressions and the information they contain. For this reason,
the authors in Jacob et. al. 1993 refer to $g$'s as \textit{priors}.
The $h$'s draw from both the gating network and the expert regressions and
are referred to as \textit{posterior} weights.


\subsection{M-Step}

Note that the paramters governing the experts and the gating network in
equation (\ref{eq:Estep}) are additively seperable. Taking each in turn, the
Jacobian of the gating portion of equation (\ref{eq:Estep}) is:

The expert regressions are weighted regressions 







\section{Inference} \label{sec:Inference}

The score vector:

\begin{equation} \label{eq:gateScore}
  \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a, i}) \equiv \frac{\partial Q}{\partial \boldsymbol{\omega}^{a,i}} = \gateprod{h^{0}_{t}}{h^{a}_{t}} \left[h^{a, i}_{t} - g^{a, i}_{t} \right] Z_{t}
\end{equation}

With

\begin{equation}
  \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a}) = \left[\boldsymbol{S}_{t}(\boldsymbol{\omega}^{a, 1}),
                                                      \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a, 2}),
                                                      \hdots, 
                                                      \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a, J - 1})\right]
\end{equation}

And

\begin{equation}
  \boldsymbol{S}(\boldsymbol{\omega}^{a}) = \sum_{t}^{T} \boldsymbol{S}_{t}(\boldsymbol{\omega}^{a})
\end{equation}

\bigskip




The hessian:
\newcommand{\bw}[1]{\boldsymbol{\omega}^{#1}}
\newcommand{\Ht}[1]{\mathbf{H}_{t}(#1)}
\newcommand{\HH}[1]{\boldsymbol{H}(#1)}
\newcommand{\HI}[1]{\boldsymbol{H}^{-1}(#1)}

\begin{equation} \label{eq:nodehessian}
  \Ht{\bw{a}} \equiv \frac{\partial^{2} Q}{\partial \boldsymbol{\omega}^{a,i} \partial \boldsymbol{\omega}^{a,j}} = \gateprod{h^{0}_{t}}{h^{a}_{t}} Z_{t} \boldsymbol{\Gamma}^{a}_{t}  Z_{t}^\top
\end{equation}

With

\begin{equation}
  \boldsymbol{\Gamma}^{a}_{t} = \begin{bmatrix}
  g_{t}^{a,1}(1-g_{t}^{a,1}) & -g_{t}^{a,1}g_{t}^{a,2}    & \dots  & -g_{t}^{a,1}g_{t}^{a,J-1}       \\
  -g_{t}^{a,1}g_{t}^{a,2}    & g_{t}^{a,2}(1-g_{t}^{a,2}) & \dots  & -g_{t}^{a,2}g_{t}^{a,J-1}       \\
  \vdots                     &  \vdots                    & \ddots & \vdots                 \\
  -g_{t}^{a,1}g_{t}^{a,J-1}  & -g_{t}^{a,2}g_{t}^{a,J-1}  & \dots  & -g_{t}^{a,J-1}(1-g_{t}^{a,J-1}) \\
    \end{bmatrix}
\end{equation}

And

\begin{equation}
  \HH{\boldsymbol{\omega}^{a}} = \sum_{t}^{T} \Ht{\boldsymbol{\omega}^{a}}
\end{equation}

Leading to the sandwich estimator:

\begin{equation} \label{eq:robustgatevarcov}
  \boldsymbol{V}(\boldsymbol{\omega}^{a}) = \HI{\boldsymbol{\omega}^{a}} \boldsymbol{S}(\boldsymbol{\omega}^{a}) \boldsymbol{S}(\boldsymbol{\omega}^{a})^\top \HI{\boldsymbol{\omega}^{a}}
\end{equation}
\section{HMRE}


Score function for HMRE

\begin{equation} \label{eq:gateJacobian}
  \begin{split}
    \frac{\partial Q}{\partial \omega^{a,i}_{p}} =& \sum_{t} \sum_{m=1}  \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \sum_{l} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} \\
  \end{split}
\end{equation}

Where we have the ratio of the posterior $h^{0,m}$ and prior 
$\sum_{l} \pi_{n^{a} \overset{l}{\longleftrightarrow} P^{m}}$ weights with resepect
to the root node and the gradient of the posterior weight with respect to the node
in questions $\frac{\partial g^{a,m}}{\partial \omega^{a,i}_{p}}$. Since
$\pi_{n^{a} \overset{l}{\longleftrightarrow} P^{m}}$ is a product of values, the
partial in equation (\ref{eq:gateJacobian}) is simply that same product chain but
absent of node $g^a$ times the partial of node $g^a$ with respect to
$\omega^{a,i}_{p}$.

\begin{equation} \label{eq:gatechainpartial}
  \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} = [\pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}} (-g^{a})] \frac{\partial g^{a, j(l)}}{\partial \omega^{a, i}}
\end{equation}



Hessian for HMRE
\begin{equation}
  \begin{split}
    \frac{\partial^{2} Q}{\partial \omega^{a,i}_{p} \partial \omega^{a',j}_{q}} =& \sum_{t} \sum_{m} \left[  \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} - \left( \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \right)^{2} \sum_{l} \sum_{l'} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}}  \frac{\partial \pi_{n^{0}_{t} \overset{l'}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a',j}_{q}} \right]
  \end{split}
\end{equation}


Posterior node for HMRE
\begin{equation} 
  h^{a,m}_{t} = \frac{P^{m}_{t} \shortsum{l} \sumgateprod{g^{a}_{t}}{P^{m}_{t}}{l}}{\sum_{k} P^{k} \shortsum{l} \sumgateprod{g^{a}_{t}}{P^{k}_{t}}{l}}
\end{equation}



\section{Marginal Effects} \label{sec:MarginalEffects}

To explore what kind of marginal effects each variable has on the mixture's 
output, we start with equation (\ref{eq:mixture}) but replace the
expert distributions $P^{m}_{t}$ with the functional form of the 
expert regression $f^{m}_{t}$ and use the relationship in equation
(\ref{eq:gpath2}). 

\begin{equation} \label{eq:mixture2}
  f_{t} = f(Y_{t} | X_{t}, Z_{t}; \, \boldsymbol{\beta}, \boldsymbol{\omega}) = \sum_{m=1}^{M} \gateprod{g^{0}_{t}}{f^{m}_{t}} f^{m}(Y_{t} | X_{t}, \boldsymbol{\beta}^{m})
\end{equation}

The functional form of the marginal effect depends if the variables are
exclusive to the gating network, $Z$, exclusive to the expert
regressions, $X$, or is present in both sets of covariates,
$W \subset X$ and $W \subset X$. For covariates
exclusive to the gating network, their marginal effects are:

\begin{equation} \label{eq:ME_gating}
  \frac{\partial f_{t}}{\partial Z} = \sum_{m=1}^{M} \frac{\partial \gateprod{g^{0}_{t}}{f^{m}_{t}}}{{\partial Z}}f^{m}_{t}
\end{equation}

For covariates exclusive to the expert regressions, their marginal effects
are:

\begin{equation} \label{eq:ME_expert}
  \frac{\partial f_{t}}{\partial X} = \sum_{m=1}^{M} \gateprod{g^{0}_{t}}{f^{m}_{t}} \frac{\partial f^{m}_{t}}{{\partial X}}
\end{equation}

And covariates in both:

\begin{equation} \label{eq:ME_both}
  \frac{\partial f_{t}}{\partial W} = \sum_{m=1}^{M} \left\{ \frac{\partial \gateprod{g^{0}_{t}}{f^{m}_{t}}}{{\partial W}}f^{m}_{t} + \gateprod{g^{0}_{t}}{f^{m}_{t}} \frac{\partial f^{m}_{t}}{{\partial W}} \right\}
\end{equation}

Just as for logistic and multinomial regressions, the
marginal effect of the entire gating network has a closed form solution.
Independently, looking at the network's marginal effects provides a sense
of what gating variables play a decisive role in directing input patterns
to the appropriate local expert regressions. Starting with equation
(\ref{eq:gpath}), we take the partial with respect to gating matrix $Z_{t}$.

\begin{equation}
  \delta^{m} \equiv \frac{\partial \gateprod{g^{0}}{f^{m}}}{\partial Z} = \frac{\partial g^{0, i} g^{i|0, j} \cdots g^{k|\cdots|j|i|0, m}}{\partial Z}
\end{equation}

Applying the product rule gives us:

\begin{equation} \label{eq:hme_gate_marginal_effect}
  \begin{split}
    \delta^{m} &= \frac{\partial g^{0, i}}{\partial Z} g^{i|0, j} \cdots g^{k|\cdots|j|i|0, m} \\
                                     &+ g^{0, i} \frac{\partial g^{i|0, j}}{\partial Z} \cdots g^{k|\cdots|j|i|0, m} \\
                                     &+ \dots                                                                        \\
                                     &+ g^{0, i} g^{i|0, j} \cdots \frac{\partial g^{k|\cdots|j|i|0, m}}{\partial Z} \\
  \end{split}
\end{equation}

and since:

\begin{equation} \label{eq:gate_marginal_effect}
  \frac{\partial g^{a, i}}{\partial Z} = g^{a, i} \left( \boldsymbol{\omega}^{a, i} - \sum_{j} g^{a, j} \boldsymbol{\omega}^{a, j} \right) = g^{a, i} \left( \boldsymbol{\omega}^{a, i} - \mean{\boldsymbol{\omega}}^{a} \right)
\end{equation}

we can substitute equation (\ref{eq:gate_marginal_effect}) into
(\ref{eq:hme_gate_marginal_effect}) to arrive at:

\begin{equation} \label{eq:marginal_effects}
  \delta^{m} = \gateprod{g^{0}}{P^{m}} \left(\boldsymbol{\omega}^{0, i} + \boldsymbol{\omega}^{i|0, j} + \cdots + \boldsymbol{\omega}^{k|\cdots|j|i|0, m} - \left( \mean{\boldsymbol{\omega}}^{0} + \mean{\boldsymbol{\omega}}^{i|0} + \cdots + \mean{\boldsymbol{\omega}}^{k|\cdots|j|i|0} \right) \right)
\end{equation}

\bigskip 

Standard errors for these marginal effects can be estimated using the
delta method. \textit{(Edit: These are actually only for the gating network, not
the entire mixture output. Also, this is only appropriate for MoE, not an
HME)}

\newcommand{\deltavar}{%
  \begin{bmatrix}
    \boldsymbol{V}(\boldsymbol{\omega}^{0}) & \boldsymbol{0}                            & \boldsymbol{0}                            & \dots  \\ 
    \boldsymbol{0}                          & \boldsymbol{V}(\boldsymbol{\omega}^{1|0}) & \boldsymbol{0}                            & \dots  \\
    \boldsymbol{0}                          & \boldsymbol{0}                            & \boldsymbol{V}(\boldsymbol{\omega}^{2|0}) &        \\
    \vdots                                  & \vdots                                    &                                           & \ddots \\
  \end{bmatrix}
}

\begin{equation}
    \boldsymbol{V}(\boldsymbol{\delta^{m}}) = \left(\frac{\partial \boldsymbol{\delta}^{m}}{\partial \boldsymbol{\omega}} \right) \deltavar  \left(\frac{\partial \boldsymbol{\delta}^{m}}{\partial \boldsymbol{\omega}} \right)^{\top}
\end{equation}

where $\boldsymbol{V}(\boldsymbol{\omega}^{a})$ is the sandwich estimator
in equation (\ref{eq:robustgatevarcov}) and:

\begin{equation}
  \frac{\partial \boldsymbol{\delta}^{m}}{\partial \boldsymbol{\omega}^{a,p}} = [1 - g^{a, p}] (\delta^{m} Z + \gateprod{g^{0}}{P^{m}}) - g^{a,p} \gateprod{g^{0}}{P^{m}} [\omega^{a,p} - \mean{\omega}^{a}] Z
\end{equation}

if $\boldsymbol{\omega}^{a,p}$ appears in the path of $\gateprod{g^{0}}{P^{m}}$, or if not,
then:

\begin{equation}
  \frac{\partial \boldsymbol{\delta}^{m}}{\partial \boldsymbol{\omega}^{a,p}} = -g^{a, p} (\delta^{m} Z + \gateprod{g^{0}}{P^{m}}) - g^{a,l} \gateprod{g^{0}}{P^{m}} [\omega^{a,p} - \mean{\omega}^{a}] Z
\end{equation}


\section{Growing and Pruning the Gating Network} \label{sec:NetworkGrowth}


\section{A simple example} \label{sec:SimpleExample}

In order to provide a concrete example of the concepts discussed previously,
the ME and HME models are demonstrated on a small and well known dataset
collected by Edgar Anderson (\cite{Anderson1936}) and popularized in the
statistics literature by Ronald Fisher (\cite{Fisher1936}). Anderson collected
50 measurements each from three different species of iris flowers; the width and
length of both the petal and the sepal. Figure \ref{fig:Iris_dataset} provides a
basic view of the species specific clustering inherent in the data.

\begin{figure}[!ht]
  \includegraphics[width=\textwidth]{basic_iris_plot.jpeg}
  \caption{Three different Iris species are presented: Setosa
  (blue circles), Versicolor (orange triangles), Virginia (green crosses).
  Sepal width is on the vertical axis and petal width on the horizontal
  axis.}
  \label{fig:Iris_dataset}
\end{figure}

The work below uses the ME and HME architectures to estimate a flower's sepal
width using only it's petal width as a predictor. The petal width will be used
as the sole covariate in the local linear expert regressions ($X$) as well as
in the gating network ($Z$). 

\begin{equation} \label{eq:HME_iris}
    sepal.width_{i} = \beta_{0} + \beta_{1} * petal.width_{i} + \varepsilon_{i} \enspace | \enspace \omega_{0} + \omega_{1} * petal.width_{i}
\end{equation}

The goal is to have the gating network of the 
models identify the inherent species-specific clustering without explicit
knowledge of each observation's species classification, and then fit an
appropriate local regression to the self-identified clusters. As a benchmark,
an OLS model is run where a flower's petal width is interacted with it's species,
resulting in a species-specific estimation of sepal width.

\begin{equation} \label{eq:OLS_iris}
    sepal.width_{is} = \beta_{0,s} + \beta_{1, s} * petal.width_{is} + \varepsilon_{is}
\end{equation}

Two sets of regressions are run. Since the Versicolor and Virginica species
can be viewed as one larger cluster, a two-expert ME model is run
and compared to a benchmark OLS where Versicolor and Virginica are labelled
as the same species. A second set of regressions are run with three mixture
experts for ME and HME, as well as their corresponding three species OLS
regression. Results are collected in table \ref{tbl:Iris}. Coefficients for
local experts in the two expert ME regression match closely with the OLS
benchmark. When moving to the three expert models, there is now a choice
on what kind of gating scheme to employ. We can go deep (HME) by adding 
a gating network with depth two, or we can go wide (ME) by keeping the
depth of the gating network at one. When comparing the coefficients of the
local regressions, the HME architecture clearly outperforms the ME
architecture. The ME model fails to identify the three seperate
species that are known to exist.  

\begin{landscape}
\begin{table} 
	\caption{Iris Dataset - OLS vs ME vs HME}
	\begin{threeparttable}
		\begin{tabular}[l]{l l l l l l l l l l l l}
  \cmidrule{1-12}

  & \multicolumn{4}{c}{2 Expert Mixture} & & \multicolumn{6}{c}{3 Expert Mixture} \\
  \cmidrule(r){2-5}  \cmidrule(r){7-12}
  & \multicolumn{2}{c}{OLS}  & \multicolumn{2}{c}{ME} & & \multicolumn{2}{c}{OLS}  & \multicolumn{2}{c}{HME}  & \multicolumn{2}{c}{ME} \\
  \cmidrule(r){2-3}  \cmidrule(r){4-5}  \cmidrule(r){7-8}  \cmidrule(r){9-10}  \cmidrule(r){11-12}
  & Coef. & SE & Coef. & SE && Coef. & SE & Coef. & SE & Coef. & SE     \\
  \cmidrule{1-12}
  
  Setosa \\
  \cmidrule(r){1-1}
  Const.             & 3.22 & 0.11** & 3.22 & 0.13** && 3.22  & 0.11** & 3.22 & 0.13** & 3.45 & 0.13**       \\
  Petal.Width        & 0.84 & 0.42*  & 0.95 & 0.49** && 0.84  & 0.41*  & 0.94 & 0.49   & 0.39 & 0.46         \\[0.3cm]
  
  Virginica \\
  \cmidrule(r){1-1}
  Const.             & --   & --     & --   & --     && 1.70  & 0.32** & 1.96 & 0.12** & 3.02 & 0.05**       \\
  Petal.Width        & --   & --     & --   & --     && 0.63  & 0.16** & 0.50 & 0.06** & 0.21 & 0.31         \\[0.3cm]
  
  Versicolor \\
  \cmidrule(r){1-1}
  Const.             & --   & --     & --   & --     && 1.37  & 0.29** & 1.15 & 0.12** & 2.13 & 0.09**       \\
  Petal.Width        & --   & --     & --   & --     && 1.05  & 0.22** & 1.29 & 0.09** & 0.44 & 0.06**       \\[0.3cm]

  Virg + Versi \\
  \cmidrule(r){1-1}
  Const.             & 2.13 & 0.13** & 2.13 & 0.09** && --    & --     & --   & --     & --   & --       \\
  Petal.Width        & 0.44 & 0.07** & 0.44 & 0.06** && --    & --     & --   & --     & --   & --       \\[0.3cm]

  AME \\
  \cmidrule(r){1-1}
  Petal.Width        & 0.57 & --     & 0.49 & --     && 0.84  & --     & 0.57 & --     & 0.62 & --         \\[0.3cm]


  Log-Like           & -35.5 & --    & -31.9 & --    && -29.3  & --    & -21.8& --     & -27.8 & -- \\
  N                  & 150   & --    & 150   & --    && 150    & --    & 150  & --     & 150   & -- \\

	\hline
		\end{tabular}
		\begin{tablenotes}
			\item{\footnotesize ** $p < 0.01$, * $p < 0.05$}
			\item{\footnotesize OLS regressions are modeled using equation (\ref{eq:OLS_iris})}
			\item{\footnotesize ME regressions are modeled using equation (\ref{eq:HME_iris}) and architecture $\boldsymbol{A}$ from figure \ref{fig:network_comparison}}
			\item{\footnotesize HME regressions are modeled using equation (\ref{eq:HME_iris}) and architecture $\boldsymbol{C}$ from figure \ref{fig:network_comparison}}
		\end{tablenotes} \label{tbl:Iris}
	\end{threeparttable}
\end{table}
\end{landscape}




\section{A Mincer Wage Equation}

For a more economically relevant example, we turn our attention to a common
topic in labor economics: the income return on an additional year of
education. At times called the "Mincer wage equation", our version of it
will be:

\begin{equation} \label{eq:mincer_equ}
  \log (wage) = \beta_{0} + \beta_{1} * \textnormal{Age} + \beta_{2} * \textnormal{Age}^{2} + \beta_{3} * \textnormal{YrsEdu} + \boldsymbol{\beta_{4}}\boldsymbol{X} + \varepsilon
\end{equation}

with $\boldsymbol{X}$ containing a set of individual-specific 
variables as well as a set of occupation-specific attributes. 
Our data will come from two sources. First, from the 2000 Census,
we devise a measure of hourly wages in addition to age,
years of education (YrsEdu), job occupations codes, 
and a set of demographic identifiers indicating the race of the
indivduals contained in census sample. Second, we pull from the
Occupation Information Network (ONet) a set of knowledge and skill-based
attributes describing what qualities are necessary to perform each
job suitably. ONet is a federally sponsered source of occupational
information. It details, on a per occupation basis, "the knowledge,
skills, and abilities required as well as how the work is
performed in terms of tasks, work activities, and other descriptors"
(\cite{ONET}). 

To provide an interesting space to classify occupational codes from
the census data, we've chosen the following four attributes to
include in the covariates $\boldsymbol{X}$ of our wage equations:

\begin{enumerate}
  \item Social Perceptiveness \footnote{Skills - Social Skills - Social Perceptiveness}
  \item Design \footnote{Work Activities - Mental Processes - Analyzing Data or Information}
  \item Data Analytics \footnote{Knowledge - Design}
  \item Creative Thinking \footnote{Work Activities - Mental Processes - Thinking Creatively}
\end{enumerate}

For these selected attributes, ONet grades their relevance on a
100 point scale. Each attribute contains two of scales, an "importance"
scale and a "level" scale. The importance scale denotes how critical the
attribute is to the occupation while the level indicates how much the
skill is required or needed to perform the occupation. To unify the two
measures, we follow the paper Prof Wijverberg gave me and take a
cobb-douglass style average with a 2/3's weight for importance and a 1/3
weight for the level scale.

To link the occupational codes in the census data to the soc codes used
by ONet, we use the cross walk provided by Sarah Porter. This matching
is not one-to-one. When more than one soc code points to a single census
code, we take the average of the soc codes.

A summary descriptions of the covariates are provided in table
\ref{tbl:census_cov_summary}. For comparative purposes, we estimate equation
(\ref{eq:mincer_equ}) for both the HME and ME architectures, testing whether
there is any inherent advantage to allowing the gating network to go deep
(HME), as opposed to staying shallow with a depth of one. For both
architectures, we start with two experts, and then continue to add them until
a minimal information criterion is found. In particular, we choose to minimize
the Bayesian information criterion (BIC). Compared to the Akaike information
criterion (AIC), the BIC places a higher penalty on the number of parameters
in a model. We find this feature preferable since the growth in the number
of paramaters can be considerable in both models as the number of experts
increase.

\begin{table} \centering
  \caption{Summary Statistics}
  \begin{threeparttable}
    \begin{tabular}[l]{l r r r r}
  \cmidrule{1-5}

              & 25\%   & Mean & 50\% & 75\%   \\
  \cmidrule{1-5}
  Wage (hr)   & 9.20  & 15.82 & 13.32  & 19.44\\
  Yrs Edu     & 12.00 & 13.78 & 14.00  & 16.00\\
  Age         & 30.00 & 39.15 & 39.00  & 48.00 \\
  Age16       & 14.00 & 23.15 & 23.00  & 32.00 \\
  Female      & --    & 40.47 & --     & --   \\
  Af Amer     & --    &  8.62 & --     & --   \\
  Indian      & --    &  1.05 & --     & --   \\
  White       & --    & 77.00 & --     & --   \\
  Hispanic    & --    & 10.00 & --     & --   \\
  Asian       & --    &  3.36 & --     & --   \\
  Creative    & 46.65 & 53.30 & 53.82  & 58.94 \\
  Design      & 15.00 & 30.58 & 26.33  & 38.97 \\
  Analytic    & 44.01 & 52.63 & 52.68  & 62.18 \\
  Perseptive  & 41.15 & 50.49 & 46.03  & 59.84 \\

  \hline
    \end{tabular}
    \begin{tablenotes}
      \item{\footnotesize N = 68,642}
    \end{tablenotes} \label{tbl:census_cov_summary}
  \end{threeparttable}
\end{table}

A natural question to consider as a researcher is where to put the variable(s)
of interest while performing an HME estimation. \cite{JiangTanner2000}
provide their proof of model consistency for HME of GLM's for the case where
all covariates appear in the gating network as well as the experts. We will
call this the \textit{full} specifications:

\begin{equation}
  log(wage) = Age + YrsEdu + Sex +  Race + Occ \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

We will compare this \textit{full} specification to two others. A
\textit{mid} specification where the local experts contain age and
years of education while removing demographic indicators:

\begin{equation}
  log(wage) = Age + YrsEdu \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

And finally a \textit{minimal} specification where our core variable
of interest, years of education, appears solely in the gating network.

\begin{equation}
  log(wage) = Age \; | \; Age + YrsEdu + Sex +  Race + Occ
\end{equation}

Results for these regressions are collected in table \ref{tbl:HME_spec_BIC}.
There are a few points worth noting. First, for this dataset, increasing the
number of experts yields increasingly better log-likelihood and BIC values.
Second, there is a clear advantage to using the HME architecture. In table
\ref{tbl:HME_spec_BIC}, compare the rows with different gating types (col 1)
but with the same number of experts (col 3).The HME approach outperforms the
ME across the board. Third, for both ME and HME types, models with more
covariates in the local expert regressions lead to higher log-likeihood
values. 

\begin{figure}[!ht]
  \includegraphics[width=\textwidth]{Job_characteristic_density.jpeg}
  \caption{Density estimates of ONet job characteristics broken down by
  sex. The job characteristics have been mean centered and scaled to
  have unit variance.}
  \label{fig:JobChar_vs_sex}
\end{figure}

\newcommand\ti{\textit}
\newcommand{\iu}[1]{\underline{\textit{#1}}}
\newcommand{\ibu}[1]{\textbf{\underline{\textit{#1}}}}
\newcommand\T{\rule{0pt}{2.5ex}}       % Top strut
\newcommand\B{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\begin{table} \centering
  \caption{Model Information}
  \begin{threeparttable}
    \begin{tabular}[r]{l l l l r r r r}
  \cmidrule{1-8}
        &            &        &         &  \multicolumn{4}{c}{Model Comparison} \\ 
   \cmidrule(l){5-8}
Model   & Stop Criterion  &  Depth & Experts & Log-Lik & AIC    & BIC    & MSE   \\ 
  \cmidrule{1-8}


Full    &  Log-Lik   &   1    &    2    & -0.540  & 1.081  & 1.086  & 0.182 \\
        &            &   2    &    3    & -0.524  & 1.050  & 1.059  & 0.181 \\
        &            &   2    &    4    & -0.515  & 1.034  & 1.047  & 0.181 \\
        &            &   3    &    5    & -0.505  & 1.015  & 1.031  & 0.178 \\
        &            &   3    &    6    & \ibu{-0.503}  & \ibu{1.011}  & \ibu{1.031}  & \ibu{0.178} \\ \T 
        &  MSE       &   1    &    2    & -0.540  & 1.081  & 1.086  & 0.182 \\
        &            &   2    &    3    & -0.524  & 1.050  & 1.059  & 0.181 \\
        &            &   2    &    4    & -0.515  & 1.034  & 1.047  & 0.181 \\
        &            &   3    &    5    & -0.505  & 1.015  & 1.031  & 0.178 \\
        &            &   3    &    6    & \ti{-0.503}  & \ti{1.011}  & \ti{1.031}  & \ibu{0.178} \\ \T \\
        \cmidrule(l){1-8}
Mid     &  Log-Lik   &   1    &    2    & -0.559  & 1.119  & 1.122  & 0.185 \\
        &            &   2    &    3    & -0.540  & 1.082  & 1.088  & 0.184 \\
        &            &   2    &    4    & -0.527  & 1.056  & 1.064  & 0.183 \\
        &            &   3    &    5    & \iu{-0.518}  & \iu{1.039}  & \iu{1.049}  & 0.182 \\
        &            &   3    &    6    & -0.519  & 1.041  & 1.053  & \iu{0.181} \\ \T
        &  MSE       &   1    &    2    & -0.559  & 1.119  & 1.122  & 0.185 \\
        &            &   2    &    3    & -0.540  & 1.082  & 1.088  & 0.184 \\
        &            &   2    &    4    & -0.529  & 1.060  & 1.067  & 0.183 \\
        &            &   3    &    5    & \ti{-0.518}  & \ti{1.039}  & \ti{1.050}  & 0.182 \\
        &            &   3    &    6    & -0.519  & 1.041  & 1.053  & \iu{0.181} \\ \T \\
        \cmidrule(l){1-8}
 Min    &  Log-Lik   &   1    &    2    & -0.595  & 1.192  & 1.195  & 0.192 \\
        &            &   1    &    3    & -0.552  & 1.106  & 1.111  & 0.183 \\
        &            &   2    &    3    & -0.580  & 1.162  & 1.168  & 0.190 \\
        &            &   2    &    4    & -0.546  & 1.093  & 1.101  & 0.182 \\
        &            &   3    &    5    & -0.523  & 1.049  & 1.058  & 0.182 \\
        &            &   3    &    6    & \iu{-0.521}  & \iu{1.045}  & \iu{1.057}  & \iu{0.181} \\ \T
        &  MSE       &   1    &    2    & -0.596  & 1.192  & 1.195  & 0.192 \\
        &            &   2    &    3    & -0.580  & 1.162  & 1.168  & 0.190 \\
        &            &   2    &    4    & -0.546  & 1.094  & 1.101  & 0.182 \\
        &            &   3    &    5    & \ti{-0.523}  & \ti{1.049}  & \ti{1.059}  & 0.182 \\
        &            &   3    &    6    & -0.531  & 1.064  & 1.076  & \iu{0.181} \\

 
\hline
\end{tabular}
    \begin{tablenotes}
      \item{\footnotesize Log-Likelihood and BIC are divided by the sample size (54,829)}
    \end{tablenotes} \label{tbl:HME_spec_BIC}
  \end{threeparttable}
\end{table}


The coefficients on years of education (or variable of interests), which
are summarized in table \ref{tbl:YrsEdu_coef}, are pretty
stable across model specifications. With the exception of the two-expert
\textit{minimal} model, coefficient values range from 0.083 to 0.092. 


\begin{table} \centering
  \caption{Returns to Years of Education}
  \begin{threeparttable}
    \begin{tabular}[l]{r r r r r}
  \cmidrule{1-5}
  \multicolumn{2}{c}{OLS: 0.096}   & \multicolumn{3}{c}{Coefficient} \\ 
  \cmidrule(r){3-5}
  Depth & Experts & Min   & Mid   & Full       \\
  \cmidrule{1-5}

  1     & 2       & 0.051 & 0.083 & 0.072      \\
  1     & 3       & --    & --    & --         \\
  1     & 4       & --    & --    & --         \\
  1     & 5       & --    & --    & --         \\
  1     & 6       & --    & --    & --         \\
  2     & 3       & 0.048 & 0.082 & 0.072      \\
  2     & 4       & 0.063 & 0.078 & 0.073      \\
  3     & 5       & 0.068 & 0.076 & 0.070      \\
  3     & 6       & 0.067 & 0.075 & 0.070      \\

  \hline
    \end{tabular}
    \begin{tablenotes}
      \item{\footnotesize OLS coef: 0.96}
      \item{\footnotesize Log-Likelihood is divided by the sample size (54,829)}
    \end{tablenotes} \label{tbl:YrsEdu_coef}
  \end{threeparttable}
\end{table}

We also compare the marginal effects across specifications. According to table
\ref{tbl:HME_spec_BIC}, the five-expert HME model performs the best for each
of the three specifications. Table \ref{tbl:model_marg_effects} summarizes
the marginal effects across specifications and benchmarks them to an standard
OLS regresssion.

The marginal effects for the constant term in the HME are not comparable
to the OLS model. These marginal effects include the constant terms in the
gating network, invalidating them as an intercept term. The return to
education does appear to be materially lower for the HME models as
compared to the standard OLS while the quadratic wage curve is similar
in the \textit{full} specification but somewhat different in the
\textit{mid} and \textit{minimal} specifications. When it comes to
the coefficients for race, a bit of variations starts to appear
across specifications. Interestingly, the \textit{mid} specification
seems to deviate the most from the OLS benchmark while the
\textit{minimal} specification maintains comparable marginal effects.
There doesn't seem to be an obvious explanation for this discrepency. 

\begin{table} \centering
  \caption{Full Marginal Effects}
  \begin{threeparttable}
    \begin{tabular}[l]{l l r r r}
      \cmidrule{1-5}

                  & OLS      & Full    & Mid     & Min    \\
      \cmidrule{1-5}

      Const.      & 0.819    &  1.036  & 1.035   & 1.057  \\
                  & 0.013**  &  --     & --      & --     \\[0.3cm]
      
      Yrs Edu     & 0.096    & 0.091   & 0.091   & 0.088  \\
                  & 0.008**  & --      & --      & --     \\[0.3cm]

      Age         & 0.044    & 0.044   & 0.041   & 0.041  \\
                  & 0.001**  & --      & --      & --     \\[0.3cm]

      Age Sq      & -0.0006  & -0.0007 & -0.0006 & -0.0006\\
                  & 0.0000** & --      & --      & --     \\[0.3cm]

      Afr Amer    & -0.175   & -0.166  & -0.120  & -0.155 \\
                  & 0.007**  & --      & --      & --     \\[0.3cm]

      Indian      & -0.118   & -0.091  & -0.098  & -0.116 \\
                  & 0.019**  & --      & --      & --     \\[0.3cm]

      Hispanic    & -0.165   & -0.174  & -0.117  & -0.132 \\
                  & 0.006**  & --      & --      & --     \\[0.3cm]

      Asian       & -0.070   & -0.049  & -0.043  & -0.070 \\
                  & 0.010**  & --      & --      & --     \\[0.3cm]


      \hline
    \end{tabular}
    \begin{tablenotes}
      \item[1]{\footnotesize Each non-OLS model is a five-expert HME.}
    \end{tablenotes} \label{tbl:model_marg_effects}
  \end{threeparttable}
\end{table}



\begin{table} \centering
  \caption{Log Wage Expert Equations - Full Specification}
  \begin{threeparttable}
    \begin{tabular}[l]{l l l l l l l}
      \cmidrule{1-7}

                  & OLS      & 1       & 2 &3 &4 & 5  \\
      \cmidrule{1-7}

      Share       & --       & 0.625   & 0.193   & 0.117   & 0.046   & 0.018    \\[0.3cm]

      Const.      & 0.819    & 1.048   & 1.493   & 1.687   & 1.341   & 1.643    \\
                  & 0.013**  & 0.014** & 0.010** & 0.014** & 0.084** & 0.013**  \\[0.3cm]
      
      Yrs Edu     & 0.096    & 0.105   & 0.032   & 0.034   & 0.040   & 0.0126   \\
                  & 0.008**  & 0.001** & 0.001** & 0.001** & 0.005** & 0.0011** \\[0.3cm]

      Age         & 0.044    & 0.024   & 0.0616  & 0.003   & 0.044   & 0.011    \\
                  & 0.001**  & 0.001** & 0.001** & 0.0006**& 0.004** & 0.0008** \\[0.3cm]

      Age Sq      & -0.0006  & -0.0003 & -0.001  & 0.00008 & -0.0006 & -0.0005  \\
                  & 0.0000** &0.00001**&0.00001**&0.00001**&0.00008**& 0.0000** \\[0.3cm]

      Afr Amer    & -0.175   & -0.209  & -0.135  & -0.036  & -0.022  & -0.078   \\
                  & 0.007**  & 0.007** & 0.006** & 0.006** & 0.045   &  0.007** \\[0.3cm]

      Indian      & -0.118   & -0.131  & -0.181  & -0.122  & 0.654  & 0.432     \\
                  & 0.019**  & 0.018** & 0.016** & 0.014** & 0.138**& 0.014**   \\[0.3cm]

      Hispanic    & -0.165   & -0.108  & 1.590   & -0.032  & -1.037 & -0.041    \\
                  & 0.006**  & 0.006** & 0.041** & 0.004** & 0.072**& 0.005**   \\[0.3cm]

      Asian       & -0.070   & -0.051  & 0.059  & -0.068   & -0.468 & 0.145     \\
                  & 0.010**  & 0.010** & 0.008**& 0.009**  & 0.076**& 0.013**   \\[0.3cm]
      
      log Var    & --       & -1.824   & -2.512 & -2.776   & -0.409 & -4.586    \\
                 & --       & 0.008**  & 0.015**& 0.022**  & 0.005**& 0.095**   \\[0.3cm]


      \hline
    \end{tabular}
    \begin{tablenotes}
      \item[1]{\footnotesize Five-expert \textit{full} HME.}
    \end{tablenotes} \label{tbl:model_local_experts}
  \end{threeparttable}
\end{table}




\begin{table} \centering
  \caption{Log Wage Expert Equations - Mid specification}
  \begin{threeparttable}
    \begin{tabular}[l]{l l l l l l l}
      \cmidrule{1-7}

                  & OLS & 1       & 2       &3 &4 & 5  \\
      \cmidrule{1-7}

      Share       & --  & 0.632   & 0.168   & 0.126   & 0.056   & 0.017     \\[0.3cm]

      Const.      &     & 1.021   & 1.291   & 1.676   & 1.449   & 1.684    \\
                  &     & 0.013** & 0.013** & 0.011** & 0.073** & 0.011**   \\[0.3cm]
      
      Yrs Edu     &     & 0.102   & 0.059   & 0.014   & 0.040   & 0.012    \\
                  &     & 0.001** & 0.001** & 0.001** & 0.005** & 0.001**  \\[0.3cm]

      Age         &     & 0.029   & 0.010   & 0.072   & 0.036   & -0.032    \\
                  &     & 0.001** & 0.001** & 0.001** & 0.004** & 0.001**  \\[0.3cm]

      Age Sq      &     & -0.0004 & -0.0002 & -0.001  & -0.0005 & 0.005   \\
                  &     & 0.0001**& 0.0000**& 0.0000**& 0.0001**& 0.000**  \\[0.3cm]
      
      log Var     & --   & -1.886 & -2.493  & -2.772  & -0.435  & -4.571     \\
                  & --   & 0.008**& 0.017** & 0.021** & 0.005** & 0.085**    \\[0.3cm]


      \hline
    \end{tabular}
    \begin{tablenotes}
      \item[1]{\footnotesize Five-expert \textit{mid} HME.}
    \end{tablenotes} \label{tbl:model_local_expert_regressions}
  \end{threeparttable}
\end{table}




\section{Miscellaneous}

\subsection{Odd Facts}

Variables that appear in the gating network but not in the expert regressions
are sometimes called Concomitant Variables (see R FlixMix package).

\subsection{Wald Test is Invalid}
S+plus GLM section on problems with binomial GLMs
  -- Hauck and Donner (1977) JASA. Quoting S+plus: If there are
  some $\hat{\beta}_{i}$ that are large, the curvature of the log-likelihood at
  $mathbf{\hat{\beta}}$ can be much less than near $\beta_{i}=0$, and so the Walk
  approximation understimates the change in log-likelihood on setting $\beta_{i}=0$.
  This happens in such a way that as $|\hat{\beta}_{i}| \rightarrow \infty$. Thus
  highly significant coefficients according to the likelihood ratio test may have
  non-significant t ratios ............. There is one fairly common circumstance
  in which both convergence problems and the Hauek-Donner phenomenon can occur.
  This is when the fitted probs are extremely close to zero or one.



\section{Diagnostics}
pg 7 of \cite{WMS1995} suggested observing the distribution of the terminal
$g_{i}$. If only one expert is responsible for each observations, $g_{i}$ will
be close to one for a single expert and near zero for all other experts.
Can we formalize this comparison in to a specific test?

density forecasts evaluations.

Standard likelihood-ratio test is not valid (\cite{CarvalhoTanner2006}) with
AIC/BIC/VOUNG test being prefered.



\printbibliography

\end{document}
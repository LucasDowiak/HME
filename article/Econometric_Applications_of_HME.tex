\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathbbol}
%\usepackage{threeparttable}
\usepackage{geometry}
\usepackage{pdflscape}
%\usepackage{algorithmic}
%\usepackage{algorithm}
\usepackage{graphicx}

\usepackage[
  backend=biber,
  style=alphabetic,
  citestyle=chicago-authordate
]{biblatex}


\newcommand{\mean}[1]{\bar{#1}}
\newcommand{\gateprod}[2]{\pi_{#1 \longleftrightarrow #2}}
\newcommand{\sumgateprod}[3]{\pi_{#1 \overset{#3}{\longleftrightarrow} #2}}
\newcommand{\shortsum}[1]{\sum \nolimits_{#1}}
\newcommand{\expmixwt}[0]{\mathbb{\Pi}}


\setlength{\parindent}{10pt}

\bibliography{hme_references}

\graphicspath{{/Users/lucasdowiak/hme/article/images/}}

 
\title{Econometric Applications of Hierarchical Mixture of Experts}
\author{Lucas C. Dowiak}

\begin{document}
 
\maketitle{}


Department of Economics, City University of New York\smallskip, Graduate
Center,

New York, NY, 10016, \textit{Email: ldowiak@gc.cuny.edu}

\qquad

\begin{quotation}
\textbf{Abstract:}
\end{quotation}

\vspace{1pt}

\begin{quotation}
\textbf{Keywords}: Hierarchical mixture of experts, expectation maximization

\textbf{JEL Classification}: 
\end{quotation}

\vspace{1pt}

\section{Introduction}

The concepts of mixture models and mixture distributions are old hat
in the economics business. \cite{Hamilton1989} and \cite{GoldfeldQuant1973}
are a few of the pioneering works for time series and cross sectional
regression, respectively. We are also knee deep in the age of machine
learning, and it's reigning champion, the artificial neural network, has
been successfuly adapted and studied in the context of applied econometrics.
This article adds to the small body of literature that employs a specific
neural network architecture to model the weights of a mixture model. In doing
so, we leverage the highly flexible nature of a neural network but maintain
interpretability and the means to quantify marginal effects.
The model under investigation is called the Hierarchical Mixture
of Experts (HME), a class of mixture models whose defining feature is 
its conditional weighting scheme. The model's origin story traces back to \cite{JJNH1991}.
The authors use a single multinomial classifier to assign, in a probablistic
sense, input patterns to local \textit{experts}. These \textit{experts} are
almost always some flavor of regression or classification model.
The multinomial structure that assigns inputs to experts is
refered to as the \textit{gating network}. \cite{JJNH1991} employ this
mixture of experts (ME) framework to model vowel discrimination in a speech
recognization context. Extending this approach, \cite{JordanJacobs1993}
propose a gating network that allows for additional layers of multinomial
partitioning of the input space, occurring in a recursive manner. The result
of this extension is a gating network that takes on a tree-like structure,
stemming from an initial multinomial split and filtering down through additional
multinomial partitions of the input space. The hierarchical nature of this
gating network is what gives this class of model it's name: the Hierarchical
Mixture of Experts (HME). HME models nest ME models as special case.

\bigskip

This article investigates the adoption of ME and HME models to an applied
econometric framework, with particular attention focused on interpretation of
the gating network and robust inference of parameter estimates. The outline for the
rest of this manuscript is as follows: the remainder of this section
fills out the literature review for the HME, section \ref{sec:Model} describes
the model in formal detail. Section \ref{sec:Estimation} discuss approaches
for estimation while section \ref{sec:Inference} concerns itself with robust
inference of the estimated parameters. Section \ref{sec:MarginalEffects}
provides detail on marginal effects of the gating network. Section
\ref{sec:NetworkGrowth} develops a proceedure to grow the gating network
in an objective manner. 


\subsection{Relevant Literature}

\cite{WaterhouseRobinson1995} puts forth a method to grow an HME from a 
single split from the root node. The authors are influenced by the growing
technique used for classification and regression trees (\cite{CART1984}) and
apply it to an HME structure. Once the gating structure to an HME tree has
been grown, the authors put forth an additional trimming algorithm as well.
\cite{FFW1997} consider (\cite{WaterhouseRobinson1995}) and alter their
growing algorithm with a mind to scaling the model to handle thousands of
experts.

\cite{JordanXuConverge1995} An extended discussion on the convergence of the
model used by \cite{JordanJacobs1993} ? (VERIFY). The authors also suggest
algorithmic improvements to help with estimation.

\cite{JiangTanner1999} discuss convergence rates of an HME model where experts
are from the exponential family with generalized linear mean functions.

\cite{JiangTanner2000} provide regularity conditions on the HME structure for
for a mixture of general linear models estimated by maximum likelihood to
produce consistent and asymptotically normal estimates of the mean response.
The conditions are validated for poisson, gamma, gaussian, and binomial experts.


\subsection{Don't leave behind time series analysis}

\cite{WMS1995} provides a detailed discussion about examining ME applied in
a time series context and provide valuable insights to avoid overfitting the
model to the data, a common problem in neural network applications.

\cite{HuertaJiangTanner2003} Extends \cite{WMS1995} to an HME.
Five and a half decades of monthly US Industrial Production
Index data. They allowed the series to choose between two models, one modeled
as a random walk and the other as trend stationary. In addition, they present a
Bayesian approach to estimation.

\cite{CarvalhoTanner2003} lay out the necessary regularity conditions to perform
hypthesis tests on stationarity MoE time series of generalized linear models (HoE-GLM)
using Wald tests. The dual cases of a well-specificed and a miss-specified model
are considered. The authors restrict their analysis to MoE-GLM models involving
lagged dependent and lagged external covariate variables only. Generazlization to
include lagged conditional mean values is left for another time.

\cite{CarvalhoTanner2005} is similar to \cite{CarvalhoTanner2003} but applied
in a purely auto-regressive context restricted to gaussian models. The authors
extend arguments in \cite{CarvalhoTanner2003} to non-stationary series and
provide simulated evidence that using the BIC is helpful in selecting the 
approprite number of experts to include.

\cite{CarvalhoTanner2006} refocus the discussion on MoE of time series
regressions restricted to exponential family distributions. Distilling
the available literature at the time, the authors cover the important
topics of estimation and asymptotic properties in the maximum likelihood
framework, selection of the number of experts, model validation and
fitting.

\cite{CarvalhoSkoulakis2010} Applies mixture-of-experts of a single time series.
Using stock returns, the authors structure the gating network using lagged
dependent variabels and an 'external' covariate capturing a measure of the
trade volume at that time. THIS NEEDS A LOT MORE...Mention Simulations...READ 
THIS ONE AGAIN




\subsection{Additional Articles to Include}


\cite{JeffriesPfeiffer2001} cross section

\cite{BleiKucukelbirMcAuliffe2006} A review of variational inference applied
to generalized linear models and basic examples.

\cite{UedaGhahramani2002}

\cite{BishopSvenson2003} find previous bayesian approaches to estimating an HME
lacking [\cite{HuertaJiangTanner2003}, \cite{UedaGhahramani2002}]. Using
variational inference, the authors provide a bayesian estimation approach
to the log marginal likelihood. With an eye to prediction, the author's advocate
that their approach makes the HME model easier to estimate without overfitting.
[Discuss how the authors approach model selection]


\cite{CarvalhoSkoulakis2005}

\bigskip


Figure (\ref{fig:HMEcomparison}) depicts the differece between an HME as
described in the current literature and what is proposed in this paper.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{Org_vs_Ext.jpeg}
  \caption{Two Hierarchical Mixture of Expert models with symmetric
  binary splits and a depth of two. Gating nodes are depicted as circles
  and experts are rectangles. The root node $N^{0}$ lies at the apex and
  splits towards two additional gating nodes ($N^{1|0}$, $N^{2|0}$) one layer
  down, each providing an  additional layer of soft-partitioning through a
  binary partition of the input space. On the left is the original expert
  design, where each path flows into a unique expert. The modification
  proposed in this paper is given on the right, with the existance of multiple
  paths leading from the root node to a reduced number of experts.}
  \label{fig:HMEcomparison}
\end{figure}





\section{Model} \label{sec:Model}

We start by presenting the HME model as a standard mixture model.
For a given input and output pair $(X_{t}, Y_{t})$, each expert
provides a probablistic model relating input $X_{t}$ to output
$Y_{t}$:

\begin{equation} \label{eq:ConditionalDistribution}
  P^{m}_{t} \equiv P^{m}(Y_{t}|X_{t}, \boldsymbol{\beta}^{m}), \quad m = 1,2,...,M
\end{equation}

where $m$ is one of the $M$ component experts in the mixture. The experts
are combined with associated weights into a mixture distribution

\begin{equation} \label{eq:staticmixture}
  P(Y_{t} | X_{t}; \, \boldsymbol{\beta}) = \sum_{m=1}^{M} \expmixwt(m|t) P^{m}(Y_{t} | X_{t}, \boldsymbol{\beta}^{m})
\end{equation}

Here, $\expmixwt_{t}(m)$ is the probability that the input unit $t$ belongs
to expert $m$ and has the usual restrictions: $0 \leq \expmixwt(m|t) \leq 1$
for each $m$ and $\sum_{m} \expmixwt(m|t) = 1$. The gating network of the model
applies a particular functional form to model $\expmixwt(m|t)$, which includes a
second set of covariates $Z_{t}$ and parameter vector $\boldsymbol{\omega}$:

\begin{equation} \label{eq:mixture}
  P(Y_{t} | X_{t}, Z_{t}; \, \boldsymbol{\beta}, \boldsymbol{\omega}) = \sum_{m=1}^{M} \expmixwt(m | Z_{t}; \boldsymbol{\omega}) P^{m}(Y_{t} | X_{t}, \boldsymbol{\beta}^{m})
\end{equation}


\subsection{Gating Network and $\expmixwt(m | Z_{t}, \boldsymbol{\omega})$}

The gating network model is structured as a collection of nodes in a tree
structure that branches out in successive layers. The location of these nodes will
be referred to by their address $a$. The root node resides at the apex of the tree
and has the address $0$. The root node then splits into $J$ different nodes,
one level down the tree. The addresses for these $J$ new nodes are 
$1|0, 2|0, ..., J|0$. This type of naming convention continues as the
rest of network is traversed. At its most general, each gating node can yield an
arbitrary number of splits. While a fully generalized gating network is
conceptually attractive, it presents practical challenges for implementation.
In this paper we address several architectures for the gating network, each
with its own set of structural restrictions on the shape of the newtork and
the number of splits each gating node can take. Figure () illustrates the 
variety of architectures that are covered this article. For arbitary
node at address $a$, we use a multinomial logistic regression to model the
split in direction $i$ to be:

\begin{equation} \label{eq:softmax}
  g^{a,i}_{t} \equiv g^{a,i}_{t}(Z_{t}, \boldsymbol{\omega}^{a}) = \frac{\exp(Z_{t} \ \boldsymbol{\omega}^{a,i})}{\sum^{J}_{j=1} {\exp(Z_{t} \ \boldsymbol{\omega}^{a,j})}}
\end{equation}

The parameters in equation (\ref{eq:softmax}) are subject to the usual
identification restrictions. For the remainder of the article, we choose
to set $\boldsymbol{\omega}^{a,J} = \boldsymbol{0}$ for every gating node.
It is important to keep track of the product path an input vector travels from
one node to another. If the observation index is supressed, the product path
from one node (say the root node $0$) to another (say $m|\ldots|j|i$)
can be defined as

\begin{equation} \label{eq:gpath}
  \gateprod{g^{0}}{g^{\ldots|j|i|0, \, m}} =
    \begin{cases} 
       g^{0, \, i} \ g^{i|0, \, j} \ldots g^{\dots|j|i|0, \, m} & \textrm{if path is feasible} \\
       1 & \textrm{otherwise}
    \end{cases}
\end{equation}

If one of the nodes is an expert, then we can define the mixture weight
of expert $m$ for input pattern to be:

\begin{equation} \label{eq:pathsum}
  \expmixwt(m | Z_{t}, \boldsymbol{\omega}) = \gateprod{n^{0}}{P^{m}}
\end{equation}

For network architectures with multiple paths from the root node to the same expert
(see bottom right of figure ()), we can index these multiples by $l$ so that:

\begin{equation} \label{eq:pathsums}
  \expmixwt(m | Z_{t}, \boldsymbol{\omega}) = \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l} 
\end{equation}



By collecting--and summing--all possible paths from the root node to each
expert, the probability given in equation (\ref{eq:mixture}) can be expanded
and expressed as a set of conditional probabilities.

\begin{equation} \label{eq:contribution}
  \begin{split}
    P(Y_{t}|X_{t}, Z_{t}; \boldsymbol{\omega}, \boldsymbol{\beta}) =& \sum_{m} \expmixwt(m | Z_{t}, \boldsymbol{\omega}) P^{m}(Y_{t}|X_{t},\beta^{m}) \\ 
      =& \sum_{m} P^{m}(Y_{t}|X_{t}, \beta^{m}) \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l}
  \end{split}
\end{equation}

The product of these individual probabilities across the full sample size $T$ yields
the likelihood function.

\begin{equation} \label{eq:likelihood}
  \mathcal{L}(\boldsymbol{\theta}| Y, X, Z) = \prod_{t}\sum_{m}P^{m}(Y_{t}|X_{t}, \beta^{m}) \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l}
\end{equation}

And taking its log yields to log likelihood

\begin{equation} \label{eq:loglikelihood}
  \boldsymbol{l}(\boldsymbol{\theta}|Y, X, Z) = \sum_{t}\log\sum_{m}P^{m}(Y_{t}|X_{t},\beta^{m}) \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l}
\end{equation}

The functional form of the log likelihood (\ref{eq:loglikelihood}) does not
lend itself easily to direct optimization, but a well established
technique using expectation maximization (\cite{EM_DLR1977}) to estimate mixture
models is available. This was the primary insight of \cite{JordanJacobs1993}'s
original paper.





\section{The EM Set-Up} \label{sec:Estimation}

The EM approach to estimating an HME model starts by suggesting that if a
resarcher had perfect information, each input vector $X_{t}$ could be matched
to the expert $P_{m}$ that generated it with certainty. If a set of indicator
variables is introduced that captures this certainty, an \textit{augmented}
version of the likelihood in equation (\ref{eq:likelihood}) can be put forward.
Define the indicator set as:

\begin{equation} \label{eq:indicator}
  I_{t}(m) = \begin{cases} 
     1 & \textrm{if input vector $X_{t}$ is generated by expert $m$} \\
     0 & \textrm{otherwise}
             \end{cases}
\end{equation}

We can then reformulate the likelihood equation

\begin{equation}  \label{eq:likelihood}
  \mathcal{L}_{c}(\boldsymbol{\theta}|Y, X, Z) = \prod_{t} \prod_{m} \left[ P^{m}(Y_{t}|X_{t},\beta^{m}) \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l} \right]^{I_{t}(m)}
\end{equation}

leading to the complete-data log-likelihood

\begin{equation}  \label{eq:loglikelihood}
  \boldsymbol{l}_{c}(\boldsymbol{\theta}|Y, X, Z) = \sum_{t} \sum_{m} I_{t}(m) \left[\log P^{m}(Y_{t}|X_{t},\beta^{m}) + \log \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l} \right]
\end{equation}

In previous works, each terminal node of the tree was usually its own unique
expert. This assumption would simplify equation (\ref{eq:likelihood})
by removing the summation of the gating paths to expert $m$ into a 
single path,
$\prod_{t}\prod_{m}[P^{m}(Y_{t}|X_{t}, Z_{t}, \beta^{m}) \pi_{n_{0} \longleftrightarrow P^{m}}]^{I_{t}(m)}$,
simplifying its value and the value of it's gradient and hessian. This paper
allows the analyst to specify the number of experts before optimization,
adding a bit more control over the mixture model but at the cost of a more
complicated complete likelihood function. In fact this paper will test whether
this can lead to any improvements, specifically the concers regarding overfitting
(see \cite{BishopSvenson2003} for more references on this) when an expert latches
on to a single observation, producing a singularity in the likelihood function.


\subsection{E-Step}
The E-step of the algorithm performs an expectation of over the log-likelhood
equation (\ref{eq:loglikelihood}).

\begin{equation} \label{eq:Estep}
  \begin{split}
  Q(\boldsymbol{\theta}) = \mathbb{E} \left [ \boldsymbol{l}_{c}(\boldsymbol{\theta}|Y,X,Z) \right] & = \sum_{t}\sum_{m} \mathbb{E} \left[ I_{t}(m) \right] \left[ \log P^{m}(Y_{t}|X_{t}, \beta^{m}) + \log \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l} \right] \\ 
   & = \sum_{t} \sum_{m} h^{0,m}_{t} \left[ \log P^{m}(Y_{t}|X_{t},\beta^{m}) + \log \shortsum{l} \sumgateprod{n^{0}}{P^{m}}{l} \right]
 \end{split}
\end{equation}

Here $h^{0,m}_{t} \equiv \mathbb{E}\left[I_{t}(m)\right] = \mathbb{P}[I_{t}(m) = 1| y_{t}, x_{t}, z_{t}; \boldsymbol{\theta}]$
is what we define as the posterior probability of oberserved value $y_{t}$ being
produced by expert $m$, given our tree structure starts at the root node $n^{0}$.
The posterior weight for any gating node $n^{a}$ can be calculated by

\begin{equation} \label{eq:posteriornode}
  h^{a,m}_{t} = \frac{P^{m}_{t} \shortsum{l} \sumgateprod{n^{a}_{t}}{P^{m}_{t}}{l}}{\sum_{k} P^{k} \shortsum{l} \sumgateprod{n^{a}_{t}}{P^{k}_{t}}{l}}
\end{equation}

\begin{equation} \label{eq:posteriornode2}
  h^{a, i}_{t} = \frac{ g^{a, i}_{t} \shortsum{l} \sumgateprod{n^{i|a, \, j}_{t}}{P^{m}_{t}}{l}}{\sum_{k} P^{k} \shortsum{l} \sumgateprod{n^{a}_{t}}{P^{k}_{t}}{l}}
\end{equation}

If each expert is unique, there is only a single path from the root node to each
expert. In equation (\ref{eq:Estep}), this implies that $l = 1$ and the researcher
can avoid taking the log of a summation, greatly simplifying the value of the
gradient and hessian. On the other hand, if the same expert appears at the end of
separate paths along the gating network, at some point (depending on the network's
structure) we still need to take the log of a summation. If the researcher wants
to specify the number of expert regimes beforehand, she can do so but at the
expense of a more complicated complete log-likelihood equation.

It's important to note a relationship between the posterior weights for consecutive
nodes in a feasible path.

\begin{equation} \label{eq:posteriorrelation}
  \mathbb{E}\left[I_{t}(1)\right] = h^{0,1}_{t} h^{1|0,1}_{t} h^{1|1|0,1}_{t}
\end{equation}

The posterior specification for equation (\ref{eq:posteriornode}) is for the
repeated expert HME only. A different notation is needed for the original HME
design and it's adaptively grown counterpart.


\subsection{M-Step}

Note that the paramters governing the experts and the gating network in
equation (\ref{eq:Estep}) are additively seperable. Taking each in turn, the
Jacobian of the gating portion of equation (\ref{eq:Estep}) is:

\begin{equation} \label{eq:gateJacobian}
  \begin{split}
    \frac{\partial Q}{\partial \omega^{a,i}_{p}} =& \sum_{t} \sum_{m=1}  \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \sum_{l} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} \\
  \end{split}
\end{equation}

Where we have the ratio of the posterior $h^{0,m}$ and prior 
$\sum_{l} \pi_{n^{a} \overset{l}{\longleftrightarrow} P^{m}}$ weights with resepect
to the root node and the gradient of the posterior weight with respect to the node
in questions $\frac{\partial g^{a,m}}{\partial \omega^{a,i}_{p}}$. Since
$\pi_{n^{a} \overset{l}{\longleftrightarrow} P^{m}}$ is a product of values, the
partial in equation (\ref{eq:gateJacobian}) is simply that same product chain but
absent of node $g^a$ times the partial of node $g^a$ with respect to
$\omega^{a,i}_{p}$.

\begin{equation} \label{eq:gatechainpartial}
  \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} = [\pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}} (-g^{a})] \frac{\partial g^{a, j(l)}}{\partial \omega^{a, i}}
\end{equation}





\section{Inference} \label{sec:Inference}

\begin{equation} \label{eq:gateScore}
  \boldsymbol{S}_{i}(\omega^{a}) = h^{a}_{i} \left[h^{a, j}_{i} - g^{a, j}_{i} \right] Z_{i}
\end{equation}


\begin{equation} \label{eq:gateHessian}
  \begin{split}
    \frac{\partial^{2} Q}{\partial \omega^{a,i}_{p} \partial \omega^{a',j}_{q}} =& \sum_{t} \sum_{m} \left[  \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}} - \left( \frac{h^{0,m}_{t}}{\sum_{l} \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}} \right)^{2} \sum_{l} \sum_{l'} \frac{\partial \pi_{n^{0}_{t} \overset{l}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a,i}_{p}}  \frac{\partial \pi_{n^{0}_{t} \overset{l'}{\longleftrightarrow} P^{m}_{t}}}{\partial \omega^{a',j}_{q}} \right]
  \end{split}
\end{equation}

-- Put his directly after the log-likelihood equation in the EM section
-- Reference \cite{MengRubinSEM1991}
-- Show how weights enter into the score and hessian

\bigskip

This isn't correct

\begin{equation} \label{eq:gatinghessianQ}
  \mathbf{A}^{a} = (\mathbf{I}_{s-1} \otimes \mathbf{Z})^{\prime} \mathbf{G}^{a} (\mathbf{I_{s-1}} \otimes \mathbf{Z})
\end{equation}

\begin{equation} \label{eq:nodehessian}
  \mathbf{G}^{a} = h^{a} \begin{bmatrix}
     g^{1|a}(1-g^{1|a}) & -g^{1|a}g^{2|a}    & \dots  & g^{1|a}g^{s-1|a}       \\
     -g^{1|a}g^{2|a}    & g^{2|a}(1-g^{1|a}) & \dots  & g^{2|a}g^{s-1|a}       \\
     \vdots             &                    & \ddots & \vdots                 \\
     g^{1|a}g^{s-1|a}   &  \dots             &        & g^{s-1|a}(1-g^{s-1|a}) \\
  \end{bmatrix}
\end{equation}

For multinomial regressions, each 



\section{Marginal Effects} \label{sec:MarginalEffects}
There is an issue of how to tie the variables in the gating network to the regression
relationship described by experts. For logit/probit regression, there is the
standard margins at the means (MEM) and the average marginal effect (AME). We should
be able to extend this kind of analysis to the entire weighting network, chaining
marginal effects from the root node down and determind/quantify/summarize the effect
each gating factor has on directing which observation to which expert. These can
then be tied with the marginal regression effect. Starting with \ref{eq:gpath},
and summing over all the paths from the root node to expert $m$, we can re-arrange
the products of the collected path as follows:  

\begin{equation}
  \begin{split}
  w^{m} &= \sum_{l} \pi_{n_{0} \overset{l}{\longleftrightarrow} P^{m}} \\
        &= \sum_{i} \sum_{j} \cdots \sum_{k} g^{0, i} g^{i|0, j} \cdots g^{k|\cdots|j|i|0, m} \\
  \end{split}
\end{equation}

Applying the chain, we can calculate the marginal effect that variable $p$ has on
the weight assinged to each observation being assinged to expert $m$.

\begin{equation}
  \begin{split}
    \frac{\partial w^{m}}{\partial Z_{p}} &= \sum_{i} \frac{\partial g^{0, i}}{\partial Z_{p}} \sum_{j} g^{i|0, j} \cdots \sum_{k} g^{k|\cdots|j|i|0, m} \\
    &+ \sum_{i} g^{0, i} \sum_{j} \frac{\partial g^{i|0, j}}{\partial Z_{p}} \cdots \sum_{k} g^{k|\cdots|j|i|0, m} \\
    &+ \sum_{i} g^{0, i} \sum_{j} g^{i|0, j} \cdots \sum_{k} \frac{\partial g^{k|\cdots|j|i|0, m}}{\partial Z_{p}} \\
  \end{split}
\end{equation}

For the original hierarchical mixture of expert model, this simplifies to

\begin{equation} \label{eq:hme_gate_marginal_effect}
  \begin{split}
    \frac{\partial w^{m}}{\partial Z_{p}} &= \frac{\partial g^{0, i}}{\partial Z_{p}} g^{i|0, j} \cdots g^{k|\cdots|j|i|0, m} \\
    &+ g^{0, i} \frac{\partial g^{i|0, j}}{\partial Z_{p}} \cdots g^{k|\cdots|j|i|0, m} \\
    &+ g^{0, i} g^{i|0, j} \cdots \frac{\partial g^{k|\cdots|j|i|0, m}}{\partial Z_{p}} \\
  \end{split}
\end{equation}

and since:

\begin{equation} \label{eq:gate_marginal_effect}
  \frac{\partial g^{a, i}}{\partial Z_{p}} = g^{a, i}(\omega^{a, i} - \sum_{j} g^{a, j} \omega^{a, j}) = g^{a, i}(\omega^{a, i} - \mean{\omega}^{a})
\end{equation}

we can substitute equation (\ref{eq:gate_marginal_effect}) into
(\ref{eq:hme_gate_marginal_effect}) to arrive at:

\begin{equation}
  \frac{\partial w^{m}}{\partial Z_{p}} = \pi_{n_{0} {\longleftrightarrow} P^{m}} (\omega^{0, i} + \omega^{i|0, j} + \cdots + \omega^{k|\cdots|j|i|0, m} - (\mean{\omega}^{0} + \mean{\omega}^{i|0} + \cdots + \mean{\omega}^{k|\cdots|j|i|0}))
\end{equation}






\section{Growing and Pruning the Gating Network} \label{sec:NetworkGrowth}



\section{Miscellaneous}



\subsection{Model Specification}
When implementng a HME model, one of the major decision points is the exact
specification for each of the model's experts. If comparing a trend-stationary
model versus a random walk with drift (see \cite{HuertaJiangTanner2003}), the
correct specification for each expert requires the inclusion of the true number
of lagged dependent variables. While standard approaches such as the autocorrelation
and partial autocorrelation functions exist, the multi-state nature of model makes
this appreach difficult due to the uncertianty surrounding which state is governing
the series at any given time $t$. Unless the researcher has some previous knowledge
of the model's form, they are left with performing standard model selection for
each expert on the entire series or some subset of the series. To overcome this
issue, we follow simple procedure in this paper.

\begin{enumerate}
\item The HME model is run where each expert is an AR(1) model without a constant.
\item Use AIC/BIC to perform model selection on each expert, performed on the full series, where the each contribution to the likelihood has been weighted using the posterior weights from equation (\ref{eq:posterior}).
\item Perform a final HME run using the specifications obtained in step (2) for each expert.
\end{enumerate}

The validity of this approach is tested on two sets of simulated autoregressive data. this approach

% \begin{table}
% \caption{Model Selection}{c}
% \begin{threeparttable}
%  \begin{tabular}[l]{l c c c | c c c || l c c c | c c c}
%    \hline \hline
% \multicolumn{7}{c}{Simulated HME-AR$^{1}$} & \multicolumn{7}{c}{Simulated Univariate AR} \\
% & \multicolumn{3}{c}{AIC$^{2}$} & \multicolumn{3}{c}{BIC$^{3}$} & & \multicolumn{3}{c}{AIC$^{2}$} & \multicolumn{3}{c}{BIC$^{3}$} \\
% N & E$_{1}$ & E$_{2}$ & E$_{3}$ & E$_{1}$ & E$_{2}$ & E$_{3}$ & N & E$_{1}$ & E$_{2}$ & E$_{3}$ & E$_{1}$ & E$_{2}$ & E$_{3}$ \\
% \hline
% 100   & 0.29 & 0.09 & 0.30 & 0.66 & 0.23 & 0.61 & 33  & 0.67 & 0.32 & 0.62 & 0.9 & 0.29 & 0.76 \\
% 200   & 0.18 & 0.25 & 0.29 & 0.71 & 0.60 & 0.58 & 66  & 0.68 & 0.62 & 0.72 & 0.98 & 0.43 & 0.95 \\
% 300   & 0.25 & 0.39 & 0.27 & 0.69 & 0.73 & 0.63 & 100 & 0.65 & 0.77 & 0.77 & 0.95 & 0.72 & 0.92 \\
% 400   & 0.19 & 0.34 & 0.18 & 0.70 & 0.83 & 0.73 & 133 & 0.70 & 0.66 & 0.73 & 0.99 & 0.77 & 0.98 \\
% 500   & 0.25 & 0.33 & 0.27 & 0.76 & 0.88 & 0.73 & 166 & 0.66 & 0.69 & 0.69 & 0.98 & 0.92 & 0.97 \\
% 600   & 0.19 & 0.33 & 0.17 & 0.74 & 0.86 & 0.72 & 200 & 0.78 & 0.74 & 0.75 & 0.99 & 0.94 & 0.96 \\
% 700   & 0.19 & 0.25 & 0.12 & 0.73 & 0.86 & 0.69 & 233 & 0.73 & 0.73 & 0.69 & 0.97 & 0.96 & 0.97 \\
% 800   & 0.17 & 0.28 & 0.21 & 0.82 & 0.91 & 0.75 & 266 & 0.68 & 0.75 & 0.77 & 0.99 & 1.00 & 0.95 \\
% 900   & 0.22 & 0.30 & 0.24 & 0.77 & 0.83 & 0.72 & 300 & 0.74 & 0.77 & 0.76 & 0.97 & 0.99 & 0.98 \\
% 1000  & 0.14 & 0.34 & 0.29 & 0.80 & 0.90 & 0.70 & 333 & 0.69 & 0.76 & 0.77 & 0.98 & 0.98 & 0.99 \\
% \hline \hline
%   \end{tabular}
%   \begin{tablenotes}
%     \item Values in the tables are percentages, out of 100 simulations, of obtaining the correct lag order of the autoregressive process for each expert. The size of the simulated series is given by first and eigth columns (N). The experts in the HME share the same specification as their univariate counterparts.
%     \item[1] Expert AR(p) specification:
%     Expert 1: ($\phi_{1}=0.7$, $\sigma^{2}=0.03$);
%     Expert 2: ($\phi_{1}=-0.5$, $\phi_{2}=0.3$, $\sigma^{2}=0.05$);
%     Expert 3: ($\phi_{1}=0.05$, $\phi_{2}=0.1$, $\phi_{3}=0.6$, $\sigma^{2}=0.1$)
%     \item[2]{AIC: -2 $\times$ loglikelihood + 2 $\times$ p}
%     \item[3]{BIC: -2 $\times$ loglikelihood + ln(N) $\times$ p}
%   \end{tablenotes}
% \label{tbl:modelselection}
% \end{threeparttable}
% \end{table}

\begin{enumerate}
\item Future research: Agostinelli and Markatou \cite{AgostinelliMarkatou} investigated weighted likelihood ratio tests
\end{enumerate}

For a time series that moves back and forth between separate regiems, including
a time trend can be tricky. Even though the weighting scheme allows different
sets of observations to be more important than others when estimating model
parameters, once the model parameters have been estimated, they are prescriptive
for the entire series. Basically, if one model estimates a trend parameter, the
trend for that entire expert is now set, and if/when the time series exists that
particular regime and re-enters at a later date, it is re-entering at a level
described by the trend.

\subsection{Wald Test is Invalid}
S+plus GLM section on problems with binomial GLMs
  -- Hauck and Donner (1977) JASA \cite{HauckDonner} Quoting S+plus: If there are
  some $\hat{\beta}_{i}$ that are large, the curvature of the log-likelihood at
  $mathbf{\hat{\beta}}$ can be much less than near $\beta_{i}=0$, and so the Walk
  approximation understimates the change in log-likelihood on setting $\beta_{i}=0$.
  This happens in such a way that as $|\hat{\beta}_{i}| \rightarrow \infty$. Thus
  highly significant coefficients according to the likelihood ratio test may have
  non-significant t ratios ............. There is one fairly common circumstance
  in which both convergence problems and the Hauek-Donner phenomenon can occur.
  This is when the fitted probs are extremely close to zero or one.



\subsection{Simulations}
In order to carry out simulated experiments of the HME framework, this paper will 
follow the following methodology.

First, we decide on the model specification $M^{i}(\phi)$ for each expert (AR(p),
trend stationary, random walk, etc.), including the choose error distribution.
These models are then parameterzied with given values $\phi=(\phi^{1},\dots,\phi^{M})$
Parameters are choosen for each model

Second, we a assign prior probabilties to each expert that summarizes the
expectation of each expert being responsible for any given input vector $Z_{t}$.
In most cases, when time is the only variable in the gating network, the
transition between experts will be a smooth function of time such as the
logistic function or a rescaled hyperbolioc tangent. In this paper, prior
probabilities for two and three expert models are smooth transitions
structured in a way that each expert governs the series for roughly the same
amount of time (figure \ref{fig:expertmembership} provides a graphical example).
We can organize our prior weights for each expert into a matrix, $G$, whose
columns represent separate experts and rows index input patterns. For notational
purposes, the M-vector of proabibility of belonging to each expert for input
vector $t$ is written as $G_{t}$, while the T-vector of expert $i$'s membership
across all input patterns will be written as $G^{s=i}$.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/smooth_expert_membership.jpeg}
  \caption{Example of the smooth transitions probabilities between experts used in this study's simulations.}
  \label{fig:expertmembership}
\end{figure}

This method of simulation, with a small tweak, provides a route to obtaining bootstrapped confidenced intervals for the estimated parameters of the an HME model. The only change is replace the random draws from a fixed distribution $F$ Another approach to parameter inference is the venerable bootstrap. If our gating network is a function of time, the following strategy using bootstrapped residuals can be used to find confidence intervals for our parameter estimates. Once an HME model is estimated, we can organize our prior weights for each expert into a matrix, $G$, where each column $s$ represents the (prior) probability of expert . Note that each row sums to one. Posterior expert weights can be collect in a similar matrix called $H$.

% \begin{algorithm}
%   \caption{Bootstrap Sample}
%   \begin{algorithmic}[1]
%     \REQUIRE $H$, $G$, $M(\hat{\phi})$, $\hat{e}$, $T$, $T_{B}$
%     \FOR{$t=-T_{B}$ to $T$}
%       \IF{$t < 1$}
%         \STATE $s \thicksim G_{t=1}$
%       \ELSE
%         \STATE $s \thicksim G_{t}$
%       \ENDIF
%       \STATE $\epsilon^{*}_{t} \thicksim \hat{e}^{i=s}$ with weights $H^{i=s}$
%       \STATE $y^{*}_{t} \leftarrow M^{i=s}(y^{*}_{-T_{B},...,t-1},\hat{\beta}^{i=s}) + \epsilon^{*}_{t}$
%     \ENDFOR
%     \RETURN $y^{*}_{1,...,T}$
%   \end{algorithmic}
% \end{algorithm}

\section{Diagnostics}
pg 7 of \cite{WMS1995} suggested observing the distribution of the terminal
$g_{i}$. If only one expert is responsible for each observations, $g_{i}$ will
be close to one for a single expert and near zero for all other experts.
Can we formalize this comparison in to a specific test?

density forecasts evaluations.

Standard likelihood-ratio test is not valid (\cite{CarvalhoTanner2006}) with
AIC/BIC/VOUNG test being prefered.



\printbibliography


\end{document}

